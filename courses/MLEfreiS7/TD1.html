<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.147">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>Machine Learning - Preface</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./TD2.html" rel="next">
<link href="./index.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script><script async="" src="https://hypothes.is/embed.js"></script><link rel="stylesheet" href="mycss.css">
</head>
<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"><div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Preface</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./" class="sidebar-logo-link">
      <img src="./img/logo_efrei_assas_white.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none"></a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Machine Learning</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle sidebar-tool" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle sidebar-tool" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Overview</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./TD1.html" class="sidebar-item-text sidebar-link active">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./TD2.html" class="sidebar-item-text sidebar-link">Lab2: Logistic Regression &amp; Regularization</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./TD3.html" class="sidebar-item-text sidebar-link">Lab3: Decision Trees and Random Forests</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./TD4.html" class="sidebar-item-text sidebar-link">Lab4: Neural Networks</a>
  </div>
</li>
    </ul>
</div>
</nav><!-- margin-sidebar --><div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#lab1-linear-regression" id="toc-lab1-linear-regression" class="nav-link active" data-scroll-target="#lab1-linear-regression">Lab1: Linear Regression</a>
  <ul class="collapse">
<li><a href="#python-environment" id="toc-python-environment" class="nav-link" data-scroll-target="#python-environment">Python environment</a></li>
  <li>
<a href="#predicting-house-value-boston-dataset" id="toc-predicting-house-value-boston-dataset" class="nav-link" data-scroll-target="#predicting-house-value-boston-dataset">Predicting House Value: Boston dataset</a>
  <ul class="collapse">
<li><a href="#loading-data" id="toc-loading-data" class="nav-link" data-scroll-target="#loading-data">Loading Data</a></li>
  </ul>
</li>
  <li><a href="#data-preprocessing" id="toc-data-preprocessing" class="nav-link" data-scroll-target="#data-preprocessing">Data preprocessing</a></li>
  </ul>
</li>
  </ul></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<h1 class="title d-none d-lg-block">Preface</h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header><p>This is a Quarto book.</p>
<p>To learn more about Quarto books visit <a href="https://quarto.org/docs/books" class="uri">https://quarto.org/docs/books</a>.</p>
<section id="lab1-linear-regression" class="level1 unnumbered"><h1 class="unnumbered">Lab1: Linear Regression</h1>
<div class="cell">

</div>
<section id="python-environment" class="level2"><h2 class="anchored" data-anchor-id="python-environment">Python environment</h2>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Anaconda
</div>
</div>
<div class="callout-body-container callout-body">
<p>During this course we are going to use Python as programming language. Anaconda is an open-source distribution for Python. It is used for data science, machine learning, deep learning, etc. It comes with more than 300 libraries for data science. Anaconda helps in simplified package management and deployment.</p>
<p>To install it, go to <a href="https://www.anaconda.com/" target="_blank">Anaconda website</a>.</p>
<p>Remark: if you have a Mac with M1 ship, you must install the 2022.05 release of Anaconda: <a href="https://www.anaconda.com/blog/new-release-anaconda-distribution-now-supporting-m1" target="_blank">(Anaconda Distribution Now Supporting M1)</a>.</p>
</div>
</div>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Jupyter
</div>
</div>
<div class="callout-body-container callout-body">
<p>During the labs, you must use Jupyter notebooks. The Jupyter Notebook is the original web application for creating and sharing computational documents. It offers a simple, streamlined, document-centric experience. Jupyter is installed by default when you install Anaconda. You can create notebooks using JupyterLab via your browser or using a text editor like VScode.</p>
</div>
</div>
</section><section id="predicting-house-value-boston-dataset" class="level2"><h2 class="anchored" data-anchor-id="predicting-house-value-boston-dataset">Predicting House Value: Boston dataset</h2>
<p>In this lab we are going to use a dataset called Boston. It records the median value of houses for 506 neighborhoods around Boston. Our task is to predict the median house value.</p>
<section id="loading-data" class="level3"><h3 class="anchored" data-anchor-id="loading-data">Loading Data</h3>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Boston dataset
</div>
</div>
<div class="callout-body-container callout-body">
<p>The dataset is available in <code>scikit-learn</code> or also <a href="datasets/Boston.csv" target="_blank">here 🔗</a>. Notice that the format/approach is not the same. You are free to use any of them, it is up to you to adapt your codes correctly.</p>
<p>There is mainly two approaches you need to know for instance:</p>
<ul>
<li>The features and the target variable are in the same dataframe. In this case you can use the argument <code>formula = target ~ features</code> in certain fitting functions (like in <code>ols()</code>, imitating R’s programming language functions).</li>
<li>The features and the target variable are separated in <code>X</code> and <code>y</code>.<br>
</li>
</ul>
</div>
</div>
<p><strong>1.</strong> Load these necessary libraries for this lab (install them if needed).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt </span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd  </span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns </span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the following line to obtain the matplotlib figures in the notebook</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># We will also use sklearn but we will load the necessary modules when needed</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">

</div>
<p><strong>2.</strong> Load the Boston dataset.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_boston</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>boston_dataset <span class="op">=</span> load_boston()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>3.</strong> Print the value of the boston_dataset to understand what it contains.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(boston_dataset.keys())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename', 'data_module'])</code></pre>
</div>
</div>
<ul>
<li>data: contains the information for various houses</li>
<li>target: prices of the house</li>
<li>feature_names: names of the features</li>
<li>DESCR: describes the dataset</li>
</ul>
<p>To know more about the features run <code>boston_dataset.DESCR</code>.</p>
<p>The prices of the house indicated by the variable <code>MEDV</code> is our <strong>target variable</strong> and the remaining are the <strong>feature variables</strong> based on which we will predict the median value of houses in a district.</p>
<p><strong>3.</strong> Load the data into a pandas dataframe using <code>pd.DataFrame</code>. Then print the first 5 rows of the data using <code><a href="https://rdrr.io/r/utils/head.html">head()</a></code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>boston <span class="op">=</span> pd.DataFrame(boston_dataset.data, columns<span class="op">=</span>boston_dataset.feature_names)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>boston.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      CRIM    ZN  INDUS  CHAS    NOX  ...  RAD    TAX  PTRATIO       B  LSTAT
0  0.00632  18.0   2.31   0.0  0.538  ...  1.0  296.0     15.3  396.90   4.98
1  0.02731   0.0   7.07   0.0  0.469  ...  2.0  242.0     17.8  396.90   9.14
2  0.02729   0.0   7.07   0.0  0.469  ...  2.0  242.0     17.8  392.83   4.03
3  0.03237   0.0   2.18   0.0  0.458  ...  3.0  222.0     18.7  394.63   2.94
4  0.06905   0.0   2.18   0.0  0.458  ...  3.0  222.0     18.7  396.90   5.33

[5 rows x 13 columns]</code></pre>
</div>
</div>
<p>We can see that the target value <code>MEDV</code> is missing from the data. We create a new column of target values and add it to the dataframe.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>boston[<span class="st">'MEDV'</span>] <span class="op">=</span> boston_dataset.target</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>Remark</strong>: the previous steps we evitable if we loaded the data from csv given above using <code>pd.read_csv()</code>.</p>
</section></section><section id="data-preprocessing" class="level2"><h2 class="anchored" data-anchor-id="data-preprocessing">Data preprocessing</h2>
<p><strong>4.</strong> Check if there are any missing values in the data.</p>
<div class="callout-important callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Solution (Click to expand)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>boston.isnull().<span class="bu">sum</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>CRIM       0
ZN         0
INDUS      0
CHAS       0
NOX        0
RM         0
AGE        0
DIS        0
RAD        0
TAX        0
PTRATIO    0
B          0
LSTAT      0
MEDV       0
dtype: int64</code></pre>
</div>
</div>
</div>
</div>
</div>
<!-- 

**Exploratory Data Analysis**

Exploratory Data Analysis is a very important step before training the model. In this section, we will use some visualizations to understand the relationship of the target variable with other features.

**5.** Plot the distribution of the target variable `MEDV`. You can use the `distplot()` function from the `seaborn` library.

:::{.callout-important collapse="true" icon="false"}
## Solution


::: {.cell}

```{.python .cell-code}
sns.distplot(boston['MEDV'], bins=30)
plt.title('Distribution of the target value')
plt.show()
```

::: {.cell-output-display}
![](TD1_files/figure-html/unnamed-chunk-11-1.png){width=672}
:::
:::

:::




**6.** Calculate the correlation matrix and visualize it (you may use `heatmap()` from `seaborn` library). Name the features that are highly correlated with the target variable. 

:::{.callout-important collapse="true" icon="false"}
## Solution


::: {.cell}

```{.python .cell-code}
boston.corr()
```

::: {.cell-output .cell-output-stdout}
```
             CRIM        ZN     INDUS  ...         B     LSTAT      MEDV
CRIM     1.000000 -0.200469  0.406583  ... -0.385064  0.455621 -0.388305
ZN      -0.200469  1.000000 -0.533828  ...  0.175520 -0.412995  0.360445
INDUS    0.406583 -0.533828  1.000000  ... -0.356977  0.603800 -0.483725
CHAS    -0.055892 -0.042697  0.062938  ...  0.048788 -0.053929  0.175260
NOX      0.420972 -0.516604  0.763651  ... -0.380051  0.590879 -0.427321
RM      -0.219247  0.311991 -0.391676  ...  0.128069 -0.613808  0.695360
AGE      0.352734 -0.569537  0.644779  ... -0.273534  0.602339 -0.376955
DIS     -0.379670  0.664408 -0.708027  ...  0.291512 -0.496996  0.249929
RAD      0.625505 -0.311948  0.595129  ... -0.444413  0.488676 -0.381626
TAX      0.582764 -0.314563  0.720760  ... -0.441808  0.543993 -0.468536
PTRATIO  0.289946 -0.391679  0.383248  ... -0.177383  0.374044 -0.507787
B       -0.385064  0.175520 -0.356977  ...  1.000000 -0.366087  0.333461
LSTAT    0.455621 -0.412995  0.603800  ... -0.366087  1.000000 -0.737663
MEDV    -0.388305  0.360445 -0.483725  ...  0.333461 -0.737663  1.000000

[14 rows x 14 columns]
```
:::
:::

::: {.cell}

```{.python .cell-code}
correlation_matrix = boston.corr().round(2)
# annot = True to print the values inside the square
sns.heatmap(data=correlation_matrix, annot=True)
```

::: {.cell-output-display}
![](TD1_files/figure-html/unnamed-chunk-13-3.png){width=672}
:::
:::

::: {.cell}

```{.python .cell-code}
correlation_matrix['MEDV'].sort_values(ascending=False)
```

::: {.cell-output .cell-output-stdout}
```
MEDV       1.00
RM         0.70
ZN         0.36
B          0.33
DIS        0.25
CHAS       0.18
AGE       -0.38
RAD       -0.38
CRIM      -0.39
NOX       -0.43
TAX       -0.47
INDUS     -0.48
PTRATIO   -0.51
LSTAT     -0.74
Name: MEDV, dtype: float64
```
:::
:::


The features that are highly correlated with the target variable are RM and LSTAT where 

- **RM**  has a strong positive correlation with MEDV (0.7) and <br>
- **LSTAT**  has a high negative correlation with MEDV(-0.74).

:::

::: {.callout-tip}
## Correlation
The correlation coefficient ranges from -1 to 1. If the value is close to 1, it means that there is a strong positive correlation (linear tendancy) between the two variables. When it is close to -1, the variables have a strong negative correlation.
:::

::: {.callout-note}
For a linear regression model, we select the features which have a high correlation with the target variable. Anyway there is some feature selection techniques you may use, one of them is Backward selection:

**Backward selection**:

* Start with all variables in the model.
* Remove the variable with the largest p-value — that is, the variable that is the least statistically significant.
* The new $(p − 1)$-variable model is fit, and the variable with the largest p-value is removed.
* Continue until a stopping rule is reached. For instance, we may stop when all remaining variables have a significant p-value defined by some significance threshold.
:::

**7.** Check for multi-colinearity between the features. More specifically `RAD` and `TAX`. 

::: {.callout-tip}
We should not select colinear features together for training the model. Check this [link](https://stats.stackexchange.com/questions/1149/is-there-an-intuitive-explanation-why-multicollinearity-is-a-problem-in-linear-r/1150#1150){target='_blank'} for one explanation.
:::

:::{.callout-important collapse="true" icon="false"}
## Solution

It is not great to have colinearities between your features as it will make harder the interpretation of which feature is important and which is not. Although I don't believe it will harm your final estimate that much. In a extreme case it can make the matrix $X^T\cdot X$ hard / impossible to invert. It will also lower the p-value of the two colinear column which may lead to not selecting them. 

"Multicollinearity does not reduce the predictive power or reliability of the model as a whole, at least within the sample data set; it only affects calculations regarding individual predictors" 


::: {.cell}

```{.python .cell-code}
plt.scatter(boston['RAD'], boston['TAX'])
plt.title(f"Correlation RAD - TAX : {correlation_matrix['RAD']['TAX']:.2f}" )
plt.show()
```

::: {.cell-output-display}
![](TD1_files/figure-html/unnamed-chunk-15-5.png){width=672}
:::
:::

::: {.cell}

```{.python .cell-code}
ncorr = correlation_matrix.drop('MEDV', axis=0).drop('MEDV', axis=1)
np.fill_diagonal(ncorr.values, 0)
ncorr[(ncorr < 0.7) & (ncorr > -0.7)] = None

plt.figure(figsize=(10,5))
sns.heatmap(ncorr, annot=True)
plt.title('High correlations in features')
plt.show()
```

::: {.cell-output-display}
![](TD1_files/figure-html/unnamed-chunk-16-7.png){width=960}
:::
:::



As we can see we have other quite high colinearity between features but RAD-TAX is the highest. 
:::


**Splitting the data into training and testing sets**

Train test split is a model validation procedure that allows you to simulate how a model would perform on new/unseen data. Here is how the procedure works:

![](img/train_test_split.jpg)

**8.** Split the data into training and testing sets. We are going to train the model with 80% of the samples and test with the remaining 20%. Use `train_test_split()` function provided by `scikit-learn` library



::: {.cell}

```{.python .cell-code}
from sklearn.model_selection import train_test_split

# complete the code
X = ...
Y = ...

X_train, X_test, Y_train, Y_test = ...(, , test_size = ..., random_state=5)

# print the shapes to verify if the splitting has occured properly
print(X_train.shape)
print(X_test.shape)
print(Y_train.shape)
print(Y_test.shape)
```
:::



:::{.callout-important collapse="true" icon="false"}
## Solution


::: {.cell}

```{.python .cell-code}
from sklearn.model_selection import train_test_split

# complete the code
X = boston.drop('MEDV', axis=1)
Y = boston[['MEDV']]

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2,random_state=5)

# print the shapes to verify if the splitting has occured properly
print(X_train.shape)
```

::: {.cell-output .cell-output-stdout}
```
(404, 13)
```
:::

```{.python .cell-code}
print(X_test.shape)
```

::: {.cell-output .cell-output-stdout}
```
(102, 13)
```
:::

```{.python .cell-code}
print(Y_train.shape)
```

::: {.cell-output .cell-output-stdout}
```
(404, 1)
```
:::

```{.python .cell-code}
print(Y_test.shape)
```

::: {.cell-output .cell-output-stdout}
```
(102, 1)
```
:::
:::

:::

## **Simple** Linear Regression model

In this part, we are going to build a simple linear regression model. We will choose `LSTAT` as a feature.

**9.** Plot `MEDV` in function of `LSTAT`.

:::{.callout-important collapse="true" icon="false"}
## Solution


::: {.cell}

```{.python .cell-code}
plt.scatter(X_train['LSTAT'], Y_train['MEDV'])
plt.show()
```

::: {.cell-output-display}
![](TD1_files/figure-html/unnamed-chunk-19-9.png){width=960}
:::
:::

:::

**10.** Fit a simple regression model using `LinearRegression()` from `sklearn.linear_model`.


::: {.cell}

```{.python .cell-code}
from sklearn.linear_model import LinearRegression

slm = LinearRegression()
slm.fit(..., ...)
```
:::


:::{.callout-important collapse="true" icon="false"}
## Solution


::: {.cell}

```{.python .cell-code}
X_train_s = X_train[['LSTAT']] # Select LSTAT only
X_test_s = X_test[['LSTAT']] # Select LSTAT only

from sklearn.linear_model import LinearRegression

slm = LinearRegression(fit_intercept=True)
slm.fit(X_train_s, Y_train)
```

::: {.cell-output .cell-output-stdout}
```
LinearRegression()
```
:::
:::

:::


**11.** The `LinearRegression()` module from `scikit-learn` does not provide a statistical summary of the regression model. To obtain this summary, re-fit a model using `ols()` from `statsmodels`. Analyse the p-value from the summary and interpret.

:::{.callout-important collapse="true" icon="false"}
## Solution


::: {.cell}

```{.python .cell-code}
import statsmodels.api as sm

X_train_d = sm.add_constant(X_train_s)
model = sm.OLS(Y_train,X_train_d)
results = model.fit()
print(results.summary())
```

::: {.cell-output .cell-output-stdout}
```
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                   MEDV   R-squared:                       0.552
Model:                            OLS   Adj. R-squared:                  0.551
Method:                 Least Squares   F-statistic:                     495.9
Date:                Sat, 03 Sep 2022   Prob (F-statistic):           3.76e-72
Time:                        23:56:20   Log-Likelihood:                -1310.5
No. Observations:                 404   AIC:                             2625.
Df Residuals:                     402   BIC:                             2633.
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         34.8729      0.630     55.341      0.000      33.634      36.112
LSTAT         -0.9798      0.044    -22.269      0.000      -1.066      -0.893
==============================================================================
Omnibus:                      111.582   Durbin-Watson:                   1.948
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              237.785
Skew:                           1.447   Prob(JB):                     2.32e-52
Kurtosis:                       5.399   Cond. No.                         29.3
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
```
:::
:::


Most of the features seem relevant except for `INDUS` and `AGE`. 
:::


**12.** Plot the regression model. 

:::{.callout-important collapse="true" icon="false"}
## Solution


::: {.cell}

```{.python .cell-code}
plt.plot(X_train_s['LSTAT'], slm.predict(X_train_s), c='red')
plt.scatter(X_train_s['LSTAT'], Y_train['MEDV'])
plt.xlabel('LSTAT')
plt.ylabel('Predicted Values')
plt.show()
```

::: {.cell-output-display}
![](TD1_files/figure-html/unnamed-chunk-23-11.png){width=960}
:::
:::

:::


**Model evaluation**

**13.** Evaluate the model using MSE (Mean Squarred Error) and R2-score. 


::: {.cell}

```{.python .cell-code}
from sklearn.metrics import mean_squared_error

# train error (MSE)
y_train_predict = slm.predict(...)
mse_train = ...(..., ...)

print("The model performance for training set")

print('MSE is {}'.format(mse_train))


# test error
y_test_predict = slm.predict(...)
mse_test = mean_squared_error(..., ...)
r2 = r2_score(..., ...)

print("The model performance for testing set")

print('MSE is {}'.format(mse_test))
print('R2 score is {}'.format(r2))
```
:::



:::{.callout-important collapse="true" icon="false"}
## Solution


::: {.cell}

```{.python .cell-code}
from sklearn.metrics import mean_squared_error, r2_score

# train error (MSE)
y_train_predict =  slm.predict(X_train_s)
mse_train = mean_squared_error(Y_train, y_train_predict)

print("The model performance for training set")
```

::: {.cell-output .cell-output-stdout}
```
The model performance for training set
```
:::

```{.python .cell-code}
print('MSE is {}'.format(mse_train))


# test error
```

::: {.cell-output .cell-output-stdout}
```
MSE is 38.45801898706332
```
:::

```{.python .cell-code}
y_test_predict = slm.predict(X_test_s)
mse_test = mean_squared_error(Y_test, y_test_predict)
r2 = r2_score(Y_test, y_test_predict)

print("The model performance for testing set")
```

::: {.cell-output .cell-output-stdout}
```
The model performance for testing set
```
:::

```{.python .cell-code}
print('MSE is {}'.format(mse_test))
```

::: {.cell-output .cell-output-stdout}
```
MSE is 38.821829014286585
```
:::

```{.python .cell-code}
print('R2 score is {}'.format(r2))
```

::: {.cell-output .cell-output-stdout}
```
R2 score is 0.5041523728903132
```
:::
:::

:::

**14.** According to the plot in **9**, the relationship between `LSTAT` and `MEDV` is not linear. Let’s try a transformation of our explanatory variable `LSTAT`. Re-do the steps from **9** to **13** but using the log of `LSTAT`. Do you obtain a better model?


:::{.callout-important collapse="true" icon="false"}
## Solution


::: {.cell}

```{.python .cell-code}
X_train_s = np.log(X_train[['LSTAT']]) # Select LSTAT only
X_test_s = np.log(X_test[['LSTAT']]) # Select LSTAT only

slm = LinearRegression(fit_intercept=True)
slm.fit(X_train_s, Y_train)

# train error (MSE)
```

::: {.cell-output .cell-output-stdout}
```
LinearRegression()
```
:::

```{.python .cell-code}
y_train_predict =  slm.predict(X_train_s)
mse_train = mean_squared_error(Y_train, y_train_predict)

print("The model performance for training set")
```

::: {.cell-output .cell-output-stdout}
```
The model performance for training set
```
:::

```{.python .cell-code}
print('MSE is {}'.format(mse_train))


# test error
```

::: {.cell-output .cell-output-stdout}
```
MSE is 28.39419482862284
```
:::

```{.python .cell-code}
y_test_predict = slm.predict(X_test_s)
mse_test = mean_squared_error(Y_test, y_test_predict)
r2 = r2_score(Y_test, y_test_predict)

print("The model performance for testing set")
```

::: {.cell-output .cell-output-stdout}
```
The model performance for testing set
```
:::

```{.python .cell-code}
print('MSE is {}'.format(mse_test))
```

::: {.cell-output .cell-output-stdout}
```
MSE is 27.899544077825233
```
:::

```{.python .cell-code}
print('R2 score is {}'.format(r2))
```

::: {.cell-output .cell-output-stdout}
```
R2 score is 0.6436560801053237
```
:::
:::

::: {.cell}

```{.python .cell-code}
plt.scatter(X_train['LSTAT'], Y_train['MEDV'])
plt.scatter(X_train['LSTAT'], slm.predict(X_train_s), c='red')
plt.xlabel('LSTAT')
plt.ylabel('Predicted Values')
plt.show()
```

::: {.cell-output-display}
![](TD1_files/figure-html/unnamed-chunk-27-13.png){width=960}
:::
:::


We do obtain a better model, interestingly we can visualise that the model is representing a non linear relationship between the variable and the output thanks to this trick. 

:::

## Multiple Linear Regression model

**15.** Train a new model using all the variables of the dataset. Evalute the performance of the model.

:::{.callout-important collapse="true" icon="false"}
## Solution


::: {.cell}

```{.python .cell-code}
X_train_d = sm.add_constant(X_train)
model = sm.OLS(Y_train,X_train_d)
results = model.fit()
print(results.summary())
```

::: {.cell-output .cell-output-stdout}
```
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                   MEDV   R-squared:                       0.738
Model:                            OLS   Adj. R-squared:                  0.730
Method:                 Least Squares   F-statistic:                     84.65
Date:                Sat, 03 Sep 2022   Prob (F-statistic):          8.21e-105
Time:                        23:56:22   Log-Likelihood:                -1202.0
No. Observations:                 404   AIC:                             2432.
Df Residuals:                     390   BIC:                             2488.
Df Model:                          13                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         37.9125      5.775      6.565      0.000      26.559      49.266
CRIM          -0.1308      0.036     -3.603      0.000      -0.202      -0.059
ZN             0.0494      0.016      3.131      0.002       0.018       0.080
INDUS          0.0011      0.072      0.015      0.988      -0.141       0.143
CHAS           2.7054      0.989      2.737      0.006       0.762       4.649
NOX          -15.9571      4.517     -3.532      0.000     -24.838      -7.076
RM             3.4140      0.470      7.266      0.000       2.490       4.338
AGE            0.0011      0.015      0.077      0.939      -0.027       0.030
DIS           -1.4931      0.233     -6.404      0.000      -1.951      -1.035
RAD            0.3644      0.079      4.628      0.000       0.210       0.519
TAX           -0.0132      0.004     -2.950      0.003      -0.022      -0.004
PTRATIO       -0.9524      0.149     -6.411      0.000      -1.244      -0.660
B              0.0117      0.003      3.795      0.000       0.006       0.018
LSTAT         -0.5941      0.058    -10.271      0.000      -0.708      -0.480
==============================================================================
Omnibus:                      134.272   Durbin-Watson:                   2.057
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              542.194
Skew:                           1.423   Prob(JB):                    1.84e-118
Kurtosis:                       7.911   Cond. No.                     1.51e+04
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 1.51e+04. This might indicate that there are
strong multicollinearity or other numerical problems.
```
:::
:::

:::

**16.** Which features are significant for the model? 

:::{.callout-important collapse="true" icon="false"}
## Solution

The ones with p-values is less than the error we fix (let's say 5%). `Age` for example.
:::

**17.** Apply backward selection to fit a model with the best subset of features. 

:::{.callout-important collapse="true" icon="false"}
## Solution


::: {.cell}

```{.python .cell-code}
columns_to_drop = ['AGE']
X_train_bck = X_train.drop(columns_to_drop, axis=1)
X_test_bck = X_test.drop(columns_to_drop, axis=1)


model = sm.OLS(Y_train,sm.add_constant(X_train_bck))
results = model.fit()
print(results.summary())
```

::: {.cell-output .cell-output-stdout}
```
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                   MEDV   R-squared:                       0.738
Model:                            OLS   Adj. R-squared:                  0.730
Method:                 Least Squares   F-statistic:                     91.94
Date:                Sat, 03 Sep 2022   Prob (F-statistic):          8.39e-106
Time:                        23:56:23   Log-Likelihood:                -1202.0
No. Observations:                 404   AIC:                             2430.
Df Residuals:                     391   BIC:                             2482.
Df Model:                          12                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         37.8841      5.756      6.582      0.000      26.568      49.200
CRIM          -0.1309      0.036     -3.610      0.000      -0.202      -0.060
ZN             0.0492      0.016      3.151      0.002       0.019       0.080
INDUS          0.0009      0.072      0.013      0.990      -0.141       0.143
CHAS           2.7098      0.986      2.749      0.006       0.772       4.648
NOX          -15.8774      4.392     -3.615      0.000     -24.512      -7.243
RM             3.4208      0.461      7.423      0.000       2.515       4.327
DIS           -1.4985      0.222     -6.754      0.000      -1.935      -1.062
RAD            0.3640      0.078      4.640      0.000       0.210       0.518
TAX           -0.0132      0.004     -2.953      0.003      -0.022      -0.004
PTRATIO       -0.9516      0.148     -6.430      0.000      -1.243      -0.661
B              0.0118      0.003      3.818      0.000       0.006       0.018
LSTAT         -0.5925      0.054    -10.938      0.000      -0.699      -0.486
==============================================================================
Omnibus:                      134.673   Durbin-Watson:                   2.057
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              545.931
Skew:                           1.426   Prob(JB):                    2.84e-119
Kurtosis:                       7.929   Cond. No.                     1.49e+04
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 1.49e+04. This might indicate that there are
strong multicollinearity or other numerical problems.
```
:::

```{.python .cell-code}
columns_to_drop = ['INDUS']
X_train_bck = X_train.drop(columns_to_drop, axis=1)
X_test_bck = X_test.drop(columns_to_drop, axis=1)

model = sm.OLS(Y_train,sm.add_constant(X_train_bck))
results = model.fit()
print(results.summary())
```

::: {.cell-output .cell-output-stdout}
```
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                   MEDV   R-squared:                       0.738
Model:                            OLS   Adj. R-squared:                  0.730
Method:                 Least Squares   F-statistic:                     91.94
Date:                Sat, 03 Sep 2022   Prob (F-statistic):          8.37e-106
Time:                        23:56:23   Log-Likelihood:                -1202.0
No. Observations:                 404   AIC:                             2430.
Df Residuals:                     391   BIC:                             2482.
Df Model:                          12                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         37.9074      5.758      6.584      0.000      26.588      49.227
CRIM          -0.1308      0.036     -3.610      0.000      -0.202      -0.060
ZN             0.0494      0.016      3.154      0.002       0.019       0.080
CHAS           2.7073      0.979      2.765      0.006       0.783       4.632
NOX          -15.9390      4.351     -3.663      0.000     -24.494      -7.384
RM             3.4133      0.467      7.304      0.000       2.495       4.332
AGE            0.0011      0.014      0.077      0.939      -0.027       0.030
DIS           -1.4938      0.227     -6.570      0.000      -1.941      -1.047
RAD            0.3641      0.076      4.799      0.000       0.215       0.513
TAX           -0.0131      0.004     -3.269      0.001      -0.021      -0.005
PTRATIO       -0.9521      0.147     -6.479      0.000      -1.241      -0.663
B              0.0117      0.003      3.802      0.000       0.006       0.018
LSTAT         -0.5940      0.058    -10.308      0.000      -0.707      -0.481
==============================================================================
Omnibus:                      134.288   Durbin-Watson:                   2.057
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              542.346
Skew:                           1.423   Prob(JB):                    1.70e-118
Kurtosis:                       7.911   Cond. No.                     1.49e+04
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 1.49e+04. This might indicate that there are
strong multicollinearity or other numerical problems.
```
:::

```{.python .cell-code}
y_test_predict = results.predict(sm.add_constant(X_test_bck))
mse_test = mean_squared_error(Y_test, y_test_predict)
r2 = r2_score(Y_test, y_test_predict)

print("The model performance for testing set")
```

::: {.cell-output .cell-output-stdout}
```
The model performance for testing set
```
:::

```{.python .cell-code}
print('MSE is {}'.format(mse_test))
```

::: {.cell-output .cell-output-stdout}
```
MSE is 20.872226101228375
```
:::

```{.python .cell-code}
print('R2 score is {}'.format(r2))
```

::: {.cell-output .cell-output-stdout}
```
R2 score is 0.7334117416007803
```
:::
:::

:::


**18.** Is the new model better than the last one with all the features? 

:::{.callout-important collapse="true" icon="false"}
## Solution


::: {.cell}

```{.python .cell-code}
y_test_predict = results.predict(sm.add_constant(X_test_bck))
mse_test = mean_squared_error(Y_test, y_test_predict)
r2 = r2_score(Y_test, y_test_predict)

print("The model performance for testing set")
```

::: {.cell-output .cell-output-stdout}
```
The model performance for testing set
```
:::

```{.python .cell-code}
print('MSE is {}'.format(mse_test))
```

::: {.cell-output .cell-output-stdout}
```
MSE is 20.872226101228375
```
:::

```{.python .cell-code}
print('R2 score is {}'.format(r2))
```

::: {.cell-output .cell-output-stdout}
```
R2 score is 0.7334117416007803
```
:::
:::

:::

**19.** In the last model we didn’t transform `LSTAT`. Re train the model using `log(LSTAT)` instead of `LSTAT`. Does this new model performs better?

:::{.callout-important collapse="true" icon="false"}
## Solution


::: {.cell}

```{.python .cell-code}
X_train_lg = X_train.drop('LSTAT',axis=1)
X_train_lg['log(LSTAT)'] = np.log(X_train['LSTAT'])

X_test_lg = X_test.drop('LSTAT',axis=1)
X_test_lg['log(LSTAT)'] = np.log(X_test['LSTAT'])


model = sm.OLS(Y_train,sm.add_constant(X_train_lg))
results = model.fit()
print(results.summary())
```

::: {.cell-output .cell-output-stdout}
```
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                   MEDV   R-squared:                       0.787
Model:                            OLS   Adj. R-squared:                  0.780
Method:                 Least Squares   F-statistic:                     111.1
Date:                Sat, 03 Sep 2022   Prob (F-statistic):          3.07e-122
Time:                        23:56:24   Log-Likelihood:                -1160.0
No. Observations:                 404   AIC:                             2348.
Df Residuals:                     390   BIC:                             2404.
Df Model:                          13                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         56.7251      5.535     10.249      0.000      45.843      67.607
CRIM          -0.1438      0.032     -4.444      0.000      -0.207      -0.080
ZN             0.0286      0.014      2.009      0.045       0.001       0.057
INDUS         -0.0018      0.065     -0.027      0.978      -0.130       0.126
CHAS           2.0388      0.894      2.281      0.023       0.281       3.796
NOX          -15.0430      4.065     -3.701      0.000     -23.034      -7.052
RM             2.1356      0.443      4.820      0.000       1.265       3.007
AGE            0.0252      0.013      1.892      0.059      -0.001       0.051
DIS           -1.2703      0.211     -6.010      0.000      -1.686      -0.855
RAD            0.3307      0.071      4.663      0.000       0.191       0.470
TAX           -0.0116      0.004     -2.874      0.004      -0.019      -0.004
PTRATIO       -0.8343      0.134     -6.212      0.000      -1.098      -0.570
B              0.0094      0.003      3.348      0.001       0.004       0.015
log(LSTAT)    -9.5323      0.643    -14.825      0.000     -10.796      -8.268
==============================================================================
Omnibus:                      120.438   Durbin-Watson:                   2.046
Prob(Omnibus):                  0.000   Jarque-Bera (JB):              557.257
Skew:                           1.212   Prob(JB):                    9.84e-122
Kurtosis:                       8.218   Cond. No.                     1.56e+04
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The condition number is large, 1.56e+04. This might indicate that there are
strong multicollinearity or other numerical problems.
```
:::

```{.python .cell-code}
y_test_predict = results.predict(sm.add_constant(X_test_lg))
mse_test = mean_squared_error(Y_test, y_test_predict)
r2 = r2_score(Y_test, y_test_predict)

print("The model performance for testing set")
```

::: {.cell-output .cell-output-stdout}
```
The model performance for testing set
```
:::

```{.python .cell-code}
print('MSE is {}'.format(mse_test))
```

::: {.cell-output .cell-output-stdout}
```
MSE is 15.039960438345249
```
:::

```{.python .cell-code}
print('R2 score is {}'.format(r2))

```

::: {.cell-output .cell-output-stdout}
```
R2 score is 0.8079037262146341
```
:::
:::


The model is better (lower test MSE).

:::

## ANOVA (ANalysis Of VAriances)

In this last part we will apply an analysis of variances (ANOVA) in order to test if there is a significant difference of means between two groups $i$ and $j$ (Consider group $i$ is the suburbs bounding the river and $j$ the suburbs which not). The hypotheses are 

$$ H_0 : \mu_i = \mu_j $$ 

$$ H_1 : \mu_i \neq \mu_j $$

Where $\mu_i$ is the mean of `MEDV` in group $i$.

::: {.callout-note}
## Anova
This analysis can be conducted during the exploratory data analysis part especially when the target is continuous and a feature is discrete.  
:::

**20.** In the Boston data set there is a categorical variable `CHAS` which corresponds to Charles River (= 1 if a suburb bounds the river; 0 otherwise). How many of the suburbs in this data set bound the Charles river?



**21.** Create Boxplots of the median value of houses with respect to the variable `CHAS`. Do we observe some difference between the median value of houses with respect to the neighborhood to Charles River? 


::: {.cell}

:::



**22.** Calculate $\mu_i$ and $\mu_j$.


::: {.cell}

:::


**23.** Apply an ANOVA test of `MEDV` with respect to `CHAS`. What do you conclude ?


::: {.cell}

:::



:::{.callout-important collapse="true" icon="false"}
## Solution


::: {.cell}

```{.python .cell-code}
boston['CHAS'].value_counts()
```

::: {.cell-output .cell-output-stdout}
```
0.0    471
1.0     35
Name: CHAS, dtype: int64
```
:::
:::

::: {.cell}

```{.python .cell-code}
sns.boxplot(x='CHAS', y='MEDV', data=boston)
plt.show()
```

::: {.cell-output-display}
![](TD1_files/figure-html/unnamed-chunk-36-1.png){width=960}
:::
:::

::: {.cell}

```{.python .cell-code}
boston.groupby('CHAS')['MEDV'].mean()
```

::: {.cell-output .cell-output-stdout}
```
CHAS
0.0    22.093843
1.0    28.440000
Name: MEDV, dtype: float64
```
:::
:::


You can use Anova's test from `statsmodels` or from `scipy`.


::: {.cell}

```{.python .cell-code}
from scipy.stats import f_oneway

f_oneway(boston[boston['CHAS'] == 1.0]['MEDV'].to_numpy(), 
         boston[boston['CHAS'] == 0.0]['MEDV'].to_numpy())
```

::: {.cell-output .cell-output-stdout}
```
F_onewayResult(statistic=15.971512420371962, pvalue=7.390623170520815e-05)
```
:::
:::


Or


::: {.cell}

```{.python .cell-code}

from statsmodels.stats.anova import anova_lm
from statsmodels.formula.api import ols

anova_boston_bound_river = anova_lm(
    ols('MEDV~C(CHAS)', data=boston[['MEDV', 'CHAS']]).fit())
print(anova_boston_bound_river)
```

::: {.cell-output .cell-output-stdout}
```
             df        sum_sq      mean_sq          F    PR(>F)
C(CHAS)     1.0   1312.079271  1312.079271  15.971512  0.000074
Residual  504.0  41404.216144    82.151223        NaN       NaN
```
:::
:::



Because the p-value is low we can reject the hypothesis that the two groups have the same mean and thus conclude that they have different means. Note that we didn't check which was the biggest here, just that they are not the same. Because we are checking only the difference between two distribution and not multiple ones. 

:::


<!-- :::{.callout-tip collapse="true"} -->
<!-- ## Expand To see solution -->
<!-- This is an example of a 'folded' caution callout that can be expanded by the user. You can use `collapse="true"` to collapse it by default or `collapse="false"` to make a collapsible callout that is expanded by default. -->
<!-- $$Y = \beta x$$ -->
<!-- ::: -->
<!-- ```{python} -->
<!-- #| eval: false -->
<!-- #| echo: false -->
<!-- # https://www.reneshbedre.com/blog/anova.html -->
<!-- ``` -->
<!-- <details><summary><span style="color: firebrick;">Voir la solution</span></summary> -->
<!-- \begin{align} -->
<!-- \frac{\partial l}{\partial \phi} & = \frac{\partial}{\partial \phi} \sum^{n}_{i=1}\left(y^{(i)}\log\phi+(1-y^{(i)})\log(1-\phi)\right) \\ -->
<!--                                 & = \sum^{n}_{i=1}\frac{\partial(y^{(i)}\log\phi)}{\partial \phi} + \frac{\partial\left((1-y^{(i)})\log(1-\phi)\right)}{\partial \phi} \\ -->
<!--                                 & = \frac{\sum^{n}_{i=1}y^{(i)}}{\phi} - \frac{\sum^{n}_{i=1}(1-y^{(i)})}{1-\phi} \\\\ -->
<!--                                 \text{on annule la dérivée } \Rightarrow & \ \frac{\sum^{n}_{i=1}y^{(i)}}{\phi} = \frac{\sum^{n}_{i=1}(1-y^{(i)})}{1-\phi} \\ -->
<!--                                 \Rightarrow & \sum^{n}_{i=1}y^{(i)}-\phi\sum^{n}_{i=1}y^{(i)} = n\phi-\phi\sum^{n}_{i=1}y^{(i)} \\ -->
<!--                                 \Rightarrow & \ \phi = \frac{\sum^{n}_{i=1}y^{(i)}}{n} \\ -->
<!--                                 \Rightarrow & \ \phi = \frac{\sum^{n}_{i=1}[y^{(i)}=1]}{n} -->
<!-- \end{align} -->
<!-- <p style="text-align: center;"><span style="color: firebrick;">_______________</span></p> -->
<!-- </details> -->


</section></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } 
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./index.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Overview</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./TD2.html" class="pagination-link">
        <span class="nav-page-text">Lab2: Logistic Regression &amp; Regularization</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>