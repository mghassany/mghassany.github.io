<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="" />
<meta property="og:type" content="book" />







<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="">

<title></title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<link rel="stylesheet" href="css\style.css" type="text/css" />
<link rel="stylesheet" href="css\toc.css" type="text/css" />

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="part"><span><b>I Unsupervised Learning</b></span></li>
<li class="has-sub"><a href="1-dimensionality-reduction.html#dimensionality-reduction"><span class="toc-section-number">1</span> Dimensionality Reduction</a><ul>
<li><a href="1-1-unsupervised-learning.html#unsupervised-learning"><span class="toc-section-number">1.1</span> Unsupervised Learning</a></li>
<li><a href="1-2-principal-components-analysis.html#principal-components-analysis"><span class="toc-section-number">1.2</span> Principal Components Analysis</a></li>
<li class="has-sub"><a href="1-3-principal-components.html#principal-components"><span class="toc-section-number">1.3</span> Principal Components</a><ul>
<li><a href="1-3-principal-components.html#notations-and-procedure">Notations and Procedure</a></li>
<li><a href="1-3-principal-components.html#first-principal-component-textpc_1-y_1">First Principal Component (<span class="math inline">\(\text{PC}_1\)</span>): <span class="math inline">\(Y_1\)</span></a></li>
<li><a href="1-3-principal-components.html#second-principal-component-textpc_2-y_2">Second Principal Component (<span class="math inline">\(\text{PC}_2\)</span>): <span class="math inline">\(Y_2\)</span></a></li>
<li><a href="1-3-principal-components.html#ith-principal-component-textpc_i-y_i"><span class="math inline">\(i^{th}\)</span> Principal Component (<span class="math inline">\(\text{PC}_i\)</span>): <span class="math inline">\(Y_i\)</span></a></li>
</ul></li>
<li class="has-sub"><a href="1-4-how-do-we-find-the-coefficients.html#how-do-we-find-the-coefficients"><span class="toc-section-number">1.4</span> How do we find the coefficients?</a><ul>
<li><a href="1-4-how-do-we-find-the-coefficients.html#why-it-may-be-possible-to-reduce-dimensions">Why It May Be Possible to Reduce Dimensions</a></li>
<li><a href="1-4-how-do-we-find-the-coefficients.html#procedure">Procedure</a></li>
</ul></li>
<li><a href="1-5-standardization-of-the-features.html#standardization-of-the-features"><span class="toc-section-number">1.5</span> Standardization of the features</a></li>
<li class="has-sub"><a href="1-6-projection-of-the-data.html#projection-of-the-data"><span class="toc-section-number">1.6</span> Projection of the data</a><ul>
<li><a href="1-6-projection-of-the-data.html#scores">Scores</a></li>
<li><a href="1-6-projection-of-the-data.html#visualization">Visualization</a></li>
<li><a href="1-6-projection-of-the-data.html#extra">Extra</a></li>
</ul></li>
<li class="has-sub"><a href="1-7-case-study.html#case-study"><span class="toc-section-number">1.7</span> Case study</a><ul>
<li><a href="1-7-case-study.html#employement-in-european-countries-in-the-late-70s">Employement in European countries in the late 70s</a></li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="how-do-we-find-the-coefficients" class="section level2">
<h2><span class="header-section-number">1.4</span> How do we find the coefficients?</h2>
<p>How do we find the coefficients <span class="math inline">\(a_{ij}\)</span> for a principal component? The solution involves the <strong>eigenvalues</strong> and <strong>eigenvectors</strong> of the variance-covariance matrix <span class="math inline">\(\Sigma\)</span>.</p>
<p>Let <span class="math inline">\(\lambda_1,\ldots,\lambda_p\)</span> denote the eigenvalues of the variance-covariance matrix <span class="math inline">\(\Sigma\)</span>. These are ordered so that <span class="math inline">\(\lambda_1\)</span> has the largest eigenvalue and <span class="math inline">\(\lambda_p\)</span> is the smallest.</p>
<p><span class="math display">\[ \lambda_1 \ge \lambda_2 \ge \dots \ge \lambda_p \]</span></p>
<p>We are also going to let the vectors <span class="math inline">\(\mathbf{a}_1, \ldots,\mathbf{a}_p\)</span> denote the corresponding eigenvectors.</p>
<p>It turns out that the elements for these eigenvectors will be the coefficients of the principal components.</p>
<div class="rmdinsight">
<p>
The elements for the eigenvectors of <span class="math inline"><em>Σ</em></span> are the coefficients of the principal components.
</p>
</div>
<p>The variance for the <span class="math inline">\(i\)</span>th principal component is equal to the <span class="math inline">\(i\)</span>th eigenvalue.</p>
<p><span class="math display">\[ \textbf{var}(Y_i) = \text{var}(a_{i1}X_1 + a_{i2}X_2 + \dots a_{ip}X_p) = \lambda_i \]</span></p>
<p>Moreover, the principal components are uncorrelated with one another.</p>
<p><span class="math display">\[\text{cov}(Y_i, Y_j) = 0\]</span></p>
<p>The variance-covariance matrix may be written as a function of the eigenvalues and their corresponding eigenvectors. In fact, the variance-covariance matrix can be written as the sum over the <span class="math inline">\(p\)</span> eigenvalues, multiplied by the product of the corresponding eigenvector times its transpose as shown in the following expression</p>
<p><span class="math display">\[ \Sigma  =  \sum_{i=1}^{p}\lambda_i \mathbf{a}_i \mathbf{a}_i^T \]</span></p>
<p>If <span class="math inline">\(\lambda_{k+1}, \lambda_{k+2}, \dots , \lambda_{p}\)</span> are small, we might approximate <span class="math inline">\(\Sigma\)</span> by</p>
<p><span class="math display">\[ \Sigma  \cong  \sum_{i=1}^{k}\lambda_i \mathbf{a}_i\mathbf{a}_i^T \]</span></p>
<p>Earlier in the chapter we defined the total variation of <span class="math inline">\(X\)</span> as the trace of the variance-covariance matrix. This is also equal to the sum of the eigenvalues as shown below:</p>
<p><span class="math display">\[ \begin{array}{lll}trace(\Sigma) &amp; = &amp; \sigma^2_1 + \sigma^2_2 + \dots +\sigma^2_p \\ &amp; = &amp; \lambda_1 + \lambda_2 + \dots + \lambda_p\end{array} \]</span></p>
<p>This will give us an interpretation of the components in terms of the amount of the full variation explained by each component. The proportion of variation explained by the <span class="math inline">\(i\)</span>th principal component is then going to be defined to be the eigenvalue for that component divided by the sum of the eigenvalues. In other words, the <span class="math inline">\(i\)</span>th principal component explains the following proportion of the total variation:</p>
<p><span class="math display">\[ \frac{\lambda_i}{\lambda_1 + \lambda_2 + \dots + \lambda_p} \]</span></p>
<p>A related quantity is the proportion of variation explained by the first <span class="math inline">\(k\)</span> principal component. This would be the sum of the first <span class="math inline">\(k\)</span> eigenvalues divided by its total variation.</p>
<p><span class="math display">\[ \frac{\lambda_1 + \lambda_2 + \dots + \lambda_k}{\lambda_1 + \lambda_2 + \dots + \lambda_p} \]</span></p>
<p>In practice, these proportions are often expressed as percentages.</p>
<p>Naturally, if the proportion of variation explained by the first <span class="math inline">\(k\)</span> principal components is large, then not much information is lost by considering only the first <span class="math inline">\(k\)</span> principal components.</p>
<div id="why-it-may-be-possible-to-reduce-dimensions" class="section level3 unnumbered">
<h3>Why It May Be Possible to Reduce Dimensions</h3>
<p>When we have correlations (multicollinarity) between the features, the data may more or less fall on a line or plane in a lower number of dimensions. For instance, imagine a plot of two features that have a nearly perfect correlation. The data points will fall close to a straight line. That line could be used as a new (one-dimensional) axis to represent the variation among data points.</p>
<div class="rmdcaution">
<p>
All of this is defined in terms of the population variance-covariance matrix <span class="math inline"><em>Σ</em></span> which is <em>unknown</em>. However, we may estimate <span class="math inline"><em>Σ</em></span> by the sample variance-covariance matrix which is given in the standard formula here:
</p>
<p>
<br /><span class="math display"><span class="math display">\[ \textbf{S} = \frac{1}{n-1} \sum_{i=1}^{n}(\mathbf{X}_i-\bar{\textbf{x}})(\mathbf{X}_i-\bar{\textbf{x}})^T \]</span></span><br />
</p>
</div>
</div>
<div id="procedure" class="section level3 unnumbered">
<h3>Procedure</h3>
<p>Compute the eigenvalues <span class="math inline">\(\hat{\lambda}_1, \hat{\lambda}_2, \dots, \hat{\lambda}_p\)</span> of the sample variance-covariance matrix <span class="math inline">\(\textbf{S}\)</span>, and the corresponding eigenvectors <span class="math inline">\(\hat{\mathbf{a}}_1, \hat{\mathbf{a}}_2, \dots, \hat{\mathbf{a}}_p\)</span>.</p>
<p>Then we will define our estimated principal components using the eigenvectors as our coefficients:</p>
<p><span class="math display">\[ \begin{array}{lll} \hat{Y}_1 &amp; = &amp; \hat{a}_{11}X_1 + \hat{a}_{12}X_2 + \dots + \hat{a}_{1p}X_p \\ \hat{Y}_2 &amp; = &amp; \hat{a}_{21}X_1 + \hat{a}_{22}X_2 + \dots + \hat{a}_{2p}X_p \\&amp;&amp;\vdots\\ \hat{Y}_p &amp; = &amp; \hat{a}_{p1}X_1 + \hat{a}_{p2}X_2 + \dots + \hat{a}_{pp}X_p \\ \end{array} \]</span></p>
<p>Generally, we only retain the first <span class="math inline">\(k\)</span> principal component. There are a number of criteria that may be used to decide how many components should be retained:</p>
<ol style="list-style-type: decimal">
<li><p>To obtain the simplest possible interpretation, we want <span class="math inline">\(k\)</span> to be as small as possible. If we can explain most of the variation just by <strong>two</strong> principal components then this would give us a much simpler description of the data.</p></li>
<li><p>Retain the first <span class="math inline">\(k\)</span> components which explain a “large” proportion of the total variation, say <span class="math inline">\(70-80\%\)</span>.</p></li>
<li><p>Examine a scree plot. This is a plot of the eigenvalues versus the component number. The idea is to look for the “elbow” which corresponds to the point after which the eigenvalues decrease more slowly. Adding components after this point explains relatively little more of the variance. See the next figure for an example of a scree plot.</p></li>
</ol>
<center>
<figure>
<img src="img/screeplot.png" alt="Scree plot showing eigenvalue by number of principal component." style="width: 400px;"/>
<figcaption>
Scree plot showing eigenvalue by number of principal component.
</figcaption>
</figure>
</center>
</div>
</div>
<p style="text-align: center;">
<a href="1-3-principal-components.html"><button class="btn btn-default">Previous</button></a>
<a href="1-5-standardization-of-the-features.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
<!-- </html> -->
