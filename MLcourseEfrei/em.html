<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-0.9.406">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>Machine Learning - 4&nbsp; Gaussian Mixture Models &amp; EM</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>
<script src="site_libs/quarto-nav/quarto-nav.js"></script><script src="site_libs/quarto-nav/headroom.min.js"></script><script src="site_libs/clipboard/clipboard.min.js"></script><meta name="quarto:offset" content="./">
<script src="site_libs/quarto-search/autocomplete.umd.js"></script><script src="site_libs/quarto-search/fuse.min.js"></script><script src="site_libs/quarto-search/quarto-search.js"></script><link href="./Lab-EM.html" rel="next">
<link href="./Lab-kmeans.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script><script src="site_libs/quarto-html/popper.min.js"></script><script src="site_libs/quarto-html/tippy.umd.min.js"></script><script src="site_libs/quarto-html/anchor.min.js"></script><link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link class="quarto-color-scheme" id="quarto-text-highlighting-styles" href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet">
<link class="quarto-color-scheme quarto-color-alternate" rel="prefetch" id="quarto-text-highlighting-styles" href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css">
<script src="site_libs/bootstrap/bootstrap.min.js"></script><link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link class="quarto-color-scheme" href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet">
<link class="quarto-color-scheme quarto-color-alternate" rel="prefetch" href="site_libs/bootstrap/bootstrap-dark.min.css">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script><script async="" src="https://hypothes.is/embed.js"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><link rel="stylesheet" href="mycss.css">
</head>
<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"><div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">
<span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Gaussian Mixture Models &amp; EM</span>
</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./" class="sidebar-logo-link">
      <img src="./img/logo_efrei_assas_white.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none"></a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Machine Learning</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle sidebar-tool" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle sidebar-tool" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
    <div class="sidebar-item-container"> 
        <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Supervised Learning</a>
      <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
        <i class="bi bi-chevron-right ms-2"></i>
      </a>
    </div>
    <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./discriminantanalysis.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">ðŸ“— Discriminant Analysis</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Lab-DA.html" class="sidebar-item-text sidebar-link">- ðŸ’» Lab</a>
  </div>
</li>
    </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
    <div class="sidebar-item-container"> 
        <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">Dimensionality Reduction</a>
      <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
        <i class="bi bi-chevron-right ms-2"></i>
      </a>
    </div>
    <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./dimreduction.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">ðŸ“— PCA &amp; t-SNE</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Lab-Dim-Red.html" class="sidebar-item-text sidebar-link">- ðŸ’» Lab</a>
  </div>
</li>
    </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
    <div class="sidebar-item-container"> 
        <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">Unsupervised Learning</a>
      <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
        <i class="bi bi-chevron-right ms-2"></i>
      </a>
    </div>
    <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./kmeans.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">ðŸ“— Kmeans &amp; Hierarchical Clustering</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Lab-kmeans.html" class="sidebar-item-text sidebar-link">- ðŸ’» Lab</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./em.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Gaussian Mixture Models</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Lab-EM.html" class="sidebar-item-text sidebar-link">- ðŸ’» Lab</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./density-based.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">ðŸ“— Density-based Clustering</span></a>
  </div>
</li>
    </ul>
</li>
    </ul>
</div>
</nav><!-- margin-sidebar --><div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li><a href="#the-gaussian-distribution" id="toc-the-gaussian-distribution" class="nav-link active" data-scroll-target="#the-gaussian-distribution"> <span class="header-section-number">4.1</span> The Gaussian distribution</a></li>
  <li><a href="#mixture-of-gaussians" id="toc-mixture-of-gaussians" class="nav-link" data-scroll-target="#mixture-of-gaussians"> <span class="header-section-number">4.2</span> Mixture of Gaussians</a></li>
  <li><a href="#em-for-gaussian-mixtures" id="toc-em-for-gaussian-mixtures" class="nav-link" data-scroll-target="#em-for-gaussian-mixtures"> <span class="header-section-number">4.3</span> EM for Gaussian Mixtures</a></li>
  </ul></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<h1 class="title d-none d-lg-block">
<span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Gaussian Mixture Models &amp; EM</span>
</h1>
</div>





<div class="quarto-title-meta">

    
    
  </div>
  

</header><p>In the previous chapter we saw the <span class="math inline">\(k\)</span>-means algorithm which is considered as a hard clustering technique, such that each point is allocated to only one cluster. In <span class="math inline">\(k\)</span>-means, a cluster is described only by its centroid. This is not too flexible, as we may have problems with clusters that are <em>overlapping</em>, or ones that are not of <em>circular shape</em>.</p>
<p>In this chapter, we will introduce a model-based clustering technique, which is <strong>E</strong>xpectation <strong>M</strong>aximization (EM). We will apply it using Gaussian Mixture Models (GMM).</p>
<p>With EM Clustering, we can go a step further and describe each cluster by its centroid (mean), covariance (so that we can have elliptical clusters), and weight (the size of the cluster). The probability that a point belongs to a cluster is now given by a multivariate Gaussian probability distribution (multivariate - depending on multiple variables). That also means that we can calculate the probability of a point being under a Gaussian â€˜bellâ€™, i.e.&nbsp;the probability of a point belonging to a cluster. A comparison between the performances of <span class="math inline">\(k\)</span>-means and EM clustering on an artificial dataset is shown in <a href="#fig-ClusterAnalysisMouse">Figure&nbsp;<span>4.1</span></a>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-ClusterAnalysisMouse" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="img/ClusterAnalysis_Mouse.png" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Figure 4.1: Comparison of <span class="math inline">\(k\)</span>-means and EM on artificial data called <a href="https://elki-project.github.io/datasets/">Mouse</a> dataset. Using the Variances, the EM algorithm can describe the normal distributions exact, while <span class="math inline">\(k\)</span>-means splits the data in <a href="https://bit.ly/1rVyJkt">Voronoi</a>-Cells.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>We start this chapter by reminding what is a Gaussian distribution, then introduce the Mixture of Gaussians and finish by explaining the Expectation-Maximization algorithm.</p>
<section id="the-gaussian-distribution" class="level2" data-number="4.1"><h2 data-number="4.1" class="anchored" data-anchor-id="the-gaussian-distribution">
<span class="header-section-number">4.1</span> The Gaussian distribution</h2>
<p>The Gaussian, also known as the normal distribution, is a widely used model for the distribution of continuous variables. In the case of a single variable <span class="math inline">\(x\)</span>, the Gaussian distribution can be written in the form</p>
<p><span id="eq-normaldist1"><span class="math display">\[
\mathcal{N}(x|m,\sigma^2)=\frac{1}{(2 \pi \sigma^2 )^{1/2}} \exp \left\lbrace - \frac{1}{2 \sigma^2} (x-m)^2\right\rbrace
\qquad(4.1)\]</span></span></p>
<p>where <span class="math inline">\(m\)</span> is the mean and <span class="math inline">\(\sigma^2\)</span> is the variance.</p>
<p>For a <span class="math inline">\(D\)</span>-dimensional vector <span class="math inline">\(X\)</span>, the multivariate Gaussian distribution take the form</p>
<p><span id="eq-normaldist2"><span class="math display">\[
\mathcal{N}(X|\mu,\Sigma)=\frac{1}{(2 \pi)^{D/2}} \frac{1}{|\Sigma|^{1/2}} \exp \left\lbrace - \frac{1}{2} (X-\mu)^T \Sigma^{-1} (X-\mu) \right\rbrace
\qquad(4.2)\]</span></span></p>
<p>where <span class="math inline">\(\mu\)</span> is a <span class="math inline">\(D\)</span>-dimensional mean vector, <span class="math inline">\(\Sigma\)</span> is a <span class="math inline">\(D\times D\)</span> covariance matrix, and <span class="math inline">\(|\Sigma|\)</span> denotes the determinant of <span class="math inline">\(\Sigma\)</span>.</p>
<p>The Gaussian distribution arises in many different contexts and can be motivated from a variety of different perspectives. For example, when we consider the sum of multiple random variables. The <em>central limit theorem</em> (due to Laplace) tells us that, subject to certain mild conditions, the sum of a set of random variables, which is of course itself a random variable, has a distribution that becomes increasingly Gaussian as the number of terms in the sum increases.</p>
</section><section id="mixture-of-gaussians" class="level2" data-number="4.2"><h2 data-number="4.2" class="anchored" data-anchor-id="mixture-of-gaussians">
<span class="header-section-number">4.2</span> Mixture of Gaussians</h2>
<p>While the Gaussian distribution has some important analytical properties, it suffers from significant limitations when it comes to modeling real data sets. Consider the example shown in <a href="#fig-gaussianfaithful">Figure&nbsp;<span>4.2</span></a> applied on the â€™Old Faithfulâ€™ data set, this data set comprises 272 measurements of the eruption of the <a href="https://en.wikipedia.org/wiki/Old_Faithful">Old Faithful geyser</a> at Yellowstone National Park in the USA. Each measurement comprises the duration of the eruption in minutes (horizontal axis) and the time in minutes to the next eruption (vertical axis). We see that the data set forms two dominant clumps, and that a simple Gaussian distribution is unable to capture this structure, whereas a linear superposition of two Gaussians gives a better characterization of the data set. Such superpositions, formed by taking linear combinations of more basic distributions such as Gaussians, can be formulated as probabilistic models known as <em>mixture distributions</em>.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-gaussianfaithful" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="img/gaussian_faithful.png" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Figure 4.2: Plots of the â€™old faithfulâ€™ data in which the blue curves show contours of constant probability density. On the left is a single Gaussian ditribution which has been fitted to the data using maximum likelihood. On the right the distribution is given by a linear combination of two Gaussians which has been fitted to the data by maximum likelihood using the EM technique, and which gives a better representation of the data</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>In <a href="#fig-gaussianmixture">Figure&nbsp;<span>4.3</span></a> we see that a linear combination of Gaussians can give rise to very complex densities. By using a sufficient number of Gaussians, and by adjusting their means and covariances as well as the coefficients in the linear combination, almost any continuous density can be approximated to arbitrary accuracy.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-gaussianmixture" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="img/gaussian_mixture.png" class="img-fluid figure-img" style="width:70.0%"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Figure 4.3: Example of a Gaussian mixture distribution in one dimension showing three Gaussians (each scaled by a coefficient) in blue and their sum in red.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>We therefore consider a superposition of <span class="math inline">\(K\)</span> Gaussian densities of the form</p>
<p><span id="eq-gaussian"><span class="math display">\[p(x)= \sum_{k=1}^{K} \pi_k \mathcal{N}(x| \mu_k, \Sigma_k)
\qquad(4.3)\]</span></span></p>
<p>which is called a <em>mixture of Gaussians</em>. Each Gaussian density <span class="math inline">\(\mathcal{N}(x| \mu_k, \Sigma_k)\)</span> is called a <em>component</em> of the mixture and has its own mean <span class="math inline">\(\mu_k\)</span> and covariance <span class="math inline">\(\Sigma_k\)</span>.</p>
<p>The parameters <span class="math inline">\(\pi_k\)</span> are called <em>mixing coefficients</em>. They verify the conditions</p>
<p><span class="math display">\[\sum_{k=1}^{K} \pi_k = 1 \quad \text{and} \quad 0 \leq \pi_k \leq 1\]</span></p>
<p>In order to find an equivalent formulation of the Gaussian mixture involving an explicit <strong>latent</strong> variable, we introduce a <span class="math inline">\(K\)</span>-dimensional binary random variable <span class="math inline">\(z\)</span> having a 1-of-<span class="math inline">\(K\)</span> representation in which a particular element <span class="math inline">\(z_k\)</span> is equal to 1 and all other elements are equal to 0. The values of <span class="math inline">\(z_k\)</span> therefore satisfy <span class="math inline">\(z_k \in \{0,1\}\)</span> and <span class="math inline">\(\sum_k z_k =1\)</span>, and we see that there are <span class="math inline">\(K\)</span> possible states for the vector <span class="math inline">\(z\)</span> according to which element is nonzero. The marginal distribution over <span class="math inline">\(z\)</span> is specified in terms of the mixing coefficients <span class="math inline">\(\pi_k\)</span> , such that <span class="math display">\[p(z_k=1)=\pi_k\]</span></p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Latent variable
</div>
</div>
<div class="callout-body-container callout-body">
<p>A <a href="https://en.wikipedia.org/wiki/Latent_variable" target="_blank">latent variable</a> is a variable that is not directly measurable, but its value can be inferred by taking other measurements.</p>
<p>This happens a lot in machine learning, robotics, statistics and other fields. For example, you may not be able to directly quantify intelligence (itâ€™s not a countable thing like the number of brain cells you have), but we think it exists and we can run experiments that may tell us about intelligence. So your intelligence is a latent variable that affects your performance on multiple tasks even though it can not be directly measured (<a href="https://www.quora.com/What-is-a-latent-variable" target="_blank">link</a>).</p>
</div>
</div>
<p>The conditional distribution of <span class="math inline">\(x\)</span> given a particular value for <span class="math inline">\(z\)</span> is a Gaussian</p>
<p><span class="math display">\[p(x|z_k=1)= \mathcal{N}(x| \mu_k, \Sigma_k)\]</span></p>
<p>The joint distribution is given by <span class="math inline">\(p(z)p(x|z)\)</span>, and the marginal distribution of <span class="math inline">\(x\)</span> is then obtained by summing the joint distribution over all possible states of <span class="math inline">\(z\)</span> to give</p>
<p><span class="math display">\[p(x)= \sum_z p(z)p(x|z) = \sum_{k=1}^{K} \pi_k \mathcal{N}(x| \mu_k, \Sigma_k)\]</span></p>
<p>Now, we are able to work with the joint distribution <span class="math inline">\(p(x|z)\)</span> instead of the marginal distribution <span class="math inline">\(p(x)\)</span>. This leads to significant simplification, most notably through the introduction of the Expectation-Maximization (EM) algorithm.</p>
<p>Another quantity that play an important role is the conditional probability of <span class="math inline">\(z\)</span> given <span class="math inline">\(x\)</span>. We shall use <span class="math inline">\(r(z_k)\)</span> to denote <span class="math inline">\(p(z_k = 1|x)\)</span>, whose value can be found using Bayesâ€™ theorem</p>
<p><span id="eq-responsibilities"><span class="math display">\[
\begin{align}
r(z_k)= p(z_k = 1|x) &amp;= \frac{ p(z_k = 1) p(x|z_k=1)}{\displaystyle \sum_{j=1}^{K} p(z_j = 1) p(x|z_j=1)} \notag \\
&amp;= \frac{\pi_k \mathcal{N}(x| \mu_k, \Sigma_k)}{\displaystyle \sum_{j=1}^{K} \pi_j \mathcal{N}(x| \mu_j, \Sigma_j)}
\end{align}
\qquad(4.4)\]</span></span></p>
<p>We shall view <span class="math inline">\(\pi_k\)</span> as the prior probability of <span class="math inline">\(z_k = 1\)</span>, and the quantity <span class="math inline">\(r(z_k)\)</span> as the corresponding posterior probability once we have observed <span class="math inline">\(x\)</span>. As we shall see in next section, <span class="math inline">\(r(z_k)\)</span> can also be viewed as the <em>responsibility</em> that component <span class="math inline">\(k\)</span> takes for â€™explainingâ€™ the observation <span class="math inline">\(x\)</span>.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Doesnâ€™t this reminds you of the Equation (<a href="discriminantanalysis.html#eq-bayes"><span>1.1</span></a>) when we used Bayesâ€™ theorm for Classification? where <span class="math inline">\(p_k(x)\)</span> was the <em>posterior</em> probability that an observation <span class="math inline">\(X=x\)</span> belongs to <span class="math inline">\(k\)</span>-th class. The difference is that here the data is unlabled (we have no class), so we create a <strong>latent</strong> (hidden, unobserved) variable <span class="math inline">\(z\)</span> that will play a similar role.</p>
</div>
</div>
<p>In <a href="#fig-gaussianmixture500samples">Figure&nbsp;<span>4.4</span></a> the role of the responsibilities is illustrated on a sample of 500 points drawn from a mixture of three Gaussians.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-gaussianmixture500samples" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="img/gaussianmixture_500samples.png" class="img-fluid figure-img" style="width:90.0%"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Figure 4.4: Example of 500 points drawn from a mixture of 3 Gaussians. (a) Samples from the joint distribution <span class="math inline">\(p(z)p(x|z)\)</span> in which the three states of <span class="math inline">\(z\)</span>, corresponding to the three components of the mixture, are depicted in red, green, and blue, and (b) the corresponding samples from the marginal distribution <span class="math inline">\(p(x)\)</span>, which is obtained by simply ignoring the values of <span class="math inline">\(z\)</span> and just plotting the <span class="math inline">\(x\)</span> values. The data set in (a) is said to be complete, whereas that in (b) is incomplete. (c) The same samples in which the colours represent the value of the responsibilities <span class="math inline">\(r(z_{nk})\)</span> associated with data point <span class="math inline">\(x_n\)</span>, obtained by plotting the corresponding point using proportions of red, blue, and green ink given by <span class="math inline">\(r(z_{nk})\)</span> for <span class="math inline">\(k = 1,2,3\)</span>, respectively.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>So the form of the Gaussian mixture distribution is governed by the parameters <span class="math inline">\(\pi\)</span>, <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\Sigma\)</span>, where we have used the notation <span class="math inline">\(\pi=\{\pi_1,\ldots,\pi_K\}\)</span>, <span class="math inline">\(\mu=\{\mu_1,\ldots,\mu_K\}\)</span> and <span class="math inline">\(\Sigma=\{\Sigma_1,\ldots,\Sigma_K\}\)</span>. One way to set the values of these parameters is to use maximum likelihood. The log of the likelihood function is given by</p>
<p><span class="math display">\[\ln p(X|\pi,\mu,\Sigma)=\sum_{n=1}^{N} \ln \left\lbrace \sum_{k=1}^{K} \pi_k \mathcal{N}(x_n | \mu_k, \Sigma_k) \right\rbrace\]</span></p>
<p>We immediately see that the situation is now much more complex than with a single Gaussian, due to the presence of the summation over <span class="math inline">\(k\)</span> inside the logarithm. As a result, the maximum likelihood solution for the parameters no longer has a closed-form analytical solution. One approach to maximizing the likelihood function is to use iterative numerical optimization techniques. Alternatively we can employ a powerful framework called <strong>E</strong>xpectation <strong>M</strong>aximization, which will be discussed in this chapter.</p>
</section><section id="em-for-gaussian-mixtures" class="level2" data-number="4.3"><h2 data-number="4.3" class="anchored" data-anchor-id="em-for-gaussian-mixtures">
<span class="header-section-number">4.3</span> EM for Gaussian Mixtures</h2>
<p>Suppose we have a data set of observations <span class="math inline">\(\{x_1, \ldots, x_N\}\)</span>, which gives a data set <span class="math inline">\(X\)</span> of size <span class="math inline">\(N \times D\)</span>, and we wish to model this data using a mixture of Gaussians. Similarly, the corresponding latent variable are denoted by a <span class="math inline">\(N \times K\)</span> matrix <span class="math inline">\(Z\)</span> with rows <span class="math inline">\(z_n^K\)</span>.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Recall that the objective is to estimate the parameters <span class="math inline">\(\pi\)</span>, <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\Sigma\)</span> in order to estimate the posterior probabilities (named also <em>responsibilities</em>, called <span class="math inline">\(\, r(z_k)\)</span> in this chapter). To do so, we find the estimators that maximize the log of the likelihood function.</p>
</div>
</div>
<p>If we assume that the data points are i.i.d. (independent and identically distributed), then we can calculate the log of the likelihood function, which is given by</p>
<p><span id="eq-gaussianlikelihood"><span class="math display">\[\ln p(X|\pi,\mu,\Sigma)=\sum_{n=1}^{N} \ln \left\lbrace \sum_{k=1}^{K} \pi_k \mathcal{N}(x_n | \mu_k, \Sigma_k) \right\rbrace \qquad(4.5)\]</span></span></p>
<p>An elegant and powerful method for finding maximum likelihood solutions for this models with latent variables is called the <strong>E</strong>xpectation <strong>M</strong>aximization algorithm, or EM algorithm.</p>
<p>Setting the derivatives of <span class="math inline">\(\ln p(X|\pi,\mu,\Sigma)\)</span> in (<a href="#eq-gaussianlikelihood"><span>4.5</span></a>) respectively with respect to the <span class="math inline">\(\mu_k,\Sigma_k\)</span> and <span class="math inline">\(\pi_k\)</span> to zero, we obtain</p>
<p><span id="eq-means"><span class="math display">\[\mu_k = \frac{1}{N_k} \sum_{n=1}^{N} r(z_{nk}) x_n  \qquad(4.6)\]</span></span></p>
<p>where we define <span class="math display">\[N_k= \sum_{n=1}^{N}r(z_{nk})\]</span></p>
<p>We can interpret <span class="math inline">\(N_k\)</span> as the effective number of points assigned to cluster <span class="math inline">\(k\)</span>. Note carefully the form of this solution. We see that the mean <span class="math inline">\(\mu_k\)</span> for the <span class="math inline">\(k\)</span>-th Gaussian component is obtained by taking a weighted mean of all of the points in the data set, in which the weighting factor for data point <span class="math inline">\(x_n\)</span> is given by the posterior probability <span class="math inline">\(r(z_{nk})\)</span> that component <span class="math inline">\(k\)</span> was responsible for generating <span class="math inline">\(x_n\)</span>.</p>
<p>As for <span class="math inline">\(\sigma_k\)</span> we obtain</p>
<p><span id="eq-sigma"><span class="math display">\[\Sigma_k= \frac{1}{N_k} \sum_{n=1}^{N} r(z_{nk}) (x_n - \mu_k)(x_n - \mu_k)^T
\qquad(4.7)\]</span></span></p>
<p>which has the same form as the corresponding result for a single Gaussian fitted to the data set, but again with each data point weighted by the corresponding posterior probability and with the denominator given by the effective number of points associated with the corresponding component.</p>
<p>Finally, for the mixing coefficients <span class="math inline">\(\pi_k\)</span> we obtain</p>
<p><span id="eq-pi"><span class="math display">\[
\pi_k=\frac{N_k}{N}
\qquad(4.8)\]</span></span></p>
<p>so that the mixing coefficient for the <span class="math inline">\(k\)</span>-th component is given by the average responsibility which that component takes for explaining the data points.</p>
<p>We first choose some initial values for the means, covariances, and mixing coefficients. Then we alternate between the following two updates that we shall call the <strong>E</strong> step and the <strong>M</strong> step. In the <em>expectation</em> step, or E step, we use the current values for the parameters to evaluate the posterior probabilities, or responsibilities, given by (<a href="#eq-responsibilities"><span>4.4</span></a>). We then use these probabilities in the <em>maximization</em> step, or M step, to re-estimate the means, covariances, and mixing coefficients using the results in Equations (<a href="#eq-means"><span>4.6</span></a>), (<a href="#eq-sigma"><span>4.7</span></a>) and (<a href="#eq-pi"><span>4.8</span></a>). The algorithm of EM for mixtures of Gaussians is shown in the following Algorithm:</p>
<table class="table">
<colgroup>
<col style="width: 37%">
<col style="width: 62%">
</colgroup>
<thead><tr class="header">
<th>The EM for Gaussian mixtures</th>
<th></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>
<strong>Data</strong>:</td>
<td>
<span class="math inline">\(\mathbf{X}= \{x_{kd}, \,\,\,\, k=1,\ldots,N, d=1,\ldots,D\}\)</span> where <span class="math inline">\(D\)</span> is the dimension of the feature space. <span class="math inline">\(Z\)</span> the latent variables matrix.</td>
</tr>
<tr class="even">
<td>
<strong>Result</strong>:</td>
<td>Posterior probabilities <span class="math inline">\(r(z_{nk})\)</span> and the model parameters <span class="math inline">\(\mu,\Sigma\)</span> and <span class="math inline">\(\pi\)</span>.</td>
</tr>
<tr class="odd">
<td>
<strong>Initialization</strong>:</td>
<td><ul>
<li>Choose a value for <span class="math inline">\(K\)</span>, <span class="math inline">\(1 &lt; K &lt; N\)</span>.</li>
<li>Initialize the means <span class="math inline">\(\mu_k\)</span>, the covariances <span class="math inline">\(\Sigma_k\)</span> and mixing coefficients <span class="math inline">\(\pi_k\)</span> randomly.</li>
<li>Evaluate the initial value of the log likelihood.</li>
</ul></td>
</tr>
<tr class="even">
<td>
<strong>Learning</strong>: <strong>repeat</strong>
</td>
<td>
<p><strong>E step</strong>:</p>
<ul>
<li>Evaluate the responsibilities using the current parameter values: <span class="math display">\[r(z_{nk})= \frac{\pi_k \mathcal{N}(x| \mu_k, \Sigma_k)}{\displaystyle \sum_{j=1}^{K} \pi_j \mathcal{N}(x| \mu_j, \Sigma_j)}\]</span>
</li>
</ul>
<p><strong>M step</strong>:</p>
<ul>
<li><p>Re-estimate the parameters using the current responsibilities: <span class="math display">\[\mu_k = \frac{1}{N_k} \sum_{n=1}^{N} r(z_{nk}) x_n\]</span> <span class="math display">\[\Sigma_k= \frac{1}{N_k} \sum_{n=1}^{N} r(z_{nk}) (x_n - \mu_k)(x_n - \mu_k)^T\]</span> <span class="math display">\[\pi_k=\frac{N_k}{N}\]</span> <span class="math display">\[\text{where} \quad N_k= \sum_{n=1}^{N}r(z_{nk})\]</span></p></li>
<li><p>Evaluate the log likelihood: <span class="math display">\[\ln p(X|\pi,\mu,\Sigma)=\sum_{n=1}^{N} \ln \left\lbrace \sum_{k=1}^{K} \pi_k \mathcal{N}(x_n | \mu_k, \Sigma_k) \right\rbrace\]</span> Until convergence of either the parameters or the log likelihood. If the convergence criterion is not satisfied return to E step.</p></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>The EM algorithm for a mixture of two Gaussians applied to the rescaled Old Faithful data set is illustrated in <a href="#fig-emfaithful">Figure&nbsp;<span>4.5</span></a>. In plot (a) we see the initial configuration, the Gaussian component are shown as blue and red circles. Plot (b) shows the result of the initial E step where we update the responsibilities. Plot (c) shows the M step where we update the parameters. Plots (d), (e), and (f) show the results after 2, 5, and 20 complete cycles of EM, respectively. In plot (f) the algorithm is close to convergence.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-emfaithful" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="img/em_faithful.png" class="img-fluid figure-img" style="width:80.0%"></p>
<p></p><figcaption aria-hidden="true" class="figure-caption">Figure 4.5: Illustration of the EM algorithm using the Old Faithful dataset. A mixture of two Gaussians is used.</figcaption><p></p>
</figure>
</div>
</div>
</div>

<!-- 
## The EM Algorithm in General

In this section, we present the general view of the EM algorithm. The
goal of the EM algorithm is to find maximum likelihood solutions for
models having latent variables. We denote $X$ the data matrix, $Z$ the
latent variables matrix. Let us denote $\theta$ the set of all model
parameters. Then the log likelihood function is given by



$$\ln p(X|\theta) = \ln \left\lbrace \sum_Z p(X,Z|\theta) \right\rbrace$$



Note that if the latent variables are continuous we get similar
equations, we only replace the over $Z$ with an integral.

The presence of the sum prevents the logarithm from acting directly on
the joint distribution, resulting in complicated expressions for the
maximum likelihood solution.

Suppose that, for each observation in $X$, we were told the
corresponding value of the latent variable $Z$. We shall call $\{X,Z\}$
the *complete* data set, and we shall refer to the actual observed data
$X$ as *incomplete*. The likelihood function for the complete data set
simply takes the form $\ln p(X,Z|\theta)$, and we shall suppose that
maximization of this complete-data log likelihood function is
straightforward.

**Initialization**:\
-Choose an initial setting for the parameters $\theta^{\text{old}}$.\
**Learning**: repeat\
Until convergence of either the parameters or the log likelihood. If the
convergence criterion is not satisfied then let $$\theta^{\text{old}}
\leftarrow \theta^{\text{new}}$$ and return to the E step.

In practice, however, we are not given the complete data set $\{X,Z\}$,
but only the incomplete data $X$. Our state of knowledge of the values
of the latent variables in $Z$ is given only by the posterior
distribution $p(Z|X, \theta)$. Because we cannot use the complete-data
log likelihood, we consider instead its expected value under the
posterior distribution of the latent variable, which corresponds to the
E step of the EM algorithm. In the subsequent M step, we maximize this
expectation. If the current estimate for the parameters is denoted
$\theta^{\text{old}}$, then a pair of successive E and M steps gives
rise to a revised estimate $\theta^{\text{new}}$. The algorithm is
initialized by choosing some starting value for the parameters
$\theta_0$.

In the E step, we use the current parameter values $\theta^{\text{old}}$
to find the posterior distribution of the latent variables given by
$p(Z|X, \theta^{\text{old}})$. We then use this posterior distribution
to find the expectation of the complete-data log likelihood evaluated
for some general parameter value $\theta$. This expectation, denoted
$\mathcal{Q}(\theta,\theta^{\text{old}})$, is given by



$$\label{eq:complete-likelihood}
 \mathcal{Q}(\theta,\theta^{\text{old}}) = \sum_Z p(Z|X, \theta^{\text{old}}) \ln p(X,Z|\theta)$$

In the M step, we determine the revised parameter estimate
$\theta^{\text{new}}$ by maximizing this function



$$\theta^{\text{new}} = \argmax_{\theta}  \mathcal{Q}(\theta,\theta^{\text{old}})$$



Note that in the definition of
$\mathcal{Q}(\theta,\theta^{\text{old}})$, the logarithm acts directly
on the joint distribution $p(X,Z|\theta)$, so the corresponding M-step
maximization will, by supposition, be tractable.

The general EM algorithm is summarized in Algorithm \[algo:em:general\].
It has the property that each cycle of EM will increase the
incomplete-data log likelihood (unless it is already at a local
maximum). -->
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Summary
</div>
</div>
<div class="callout-body-container callout-body">
<p>Summary of this chapter:</p>
<ul>
<li>Gaussian Mixture Models (GMM) take a Gaussian and add another Gaussian(s).</li>
<li>This allows to model more complex data.</li>
<li>We fit a GMM with the Expectation-Maximization (EM) algorithm.</li>
<li>Expectation-Maximization (EM) algorithm is a series of steps to find good parameter estimates when there are latent variables.</li>
<li>EM steps:
<ol type="1">
<li>Initialize t he parameter estimates.</li>
<li>Given the current parameter estimates, find the minimum log likelihood for <span class="math inline">\(Z\)</span> (data + latent variables).</li>
<li>Givent the current data, find better parameter estimates.</li>
<li>Repeat steps 2 &amp; 3.</li>
</ol>
</li>
</ul>
</div>
</div>
<p align="right">
â—¼
</p>


</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
    } else {
      disableStylesheet(alternateStylesheets);
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } 
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./Lab-kmeans.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">- ðŸ’» Lab</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./Lab-EM.html" class="pagination-link">
        <span class="nav-page-text">- ðŸ’» Lab</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>