<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Machine Learning</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Machine Learning course">
  <meta name="generator" content="bookdown 0.2 and GitBook 2.6.7">

  <meta property="og:title" content="Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Machine Learning course" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Machine Learning" />
  
  <meta name="twitter:description" content="Machine Learning course" />
  

<meta name="author" content="Mohamad Ghassany">


<meta name="date" content="2016-12-15">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="pw-2.html">
<link rel="next" href="pw-3.html">

<script src="book_assets/jquery-2.2.3/jquery.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">ESILV Intoduction to Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a><ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#what-is-machine-learning"><i class="fa fa-check"></i>What is Machine Learning ?</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#supervised-learning"><i class="fa fa-check"></i>Supervised Learning</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#unsupervised-learning"><i class="fa fa-check"></i>Unsupervised Learning</a></li>
</ul></li>
<li class="part"><span><b>Supervised Learning</b></span></li>
<li class="part"><span><b>Regression</b></span></li>
<li class="chapter" data-level="1" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>1</b> Linear Regression</a><ul>
<li class="chapter" data-level="1.1" data-path="linear-regression.html"><a href="linear-regression.html#notation"><i class="fa fa-check"></i><b>1.1</b> Notation</a></li>
<li class="chapter" data-level="1.2" data-path="linear-regression.html"><a href="linear-regression.html#model-representation"><i class="fa fa-check"></i><b>1.2</b> Model Representation</a></li>
<li class="chapter" data-level="1.3" data-path="linear-regression.html"><a href="linear-regression.html#why-estimate-f"><i class="fa fa-check"></i><b>1.3</b> Why Estimate <span class="math inline">\(f\)</span> ?</a><ul>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html#prediction"><i class="fa fa-check"></i>Prediction</a></li>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html#inference"><i class="fa fa-check"></i>Inference</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="linear-regression.html"><a href="linear-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>1.4</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="1.4.1" data-path="linear-regression.html"><a href="linear-regression.html#model"><i class="fa fa-check"></i><b>1.4.1</b> Model</a></li>
<li class="chapter" data-level="1.4.2" data-path="linear-regression.html"><a href="linear-regression.html#estimating-the-coefficients"><i class="fa fa-check"></i><b>1.4.2</b> Estimating the Coefficients</a></li>
<li class="chapter" data-level="1.4.3" data-path="linear-regression.html"><a href="linear-regression.html#assessing-the-accuracy-of-the-coefficient-estimates"><i class="fa fa-check"></i><b>1.4.3</b> Assessing the Accuracy of the Coefficient Estimates</a></li>
<li class="chapter" data-level="1.4.4" data-path="linear-regression.html"><a href="linear-regression.html#assessing-the-accuracy-of-the-model"><i class="fa fa-check"></i><b>1.4.4</b> Assessing the Accuracy of the Model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="pw-1.html"><a href="pw-1.html"><i class="fa fa-check"></i>PW 1</a><ul>
<li class="chapter" data-level="" data-path="pw-1.html"><a href="pw-1.html#how-to-use-r"><i class="fa fa-check"></i>How to use R</a></li>
<li class="chapter" data-level="" data-path="pw-1.html"><a href="pw-1.html#get-familiar-with-r"><i class="fa fa-check"></i>Get familiar with R</a></li>
<li class="chapter" data-level="" data-path="pw-1.html"><a href="pw-1.html#linear-regression-1"><i class="fa fa-check"></i>Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>2</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#the-model"><i class="fa fa-check"></i><b>2.1</b> The Model</a></li>
<li class="chapter" data-level="2.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#estimating-the-regression-coefficients"><i class="fa fa-check"></i><b>2.2</b> Estimating the Regression Coefficients</a></li>
<li class="chapter" data-level="2.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#some-important-questions"><i class="fa fa-check"></i><b>2.3</b> Some important questions</a><ul>
<li class="chapter" data-level="2.3.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#other-considerations-in-regression-model"><i class="fa fa-check"></i><b>2.3.1</b> Other Considerations in Regression Model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="pw-2.html"><a href="pw-2.html"><i class="fa fa-check"></i>PW 2</a><ul>
<li class="chapter" data-level="" data-path="pw-2.html"><a href="pw-2.html#rmarkdown"><i class="fa fa-check"></i>RMarkdown</a></li>
<li class="chapter" data-level="" data-path="pw-2.html"><a href="pw-2.html#multiple-linear-regression-1"><i class="fa fa-check"></i>Multiple Linear Regression</a></li>
</ul></li>
<li class="part"><span><b>Classification</b></span></li>
<li class="chapter" data-level="3" data-path="logistic-regression-k-nn.html"><a href="logistic-regression-k-nn.html"><i class="fa fa-check"></i><b>3</b> Logistic Regression + K-NN</a><ul>
<li class="chapter" data-level="3.1" data-path="logistic-regression-k-nn.html"><a href="logistic-regression-k-nn.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="logistic-regression-k-nn.html"><a href="logistic-regression-k-nn.html#logistic-regression"><i class="fa fa-check"></i><b>3.2</b> Logistic Regression</a><ul>
<li class="chapter" data-level="3.2.1" data-path="logistic-regression-k-nn.html"><a href="logistic-regression-k-nn.html#the-logistic-model"><i class="fa fa-check"></i><b>3.2.1</b> The Logistic Model</a></li>
<li class="chapter" data-level="3.2.2" data-path="logistic-regression-k-nn.html"><a href="logistic-regression-k-nn.html#estimating-the-regression-coefficients-1"><i class="fa fa-check"></i><b>3.2.2</b> Estimating the Regression Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="logistic-regression-k-nn.html"><a href="logistic-regression-k-nn.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>3.3</b> Multiple Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="pw-3.html"><a href="pw-3.html"><i class="fa fa-check"></i>PW 3</a><ul>
<li class="chapter" data-level="" data-path="pw-3.html"><a href="pw-3.html#subsection"><i class="fa fa-check"></i>Subsection</a></li>
<li class="chapter" data-level="" data-path="pw-3.html"><a href="pw-3.html#subsection-1"><i class="fa fa-check"></i>Subsection</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html"><i class="fa fa-check"></i><b>4</b> Discriminant Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#subsection-1-1"><i class="fa fa-check"></i><b>4.1</b> Subsection 1</a></li>
<li class="chapter" data-level="4.2" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#subsection-2"><i class="fa fa-check"></i><b>4.2</b> Subsection 2</a><ul>
<li class="chapter" data-level="4.2.1" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#subsection-3"><i class="fa fa-check"></i><b>4.2.1</b>  Subsection 3</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="pw-4.html"><a href="pw-4.html"><i class="fa fa-check"></i>PW 4</a><ul>
<li class="chapter" data-level="" data-path="pw-4.html"><a href="pw-4.html#subsection-4"><i class="fa fa-check"></i>Subsection</a></li>
<li class="chapter" data-level="" data-path="pw-4.html"><a href="pw-4.html#subsection-5"><i class="fa fa-check"></i>Subsection</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>5</b> Support Vector Machines</a><ul>
<li class="chapter" data-level="5.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#subsection-1-2"><i class="fa fa-check"></i><b>5.1</b> Subsection 1</a></li>
<li class="chapter" data-level="5.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#subsection-2-1"><i class="fa fa-check"></i><b>5.2</b> Subsection 2</a><ul>
<li class="chapter" data-level="5.2.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#subsection-3-1"><i class="fa fa-check"></i><b>5.2.1</b>  Subsection 3</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="pw-5.html"><a href="pw-5.html"><i class="fa fa-check"></i>PW 5</a><ul>
<li class="chapter" data-level="" data-path="pw-5.html"><a href="pw-5.html#subsection-6"><i class="fa fa-check"></i>Subsection</a></li>
<li class="chapter" data-level="" data-path="pw-5.html"><a href="pw-5.html#subsection-7"><i class="fa fa-check"></i>Subsection</a></li>
</ul></li>
<li class="part"><span><b>Unsupervised Learning</b></span></li>
<li class="chapter" data-level="6" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html"><i class="fa fa-check"></i><b>6</b> Dimensionality Reduction</a><ul>
<li class="chapter" data-level="6.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#subsection-1-3"><i class="fa fa-check"></i><b>6.1</b> Subsection 1</a></li>
<li class="chapter" data-level="6.2" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#subsection-2-2"><i class="fa fa-check"></i><b>6.2</b> Subsection 2</a><ul>
<li class="chapter" data-level="6.2.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#subsection-3-2"><i class="fa fa-check"></i><b>6.2.1</b>  Subsection 3</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="pw-6.html"><a href="pw-6.html"><i class="fa fa-check"></i>PW 6</a><ul>
<li class="chapter" data-level="" data-path="pw-6.html"><a href="pw-6.html#subsection-8"><i class="fa fa-check"></i>Subsection</a></li>
<li class="chapter" data-level="" data-path="pw-6.html"><a href="pw-6.html#subsection-9"><i class="fa fa-check"></i>Subsection</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="clustering-kmeans.html"><a href="clustering-kmeans.html"><i class="fa fa-check"></i><b>7</b> Clustering: Kmeans</a><ul>
<li class="chapter" data-level="7.1" data-path="clustering-kmeans.html"><a href="clustering-kmeans.html#subsection-1-4"><i class="fa fa-check"></i><b>7.1</b> Subsection 1</a></li>
<li class="chapter" data-level="7.2" data-path="clustering-kmeans.html"><a href="clustering-kmeans.html#subsection-2-3"><i class="fa fa-check"></i><b>7.2</b> Subsection 2</a><ul>
<li class="chapter" data-level="7.2.1" data-path="clustering-kmeans.html"><a href="clustering-kmeans.html#subsection-3-3"><i class="fa fa-check"></i><b>7.2.1</b>  Subsection 3</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="pw-7.html"><a href="pw-7.html"><i class="fa fa-check"></i>PW 7</a><ul>
<li class="chapter" data-level="" data-path="pw-7.html"><a href="pw-7.html#subsection-10"><i class="fa fa-check"></i>Subsection</a></li>
<li class="chapter" data-level="" data-path="pw-7.html"><a href="pw-7.html#subsection-11"><i class="fa fa-check"></i>Subsection</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="expectation-maximisation.html"><a href="expectation-maximisation.html"><i class="fa fa-check"></i><b>8</b> Expectation Maximisation</a><ul>
<li class="chapter" data-level="8.1" data-path="expectation-maximisation.html"><a href="expectation-maximisation.html#subsection-1-5"><i class="fa fa-check"></i><b>8.1</b> Subsection 1</a></li>
<li class="chapter" data-level="8.2" data-path="expectation-maximisation.html"><a href="expectation-maximisation.html#subsection-2-4"><i class="fa fa-check"></i><b>8.2</b> Subsection 2</a><ul>
<li class="chapter" data-level="8.2.1" data-path="expectation-maximisation.html"><a href="expectation-maximisation.html#subsection-3-4"><i class="fa fa-check"></i><b>8.2.1</b>  Subsection 3</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="pw-8.html"><a href="pw-8.html"><i class="fa fa-check"></i>PW 8</a><ul>
<li class="chapter" data-level="" data-path="pw-8.html"><a href="pw-8.html#subsection-12"><i class="fa fa-check"></i>Subsection</a></li>
<li class="chapter" data-level="" data-path="pw-8.html"><a href="pw-8.html#subsection-13"><i class="fa fa-check"></i>Subsection</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-resources.html"><a href="references-resources.html"><i class="fa fa-check"></i>References &amp; Resources</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="logistic-regression-k-nn" class="section level1">
<h1><span class="header-section-number">3</span> Logistic Regression + K-NN</h1>
<div id="introduction-1" class="section level2">
<h2><span class="header-section-number">3.1</span> Introduction</h2>
<p>The linear regression model discussed in the previous two chapters assumes that the response variable <span class="math inline">\(Y\)</span> is quantitative. But in many situations, the response variable is instead qualitative (categorical). For example, eye color is qualitative, taking on values blue, brown, or green.</p>
<p>The process for predicting qualitative responses is known as <strong><em>classification</em></strong>.</p>
<p>Given a feature vector <span class="math inline">\(X\)</span> and a qualitative response <span class="math inline">\(Y\)</span> taking values in the set <span class="math inline">\(\mathcal{C}\)</span>, the classification task is to build a function <span class="math inline">\(C(X)\)</span> that takes as input the feature vector <span class="math inline">\(X\)</span> and predicts its value for <span class="math inline">\(Y\)</span>; i.e. <span class="math inline">\(C(X) \in \mathcal{C}\)</span>. We are often more interested in estimating the probabilities that <span class="math inline">\(X\)</span> belongs to each category in <span class="math inline">\(\mathcal{C}\)</span>.</p>
<blockquote>
<p>If <span class="math inline">\(c\)</span> is a category (<span class="math inline">\(c \in \mathcal{C}\)</span>), by the probability that <span class="math inline">\(X\)</span> belongs to <span class="math inline">\(c\)</span> we mean <span class="math inline">\(p(X \in c) = \text{Pr}(Y=c|X)\)</span>.</p>
</blockquote>
<p><font color="red"> Now take the example of Andrew </font></p>
</div>
<div id="logistic-regression" class="section level2">
<h2><span class="header-section-number">3.2</span> Logistic Regression</h2>
<p>Consider a data set where the response falls into one of two categories, Yes or No. Rather than modeling the response <span class="math inline">\(Y\)</span> directly, logistic regression models the <em>probability</em> that <span class="math inline">\(Y\)</span> belongs to a particular category.</p>
<div id="the-logistic-model" class="section level3">
<h3><span class="header-section-number">3.2.1</span> The Logistic Model</h3>
<p>Let us suppose the response has two categories and we use the generic 0/1 coding for the response. How should we model the relationship between <span class="math inline">\(p(X) = \text{Pr}(Y = 1|X)\)</span> and <span class="math inline">\(X\)</span>?</p>
<p><em>Why not linear regression?</em> Any time a straight line is fit to a binary response that is coded as 0 or 1, in principle we can always predict <span class="math inline">\(p(X) &lt; 0\)</span> for some values of <span class="math inline">\(X\)</span> and <span class="math inline">\(p(X) &gt; 1\)</span> for others (unless the range of <span class="math inline">\(X\)</span> is limited).</p>
<p>To avoid this problem, we must model <span class="math inline">\(p(X)\)</span> using a function that gives outputs between 0 and 1 for all values of <span class="math inline">\(X\)</span>. Many functions meet this description. In logistic regression, we use the <em>logistic function</em>,</p>
<p><span class="math display">\[ p(X) = \frac{e^{\beta_0 + \beta_1 X}}{1+e^{\beta_0 + \beta_1 X}} \]</span></p>
<blockquote>
<p>No matter what values <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span> or <span class="math inline">\(X\)</span> take, <span class="math inline">\(p(X)\)</span> will have values between 0 and 1.</p>
</blockquote>
<blockquote>
<p>The logistic function will always produce an <em>S-shaped</em> curve.</p>
</blockquote>
<p>After a bit of manipulation of the previous equation, we find that</p>
<p><span class="math display">\[ \frac{p(X)}{1-p(X)} = e^{\beta_0 + \beta_1 X} \]</span></p>
<blockquote>
<p>The quantity <span class="math inline">\(p(X)/[1−p(X)]\)</span> is called the <em>odds</em>, and can take on any value between <span class="math inline">\(0\)</span> and <span class="math inline">\(\infty\)</span>.</p>
</blockquote>
<p>By taking the logarithm of both sides of the equation, we arrive at</p>
<p><span class="math display">\[ \log( \frac{p(X)}{1-p(X)} ) = \beta_0 + \beta_1 X \]</span></p>
<blockquote>
<p>The left-hand side is called the <em>log-odds</em> or <em>logit</em>. We see that the logistic regression model has a logit that is linear in X.</p>
</blockquote>
</div>
<div id="estimating-the-regression-coefficients-1" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Estimating the Regression Coefficients</h3>
<p>We estimate <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> using the <em>maximum likelihood</em> method. The basic intuition behind using maximum likelihood to fit a logistic regression model is as follows: we seek estimates for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> such that the predicted probability <span class="math inline">\(\hat{p}(x_i)\)</span> of the response for each individual, corresponds as closely as possible to the individual’s observed response status (recall that the response <span class="math inline">\(Y\)</span> is categorical). The <em>likelihood function</em> is</p>
<p><span class="math display">\[ l(\beta_0,\beta_1) = \prod_{i:y=1} p(x_i) \prod_{i&#39;:y=0} (1-p(x_{i&#39;})) \]</span></p>
<p>This likelihood gives the probability of the observed zeros and ones in the data. The estimates <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span> are chosen to <em>maximize</em> this likelihood function.</p>
<p>Some remarks: * <span class="math inline">\(p(x_i)\)</span> is the probability of <span class="math inline">\(x_i\)</span> is 1. And <span class="math inline">\(1-p(x_{i})\)</span> is the probability of <span class="math inline">\(x_i\)</span> is 0. * The likelihood expression written in the equation above is the joint probability of the observed sequence of 0’s and 1’s. * We suppose that the <span class="math inline">\(x_i\)</span>’s are independent.</p>
<blockquote>
<p>In the linear regression setting, the least squares approach is a special case of maximum likelihood.</p>
</blockquote>
<p>We will not give mathematical details about the maximum likelihood and how to estimate the parameters. We will use R to fit the logistic regression models (using <code>glm</code> function).</p>
<p><strong>Exapmle</strong></p>
<table>
<thead>
<tr class="header">
<th></th>
<th>Coefficient</th>
<th>Std. error</th>
<th><span class="math inline">\(Z\)</span>-statistic</th>
<th>p-value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Constant</td>
<td>-10.6513</td>
<td>0.3612</td>
<td>-29.5</td>
<td>&lt;0.0001</td>
</tr>
<tr class="even">
<td><span class="math inline">\(X\)</span></td>
<td>0.0055</td>
<td>0.0002</td>
<td>24.9</td>
<td>&lt;0.0001</td>
</tr>
</tbody>
</table>
<p>In this example, <span class="math inline">\(\hat{\beta_0} = -10.6513\)</span> and <span class="math inline">\(\hat{\beta_1} = 0.0055\)</span>. It produces the blue curve that separates that data in the following figure,</p>
<div class="figure">
<img src="img/lr_example.png" />

</div>
<p>As for prediction, we use the model built with the estimated parameters to predict probabilities. For example,</p>
<p>If <span class="math inline">\(X=1000\)</span>,</p>
<p><span class="math display">\[ \hat{p}(X) = \frac{e^{\hat{\beta_0} + \hat{\beta_1} X}}{1+e^{\hat{\beta_0} + \hat{\beta_1} X}} = \frac{e^{-10.6513+0.0055 \times 1000}}{1+e^{-10.6513+0.0055 \times 1000}} = 0.006\]</span></p>
<p>If <span class="math inline">\(X=2000\)</span>,</p>
<p><span class="math display">\[ \hat{p}(X) = \frac{e^{\hat{\beta_0} + \hat{\beta_1} X}}{1+e^{\hat{\beta_0} + \hat{\beta_1} X}} = \frac{e^{-10.6513+0.0055 \times 2000}}{1+e^{-10.6513+0.0055 \times 2000}} = 0.586\]</span></p>
</div>
</div>
<div id="multiple-logistic-regression" class="section level2">
<h2><span class="header-section-number">3.3</span> Multiple Logistic Regression</h2>
<p>We now consider the problem of predicting a binary response using multiple predictors. By analogy with the extension from simple to multiple linear regression in the previous chapters, we can generalize the simple logistic regression equation as follows:</p>
<p><span class="math display">\[ \log( \frac{p(X)}{1-p(X)} ) = \beta_0 + \beta_1 X_1 +  \ldots + \beta_p X_p\]</span></p>
<p>where <span class="math inline">\(X=(X_1,\ldots,X_p)\)</span> are <span class="math inline">\(p\)</span> predictors. The equation above can be rewritten as</p>
<p><span class="math display">\[ p(X) = \frac{e^{\beta_0 + \beta_1 X_1 +  \ldots + \beta_p X_p}}{1+e^{\beta_0 + \beta_1 X_1 +  \ldots + \beta_p X_p}} \]</span></p>
<p>Just as in the simple logistic regression we use the maximum likelihood method to estimate <span class="math inline">\(\beta_0,\beta_1,\ldots,\beta_p\)</span>.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="pw-2.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="pw-3.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["Machine-Learning.pdf"],
"toc": {
"collapse": "section"
},
"toolbar": {
"position": "fixed"
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
