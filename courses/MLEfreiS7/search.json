[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Overview\nWelcome! In this course you will learn about the state of the art of Machine Learning and also gain practice implementing and deploying machine learning algorithms.\nThis course is destined for students of Data Science Filière in EFREI Paris engineering school. In Data Science Filière there is the following master programs:\nThe aim of Machine Learning is to build computer systems that can adapt to their environments and learn form experience. Learning techniques and methods from this field are successfully applied to a variety of learning tasks in a broad range of areas, including, for example, spam recognition, text classification, gene discovery, financial forecasting. The course will give an overview of many concepts, techniques, and algorithms in machine learning, beginning with topics such as linear regression and classification and ending up with topics such as kmeans and Expectation Maximization. The course will give you the basic ideas and intuition behind these methods, as well as a more formal statistical and computational understanding. You will have an opportunity to experiment with machine learning techniques in R and apply them to a selected problem."
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Machine Learning",
    "section": "Schedule",
    "text": "Schedule\n\n\n\nSession\nTopic\nSlides\nLab\n\n\n\n\n1\nIntroduction to ML  Regression\n📖\n💻\n\n\n2\nClassification: Logistic Regression & Regularization\n📖\n💻\n\n\n3\nDecision Trees & Random Forests\n📖\n💻\n\n\n4\nIntroduction to Neural Networks & Deep Learning\n📖\n💻\n\n\n5\nProject\n\n💻"
  },
  {
    "objectID": "TD1.html#python-environment",
    "href": "TD1.html#python-environment",
    "title": "Lab1: Linear Regression",
    "section": "Python environment",
    "text": "Python environment\n\n\n\n\n\n\nAnaconda\n\n\n\nDuring this course we are going to use Python as programming language. Anaconda is an open-source distribution for Python. It is used for data science, machine learning, deep learning, etc. It comes with more than 300 libraries for data science. Anaconda helps in simplified package management and deployment.\nTo install it, go to Anaconda website.\nRemark: if you have a Mac with M1 ship, you must install the 2022.05 release of Anaconda: (Anaconda Distribution Now Supporting M1).\n\n\n\n\n\n\n\n\nJupyter\n\n\n\nDuring the labs, you must use Jupyter notebooks. The Jupyter Notebook is the original web application for creating and sharing computational documents. It offers a simple, streamlined, document-centric experience. Jupyter is installed by default when you install Anaconda. You can create notebooks using JupyterLab via your browser or using a text editor like VScode."
  },
  {
    "objectID": "TD1.html#predicting-house-value-boston-dataset",
    "href": "TD1.html#predicting-house-value-boston-dataset",
    "title": "Lab1: Linear Regression",
    "section": "Predicting House Value: Boston dataset",
    "text": "Predicting House Value: Boston dataset\nIn this lab we are going to use a dataset called Boston. It records the median value of houses for 506 neighborhoods around Boston. Our task is to predict the median house value.\nLoading Data\n\n\n\n\n\n\nBoston dataset\n\n\n\nThe dataset is available in scikit-learn or also here 🔗. Notice that the format/approach is not the same. You are free to use any of them, it is up to you to adapt your codes correctly.\nThere is mainly two approaches you need to know for instance:\n\nThe features and the target variable are in the same dataframe. In this case you can use the argument formula = target ~ features in certain fitting functions (like in ols(), imitating R’s programming language functions).\nThe features and the target variable are separated in X and y.\n\n\n\n\n1. Load these necessary libraries for this lab (install them if needed).\n\nimport numpy as np\nimport matplotlib.pyplot as plt \n\nimport pandas as pd  \nimport seaborn as sns \n\n# Run the following line to obtain the matplotlib figures in the notebook\n%matplotlib inline\n\n# We will also use sklearn but we will load the necessary modules when needed\n\n\n\n\n2. Load the Boston dataset.\n\nfrom sklearn.datasets import load_boston\nboston_dataset = load_boston()\n\n3. Print the value of the boston_dataset to understand what it contains.\n\nprint(boston_dataset.keys())\n\ndict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename', 'data_module'])\n\n\n\ndata: contains the information for various houses\ntarget: prices of the house\nfeature_names: names of the features\nDESCR: describes the dataset\n\nTo know more about the features run boston_dataset.DESCR.\nThe prices of the house indicated by the variable MEDV is our target variable and the remaining are the feature variables based on which we will predict the median value of houses in a district.\n3. Load the data into a pandas dataframe using pd.DataFrame. Then print the first 5 rows of the data using head().\n\nboston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\nboston.head()\n\n      CRIM    ZN  INDUS  CHAS    NOX  ...  RAD    TAX  PTRATIO       B  LSTAT\n0  0.00632  18.0   2.31   0.0  0.538  ...  1.0  296.0     15.3  396.90   4.98\n1  0.02731   0.0   7.07   0.0  0.469  ...  2.0  242.0     17.8  396.90   9.14\n2  0.02729   0.0   7.07   0.0  0.469  ...  2.0  242.0     17.8  392.83   4.03\n3  0.03237   0.0   2.18   0.0  0.458  ...  3.0  222.0     18.7  394.63   2.94\n4  0.06905   0.0   2.18   0.0  0.458  ...  3.0  222.0     18.7  396.90   5.33\n\n[5 rows x 13 columns]\n\n\nWe can see that the target value MEDV is missing from the data. We create a new column of target values and add it to the dataframe.\n\nboston['MEDV'] = boston_dataset.target\n\nRemark: the previous steps we evitable if we loaded the data from csv given above using pd.read_csv()."
  },
  {
    "objectID": "TD1.html#data-preprocessing",
    "href": "TD1.html#data-preprocessing",
    "title": "Lab1: Linear Regression",
    "section": "Data preprocessing",
    "text": "Data preprocessing\n4. Check if there are any missing values in the data.\n\n\n\n\n\n\nSolution (Click to expand)\n\n\n\n\n\n\nboston.isnull().sum()\n\nCRIM       0\nZN         0\nINDUS      0\nCHAS       0\nNOX        0\nRM         0\nAGE        0\nDIS        0\nRAD        0\nTAX        0\nPTRATIO    0\nB          0\nLSTAT      0\nMEDV       0\ndtype: int64\n\n\n\n\n\nExploratory Data Analysis\nExploratory Data Analysis is a very important step before training the model. In this section, we will use some visualizations to understand the relationship of the target variable with other features.\n5. Plot the distribution of the target variable MEDV. You can use the distplot() function from the seaborn library.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsns.distplot(boston['MEDV'], bins=30)\nplt.title('Distribution of the target value')\nplt.show()\n\n\n\n\n\n\n\n6. Calculate the correlation matrix and visualize it (you may use heatmap() from seaborn library). Name the features that are highly correlated with the target variable.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nboston.corr()\n\n             CRIM        ZN     INDUS  ...         B     LSTAT      MEDV\nCRIM     1.000000 -0.200469  0.406583  ... -0.385064  0.455621 -0.388305\nZN      -0.200469  1.000000 -0.533828  ...  0.175520 -0.412995  0.360445\nINDUS    0.406583 -0.533828  1.000000  ... -0.356977  0.603800 -0.483725\nCHAS    -0.055892 -0.042697  0.062938  ...  0.048788 -0.053929  0.175260\nNOX      0.420972 -0.516604  0.763651  ... -0.380051  0.590879 -0.427321\nRM      -0.219247  0.311991 -0.391676  ...  0.128069 -0.613808  0.695360\nAGE      0.352734 -0.569537  0.644779  ... -0.273534  0.602339 -0.376955\nDIS     -0.379670  0.664408 -0.708027  ...  0.291512 -0.496996  0.249929\nRAD      0.625505 -0.311948  0.595129  ... -0.444413  0.488676 -0.381626\nTAX      0.582764 -0.314563  0.720760  ... -0.441808  0.543993 -0.468536\nPTRATIO  0.289946 -0.391679  0.383248  ... -0.177383  0.374044 -0.507787\nB       -0.385064  0.175520 -0.356977  ...  1.000000 -0.366087  0.333461\nLSTAT    0.455621 -0.412995  0.603800  ... -0.366087  1.000000 -0.737663\nMEDV    -0.388305  0.360445 -0.483725  ...  0.333461 -0.737663  1.000000\n\n[14 rows x 14 columns]\n\n\n\ncorrelation_matrix = boston.corr().round(2)\n# annot = True to print the values inside the square\nsns.heatmap(data=correlation_matrix, annot=True)\n\n\n\n\n\ncorrelation_matrix['MEDV'].sort_values(ascending=False)\n\nMEDV       1.00\nRM         0.70\nZN         0.36\nB          0.33\nDIS        0.25\nCHAS       0.18\nAGE       -0.38\nRAD       -0.38\nCRIM      -0.39\nNOX       -0.43\nTAX       -0.47\nINDUS     -0.48\nPTRATIO   -0.51\nLSTAT     -0.74\nName: MEDV, dtype: float64\n\n\nThe features that are highly correlated with the target variable are RM and LSTAT where\n\n\nRM has a strong positive correlation with MEDV (0.7) and \n\n\nLSTAT has a high negative correlation with MEDV(-0.74).\n\n\n\n\n\n\n\n\n\n\nCorrelation\n\n\n\nThe correlation coefficient ranges from -1 to 1. If the value is close to 1, it means that there is a strong positive correlation (linear tendancy) between the two variables. When it is close to -1, the variables have a strong negative correlation.\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor a linear regression model, we select the features which have a high correlation with the target variable. Anyway there is some feature selection techniques you may use, one of them is Backward selection:\nBackward selection:\n\nStart with all variables in the model.\nRemove the variable with the largest p-value — that is, the variable that is the least statistically significant.\nThe new \\((p − 1)\\)-variable model is fit, and the variable with the largest p-value is removed.\nContinue until a stopping rule is reached. For instance, we may stop when all remaining variables have a significant p-value defined by some significance threshold.\n\n\n\n7. Check for multi-colinearity between the features. More specifically RAD and TAX.\n\n\n\n\n\n\nTip\n\n\n\nWe should not select colinear features together for training the model. Check this link for one explanation.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIt is not great to have colinearities between your features as it will make harder the interpretation of which feature is important and which is not. Although I don’t believe it will harm your final estimate that much. In a extreme case it can make the matrix \\(X^T\\cdot X\\) hard / impossible to invert. It will also lower the p-value of the two colinear column which may lead to not selecting them.\n“Multicollinearity does not reduce the predictive power or reliability of the model as a whole, at least within the sample data set; it only affects calculations regarding individual predictors”\n\nplt.scatter(boston['RAD'], boston['TAX'])\nplt.title(f\"Correlation RAD - TAX : {correlation_matrix['RAD']['TAX']:.2f}\" )\nplt.show()\n\n\n\n\n\nncorr = correlation_matrix.drop('MEDV', axis=0).drop('MEDV', axis=1)\nnp.fill_diagonal(ncorr.values, 0)\nncorr[(ncorr < 0.7) & (ncorr > -0.7)] = None\n\nplt.figure(figsize=(10,5))\nsns.heatmap(ncorr, annot=True)\nplt.title('High correlations in features')\nplt.show()\n\n\n\n\nAs we can see we have other quite high colinearity between features but RAD-TAX is the highest.\n\n\n\nSplitting the data into training and testing sets\nTrain test split is a model validation procedure that allows you to simulate how a model would perform on new/unseen data. Here is how the procedure works:\n\n8. Split the data into training and testing sets. We are going to train the model with 80% of the samples and test with the remaining 20%. Use train_test_split() function provided by scikit-learn library\n\nfrom sklearn.model_selection import train_test_split\n\n# complete the code\nX = ...\nY = ...\n\nX_train, X_test, Y_train, Y_test = ...(, , test_size = ..., random_state=5)\n\n# print the shapes to verify if the splitting has occured properly\nprint(X_train.shape)\nprint(X_test.shape)\nprint(Y_train.shape)\nprint(Y_test.shape)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n\n# complete the code\nX = boston.drop('MEDV', axis=1)\nY = boston[['MEDV']]\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2,random_state=5)\n\n# print the shapes to verify if the splitting has occured properly\nprint(X_train.shape)\n\n(404, 13)\n\nprint(X_test.shape)\n\n(102, 13)\n\nprint(Y_train.shape)\n\n(404, 1)\n\nprint(Y_test.shape)\n\n(102, 1)"
  },
  {
    "objectID": "TD1.html#simple-linear-regression-model",
    "href": "TD1.html#simple-linear-regression-model",
    "title": "Lab1: Linear Regression",
    "section": "\nSimple Linear Regression model",
    "text": "Simple Linear Regression model\nIn this part, we are going to build a simple linear regression model. We will choose LSTAT as a feature.\n9. Plot MEDV in function of LSTAT.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nplt.scatter(X_train['LSTAT'], Y_train['MEDV'])\nplt.show()\n\n\n\n\n\n\n\n10. Fit a simple regression model using LinearRegression() from sklearn.linear_model.\n\nfrom sklearn.linear_model import LinearRegression\n\nslm = LinearRegression()\nslm.fit(..., ...)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nX_train_s = X_train[['LSTAT']] # Select LSTAT only\nX_test_s = X_test[['LSTAT']] # Select LSTAT only\n\nfrom sklearn.linear_model import LinearRegression\n\nslm = LinearRegression(fit_intercept=True)\nslm.fit(X_train_s, Y_train)\n\nLinearRegression()\n\n\n\n\n\n11. The LinearRegression() module from scikit-learn does not provide a statistical summary of the regression model. To obtain this summary, re-fit a model using ols() from statsmodels. Analyse the p-value from the summary and interpret.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nimport statsmodels.api as sm\n\nX_train_d = sm.add_constant(X_train_s)\nmodel = sm.OLS(Y_train,X_train_d)\nresults = model.fit()\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   MEDV   R-squared:                       0.552\nModel:                            OLS   Adj. R-squared:                  0.551\nMethod:                 Least Squares   F-statistic:                     495.9\nDate:                Mon, 19 Sep 2022   Prob (F-statistic):           3.76e-72\nTime:                        23:35:33   Log-Likelihood:                -1310.5\nNo. Observations:                 404   AIC:                             2625.\nDf Residuals:                     402   BIC:                             2633.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         34.8729      0.630     55.341      0.000      33.634      36.112\nLSTAT         -0.9798      0.044    -22.269      0.000      -1.066      -0.893\n==============================================================================\nOmnibus:                      111.582   Durbin-Watson:                   1.948\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              237.785\nSkew:                           1.447   Prob(JB):                     2.32e-52\nKurtosis:                       5.399   Cond. No.                         29.3\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nMost of the features seem relevant except for INDUS and AGE.\n\n\n\n12. Plot the regression model.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nplt.plot(X_train_s['LSTAT'], slm.predict(X_train_s), c='red')\nplt.scatter(X_train_s['LSTAT'], Y_train['MEDV'])\nplt.xlabel('LSTAT')\nplt.ylabel('Predicted Values')\nplt.show()\n\n\n\n\n\n\n\nModel evaluation\n13. Evaluate the model using MSE (Mean Squarred Error) and R2-score.\n\nfrom sklearn.metrics import mean_squared_error\n\n# train error (MSE)\ny_train_predict = slm.predict(...)\nmse_train = ...(..., ...)\n\nprint(\"The model performance for training set\")\n\nprint('MSE is {}'.format(mse_train))\n\n\n# test error\ny_test_predict = slm.predict(...)\nmse_test = mean_squared_error(..., ...)\nr2 = r2_score(..., ...)\n\nprint(\"The model performance for testing set\")\n\nprint('MSE is {}'.format(mse_test))\nprint('R2 score is {}'.format(r2))\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# train error (MSE)\ny_train_predict =  slm.predict(X_train_s)\nmse_train = mean_squared_error(Y_train, y_train_predict)\n\nprint(\"The model performance for training set\")\n\nThe model performance for training set\n\nprint('MSE is {}'.format(mse_train))\n\n\n# test error\n\nMSE is 38.45801898706332\n\ny_test_predict = slm.predict(X_test_s)\nmse_test = mean_squared_error(Y_test, y_test_predict)\nr2 = r2_score(Y_test, y_test_predict)\n\nprint(\"The model performance for testing set\")\n\nThe model performance for testing set\n\nprint('MSE is {}'.format(mse_test))\n\nMSE is 38.821829014286585\n\nprint('R2 score is {}'.format(r2))\n\nR2 score is 0.5041523728903132\n\n\n\n\n\n14. According to the plot in 9, the relationship between LSTAT and MEDV is not linear. Let’s try a transformation of our explanatory variable LSTAT. Re-do the steps from 9 to 13 but using the log of LSTAT. Do you obtain a better model?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nX_train_s = np.log(X_train[['LSTAT']]) # Select LSTAT only\nX_test_s = np.log(X_test[['LSTAT']]) # Select LSTAT only\n\nslm = LinearRegression(fit_intercept=True)\nslm.fit(X_train_s, Y_train)\n\n# train error (MSE)\n\nLinearRegression()\n\ny_train_predict =  slm.predict(X_train_s)\nmse_train = mean_squared_error(Y_train, y_train_predict)\n\nprint(\"The model performance for training set\")\n\nThe model performance for training set\n\nprint('MSE is {}'.format(mse_train))\n\n\n# test error\n\nMSE is 28.39419482862284\n\ny_test_predict = slm.predict(X_test_s)\nmse_test = mean_squared_error(Y_test, y_test_predict)\nr2 = r2_score(Y_test, y_test_predict)\n\nprint(\"The model performance for testing set\")\n\nThe model performance for testing set\n\nprint('MSE is {}'.format(mse_test))\n\nMSE is 27.899544077825233\n\nprint('R2 score is {}'.format(r2))\n\nR2 score is 0.6436560801053237\n\n\n\nplt.scatter(X_train['LSTAT'], Y_train['MEDV'])\nplt.scatter(X_train['LSTAT'], slm.predict(X_train_s), c='red')\nplt.xlabel('LSTAT')\nplt.ylabel('Predicted Values')\nplt.show()\n\n\n\n\nWe do obtain a better model, interestingly we can visualise that the model is representing a non linear relationship between the variable and the output thanks to this trick."
  },
  {
    "objectID": "TD1.html#multiple-linear-regression-model",
    "href": "TD1.html#multiple-linear-regression-model",
    "title": "Lab1: Linear Regression",
    "section": "Multiple Linear Regression model",
    "text": "Multiple Linear Regression model\n15. Train a new model using all the variables of the dataset. Evalute the performance of the model.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nX_train_d = sm.add_constant(X_train)\nmodel = sm.OLS(Y_train,X_train_d)\nresults = model.fit()\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   MEDV   R-squared:                       0.738\nModel:                            OLS   Adj. R-squared:                  0.730\nMethod:                 Least Squares   F-statistic:                     84.65\nDate:                Mon, 19 Sep 2022   Prob (F-statistic):          8.21e-105\nTime:                        23:35:36   Log-Likelihood:                -1202.0\nNo. Observations:                 404   AIC:                             2432.\nDf Residuals:                     390   BIC:                             2488.\nDf Model:                          13                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         37.9125      5.775      6.565      0.000      26.559      49.266\nCRIM          -0.1308      0.036     -3.603      0.000      -0.202      -0.059\nZN             0.0494      0.016      3.131      0.002       0.018       0.080\nINDUS          0.0011      0.072      0.015      0.988      -0.141       0.143\nCHAS           2.7054      0.989      2.737      0.006       0.762       4.649\nNOX          -15.9571      4.517     -3.532      0.000     -24.838      -7.076\nRM             3.4140      0.470      7.266      0.000       2.490       4.338\nAGE            0.0011      0.015      0.077      0.939      -0.027       0.030\nDIS           -1.4931      0.233     -6.404      0.000      -1.951      -1.035\nRAD            0.3644      0.079      4.628      0.000       0.210       0.519\nTAX           -0.0132      0.004     -2.950      0.003      -0.022      -0.004\nPTRATIO       -0.9524      0.149     -6.411      0.000      -1.244      -0.660\nB              0.0117      0.003      3.795      0.000       0.006       0.018\nLSTAT         -0.5941      0.058    -10.271      0.000      -0.708      -0.480\n==============================================================================\nOmnibus:                      134.272   Durbin-Watson:                   2.057\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              542.194\nSkew:                           1.423   Prob(JB):                    1.84e-118\nKurtosis:                       7.911   Cond. No.                     1.51e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.51e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\n\n\n16. Which features are significant for the model?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe ones with p-values is less than the error we fix (let’s say 5%). Age for example.\n\n\n\n17. Apply backward selection to fit a model with the best subset of features.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ncolumns_to_drop = ['AGE']\nX_train_bck = X_train.drop(columns_to_drop, axis=1)\nX_test_bck = X_test.drop(columns_to_drop, axis=1)\n\n\nmodel = sm.OLS(Y_train,sm.add_constant(X_train_bck))\nresults = model.fit()\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   MEDV   R-squared:                       0.738\nModel:                            OLS   Adj. R-squared:                  0.730\nMethod:                 Least Squares   F-statistic:                     91.94\nDate:                Mon, 19 Sep 2022   Prob (F-statistic):          8.39e-106\nTime:                        23:35:37   Log-Likelihood:                -1202.0\nNo. Observations:                 404   AIC:                             2430.\nDf Residuals:                     391   BIC:                             2482.\nDf Model:                          12                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         37.8841      5.756      6.582      0.000      26.568      49.200\nCRIM          -0.1309      0.036     -3.610      0.000      -0.202      -0.060\nZN             0.0492      0.016      3.151      0.002       0.019       0.080\nINDUS          0.0009      0.072      0.013      0.990      -0.141       0.143\nCHAS           2.7098      0.986      2.749      0.006       0.772       4.648\nNOX          -15.8774      4.392     -3.615      0.000     -24.512      -7.243\nRM             3.4208      0.461      7.423      0.000       2.515       4.327\nDIS           -1.4985      0.222     -6.754      0.000      -1.935      -1.062\nRAD            0.3640      0.078      4.640      0.000       0.210       0.518\nTAX           -0.0132      0.004     -2.953      0.003      -0.022      -0.004\nPTRATIO       -0.9516      0.148     -6.430      0.000      -1.243      -0.661\nB              0.0118      0.003      3.818      0.000       0.006       0.018\nLSTAT         -0.5925      0.054    -10.938      0.000      -0.699      -0.486\n==============================================================================\nOmnibus:                      134.673   Durbin-Watson:                   2.057\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              545.931\nSkew:                           1.426   Prob(JB):                    2.84e-119\nKurtosis:                       7.929   Cond. No.                     1.49e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.49e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\ncolumns_to_drop = ['INDUS']\nX_train_bck = X_train.drop(columns_to_drop, axis=1)\nX_test_bck = X_test.drop(columns_to_drop, axis=1)\n\nmodel = sm.OLS(Y_train,sm.add_constant(X_train_bck))\nresults = model.fit()\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   MEDV   R-squared:                       0.738\nModel:                            OLS   Adj. R-squared:                  0.730\nMethod:                 Least Squares   F-statistic:                     91.94\nDate:                Mon, 19 Sep 2022   Prob (F-statistic):          8.37e-106\nTime:                        23:35:37   Log-Likelihood:                -1202.0\nNo. Observations:                 404   AIC:                             2430.\nDf Residuals:                     391   BIC:                             2482.\nDf Model:                          12                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         37.9074      5.758      6.584      0.000      26.588      49.227\nCRIM          -0.1308      0.036     -3.610      0.000      -0.202      -0.060\nZN             0.0494      0.016      3.154      0.002       0.019       0.080\nCHAS           2.7073      0.979      2.765      0.006       0.783       4.632\nNOX          -15.9390      4.351     -3.663      0.000     -24.494      -7.384\nRM             3.4133      0.467      7.304      0.000       2.495       4.332\nAGE            0.0011      0.014      0.077      0.939      -0.027       0.030\nDIS           -1.4938      0.227     -6.570      0.000      -1.941      -1.047\nRAD            0.3641      0.076      4.799      0.000       0.215       0.513\nTAX           -0.0131      0.004     -3.269      0.001      -0.021      -0.005\nPTRATIO       -0.9521      0.147     -6.479      0.000      -1.241      -0.663\nB              0.0117      0.003      3.802      0.000       0.006       0.018\nLSTAT         -0.5940      0.058    -10.308      0.000      -0.707      -0.481\n==============================================================================\nOmnibus:                      134.288   Durbin-Watson:                   2.057\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              542.346\nSkew:                           1.423   Prob(JB):                    1.70e-118\nKurtosis:                       7.911   Cond. No.                     1.49e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.49e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\ny_test_predict = results.predict(sm.add_constant(X_test_bck))\nmse_test = mean_squared_error(Y_test, y_test_predict)\nr2 = r2_score(Y_test, y_test_predict)\n\nprint(\"The model performance for testing set\")\n\nThe model performance for testing set\n\nprint('MSE is {}'.format(mse_test))\n\nMSE is 20.872226101228375\n\nprint('R2 score is {}'.format(r2))\n\nR2 score is 0.7334117416007803\n\n\n\n\n\n18. Is the new model better than the last one with all the features?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ny_test_predict = results.predict(sm.add_constant(X_test_bck))\nmse_test = mean_squared_error(Y_test, y_test_predict)\nr2 = r2_score(Y_test, y_test_predict)\n\nprint(\"The model performance for testing set\")\n\nThe model performance for testing set\n\nprint('MSE is {}'.format(mse_test))\n\nMSE is 20.872226101228375\n\nprint('R2 score is {}'.format(r2))\n\nR2 score is 0.7334117416007803\n\n\n\n\n\n19. In the last model we didn’t transform LSTAT. Re train the model using log(LSTAT) instead of LSTAT. Does this new model performs better?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nX_train_lg = X_train.drop('LSTAT',axis=1)\nX_train_lg['log(LSTAT)'] = np.log(X_train['LSTAT'])\n\nX_test_lg = X_test.drop('LSTAT',axis=1)\nX_test_lg['log(LSTAT)'] = np.log(X_test['LSTAT'])\n\n\nmodel = sm.OLS(Y_train,sm.add_constant(X_train_lg))\nresults = model.fit()\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   MEDV   R-squared:                       0.787\nModel:                            OLS   Adj. R-squared:                  0.780\nMethod:                 Least Squares   F-statistic:                     111.1\nDate:                Mon, 19 Sep 2022   Prob (F-statistic):          3.07e-122\nTime:                        23:35:38   Log-Likelihood:                -1160.0\nNo. Observations:                 404   AIC:                             2348.\nDf Residuals:                     390   BIC:                             2404.\nDf Model:                          13                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         56.7251      5.535     10.249      0.000      45.843      67.607\nCRIM          -0.1438      0.032     -4.444      0.000      -0.207      -0.080\nZN             0.0286      0.014      2.009      0.045       0.001       0.057\nINDUS         -0.0018      0.065     -0.027      0.978      -0.130       0.126\nCHAS           2.0388      0.894      2.281      0.023       0.281       3.796\nNOX          -15.0430      4.065     -3.701      0.000     -23.034      -7.052\nRM             2.1356      0.443      4.820      0.000       1.265       3.007\nAGE            0.0252      0.013      1.892      0.059      -0.001       0.051\nDIS           -1.2703      0.211     -6.010      0.000      -1.686      -0.855\nRAD            0.3307      0.071      4.663      0.000       0.191       0.470\nTAX           -0.0116      0.004     -2.874      0.004      -0.019      -0.004\nPTRATIO       -0.8343      0.134     -6.212      0.000      -1.098      -0.570\nB              0.0094      0.003      3.348      0.001       0.004       0.015\nlog(LSTAT)    -9.5323      0.643    -14.825      0.000     -10.796      -8.268\n==============================================================================\nOmnibus:                      120.438   Durbin-Watson:                   2.046\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              557.257\nSkew:                           1.212   Prob(JB):                    9.84e-122\nKurtosis:                       8.218   Cond. No.                     1.56e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.56e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\ny_test_predict = results.predict(sm.add_constant(X_test_lg))\nmse_test = mean_squared_error(Y_test, y_test_predict)\nr2 = r2_score(Y_test, y_test_predict)\n\nprint(\"The model performance for testing set\")\n\nThe model performance for testing set\n\nprint('MSE is {}'.format(mse_test))\n\nMSE is 15.039960438345249\n\nprint('R2 score is {}'.format(r2))\n\nR2 score is 0.8079037262146341\n\n\nThe model is better (lower test MSE)."
  },
  {
    "objectID": "TD1.html#anova-analysis-of-variances",
    "href": "TD1.html#anova-analysis-of-variances",
    "title": "Lab1: Linear Regression",
    "section": "ANOVA (ANalysis Of VAriances)",
    "text": "ANOVA (ANalysis Of VAriances)\nIn this last part we will apply an analysis of variances (ANOVA) in order to test if there is a significant difference of means between two groups \\(i\\) and \\(j\\) (Consider group \\(i\\) is the suburbs bounding the river and \\(j\\) the suburbs which not). The hypotheses are\n\\[ H_0 : \\mu_i = \\mu_j \\]\n\\[ H_1 : \\mu_i \\neq \\mu_j \\]\nWhere \\(\\mu_i\\) is the mean of MEDV in group \\(i\\).\n\n\n\n\n\n\nAnova\n\n\n\nThis analysis can be conducted during the exploratory data analysis part especially when the target is continuous and a feature is discrete.\n\n\n20. In the Boston data set there is a categorical variable CHAS which corresponds to Charles River (= 1 if a suburb bounds the river; 0 otherwise). How many of the suburbs in this data set bound the Charles river?\n21. Create Boxplots of the median value of houses with respect to the variable CHAS. Do we observe some difference between the median value of houses with respect to the neighborhood to Charles River?\n\n\n\n22. Calculate \\(\\mu_i\\) and \\(\\mu_j\\).\n\n\n\n23. Apply an ANOVA test of MEDV with respect to CHAS. What do you conclude ?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nboston['CHAS'].value_counts()\n\n0.0    471\n1.0     35\nName: CHAS, dtype: int64\n\n\n\nsns.boxplot(x='CHAS', y='MEDV', data=boston)\nplt.show()\n\n\n\n\n\nboston.groupby('CHAS')['MEDV'].mean()\n\nCHAS\n0.0    22.093843\n1.0    28.440000\nName: MEDV, dtype: float64\n\n\nYou can use Anova’s test from statsmodels or from scipy.\n\nfrom scipy.stats import f_oneway\n\nf_oneway(boston[boston['CHAS'] == 1.0]['MEDV'].to_numpy(), \n         boston[boston['CHAS'] == 0.0]['MEDV'].to_numpy())\n\nF_onewayResult(statistic=15.971512420371962, pvalue=7.390623170520815e-05)\n\n\nOr\n\n\nfrom statsmodels.stats.anova import anova_lm\nfrom statsmodels.formula.api import ols\n\nanova_boston_bound_river = anova_lm(\n    ols('MEDV~C(CHAS)', data=boston[['MEDV', 'CHAS']]).fit())\nprint(anova_boston_bound_river)\n\n             df        sum_sq      mean_sq          F    PR(>F)\nC(CHAS)     1.0   1312.079271  1312.079271  15.971512  0.000074\nResidual  504.0  41404.216144    82.151223        NaN       NaN\n\n\nBecause the p-value is low we can reject the hypothesis that the two groups have the same mean and thus conclude that they have different means. Note that we didn’t check which was the biggest here, just that they are not the same. Because we are checking only the difference between two distribution and not multiple ones."
  },
  {
    "objectID": "TD2.html#introduction",
    "href": "TD2.html#introduction",
    "title": "Lab2: Logistic Regression & Regularization",
    "section": "Introduction",
    "text": "Introduction\nIn this lab, you will implement logistic regression and apply it to two different datasets.\nBefore we begin with the exercises, we need to import all libraries required for this programming exercise. Throughout the course, we will be using numpy for all arrays and matrix operations, and matplotlib for plotting. In this assignment, we will also use scipy, which contains scientific and numerical computation functions and tools.\n\n# Scientific and vector computation for python\nimport numpy as np\n\n# Plotting library\nimport matplotlib.pyplot as plt\n\n# Optimization module in scipy\nfrom scipy import optimize\n\n# do not forget to tell matplotlib to embed plots within the notebook\n%matplotlib inline"
  },
  {
    "objectID": "TD2.html#logistic-regression",
    "href": "TD2.html#logistic-regression",
    "title": "Lab2: Logistic Regression & Regularization",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nIn this part of the exercise, you will build a logistic regression model to predict whether a student gets admitted into a university. Suppose that you are the administrator of a university department and you want to determine each applicant’s chance of admission based on their results on two exams. You have historical data from previous applicants that you can use as a training set for logistic regression. For each training example, you have the applicant’s scores on two exams and the admissions decision.\nYour task is to build a classification model that estimates an applicant’s probability of admission based the scores from those two exams.\n1. Load the data tp2data1.txt from here  using the loadtxt() from numpy. The first two columns contains the exam scores and the third column contains the label. Then separate the features from label. Name the feature matrix X and the label y.\n\n\n\n\n\n\n2. Print the first 5 samples from X and y.\nVisualizing the data\n3. Display the data on a 2-dimensional plot where the axes are the two exam scores, and the positive and negative examples are shown with different colors (or markers).\nYou should produce something like this:\n\n\n\n\n\n\n\n\nImplementation\nSigmoid function\nRecall that the logistic regression hypothesis is defined as:\n\\[ h_\\omega(x) = g(\\omega^T x)\\]\nwhere function \\(g\\) is the sigmoid function. The sigmoid function is defined as:\n\\[g(z) = \\frac{1}{1+e^{-z}}\\]\n4. Implement the sigmoid function so it can be called by the rest of your program. When you are finished, try testing a few values by calling sigmoid(x) in a new cell. For large positive values of x, the sigmoid should be close to 1, while for large negative values, the sigmoid should be close to 0. Evaluating sigmoid(0) should give you exactly 0.5. Your code should also work with vectors and matrices. For a matrix, your function should perform the sigmoid function on every element.\n5. Plot the sigmoid function, like this:\n\n\n\n\n\n\n\n\n\n\n\nCost function and gradient\n6. Before proceeding, add the intercept term to X. (hint: you can use np.concatenate or np.hstack)\n\n\n\n7. Implement the cost function and its gradient for logistic regression.\nRecall that the cost function for logistic regression is\n\\[ J(\\omega) = \\frac{1}{m} \\sum_{i=1}^{m} \\left[ -y^{(i)} \\log\\left(h_\\omega\\left( x^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - h_\\omega\\left( x^{(i)} \\right) \\right) \\right]\\]\nRecall that the gradient of the cost is a vector of the same length as \\(\\omega\\) where the \\(j^{th}\\) element (for \\(j = 0, 1, \\cdots , n\\)) is defined as follows:\n\\[ \\frac{\\partial J(\\omega)}{\\partial \\omega_j} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\omega \\left( x^{(i)} \\right) - y^{(i)} \\right) x_j^{(i)} \\]\n\n\n\n8. Call your implemented function using two test cases for \\(\\omega\\). You should see that the cost is about 0.693 for \\(\\omega = (0,0,0)\\).\n\n\n\n\n\n\nLearning the parameters\nLearning parameters using your implemented Gradient Descent\n9. Implement the gradient descent algorithm for logistic regression: write a cost function and calculate its gradient, then take a gradient descent step accordingly in order to find the optimal parameters. Run it on the training set. Print the results (the parameters values and the cost function).\nLearning parameters using scipy.optimize\nIn this part you will use the scipy.optimize module. SciPy is a numerical computing library for python. It provides an optimization module for root finding and minimization. As of scipy 1.0, the function scipy.optimize.minimize is the method to use for optimization problems(both constrained and unconstrained).\nFor logistic regression, you want to optimize the cost function \\(J(\\omega)\\) with parameters \\(\\omega\\). Concretely, you are going to use optimize.minimize to find the best parameters \\(\\omega\\) for the logistic regression cost function, given a fixed dataset (of X and y values). You will pass to optimize.minimize the following inputs:\n\n\ncostFunction: A cost function that, when given the training set and a particular \\(\\omega\\), computes the logistic regression cost and gradient with respect to \\(\\omega\\) for the dataset (X, y). It is important to note that we only pass the name of the function without the parenthesis. This indicates that we are only providing a reference to this function, and not evaluating the result from this function.\n\ninitial_omega: The initial values of the parameters we are trying to optimize.\n\n(X, y): These are additional arguments to the cost function.\n\njac: Indication if the cost function returns the Jacobian (gradient) along with cost value. (True)\n\nmethod: Optimization method/algorithm to use\n\noptions: Additional options which might be specific to the specific optimization method. In the following, we only tell the algorithm the maximum number of iterations before it terminates.\n\nIf you have calculated the cost function correctly, optimize.minimize will converge on the right optimization parameters and return the final values of the cost and \\(\\omega\\) in a class object. Notice that by using optimize.minimize, you did not have to write any loops yourself, or set a learning rate like you did for gradient descent. This is all done by optimize.minimize: you only needed to provide a function calculating the cost and the gradient.\nIn the following, a code written to call optimize.minimize with the correct arguments.\n\n# set options for optimize.minimize\noptions= {'maxiter': 400}\n\n# see documention for scipy's optimize.minimize  for description about\n# the different parameters\n# The function returns an object `OptimizeResult`\n# We use truncated Newton algorithm for optimization which is \n# equivalent to MATLAB's fminunc\n# See https://stackoverflow.com/questions/18801002/fminunc-alternate-in-numpy\n\n# initial_omega = np.array([0, 0, 0])\n\nres = optimize.minimize(costFunction,\n                        initial_omega,\n                        (X_train, y),\n                        jac=True,\n                        method='TNC',\n                        options=options)\n\n# the fun property of `OptimizeResult` object returns\n# the value of costFunction at optimized omega\ncost = res.fun\n\n# the optimized omega is in the x property\nomega = res.x\n\n# Print omega to screen\nprint('Cost at omega found by optimize.minimize: {:.3f}'.format(cost))\nprint('Expected cost (approx): 0.203\\n');\n\nprint('omega:')\nprint('\\t[{:.3f}, {:.3f}, {:.3f}]'.format(*omega))\nprint('Expected omega (approx):\\n\\t[-25.161, 0.206, 0.201]')\n\n10. Run the code above on your cost function and verify that you obtain the correct (or almost) values.\n\n\n\nPlotting the decision boundary\nSince the decision boundary of logistic regression is a linear (you know that right?) and the dimension of the feature space here is 2, the decision boundary in this 2-dimensional space is a line that separates the predicted classes “0” and “1”.\nFor logistic regression, we predict \\(y=1\\) if \\(\\omega^T X \\geq 0\\) (right side of the line) and \\(y=0\\) if \\(\\omega^T X \\lt 0\\) (left side of the line). Where\n\\[\n\\omega = \\begin{pmatrix} \\omega_0 \\\\ \\omega_1 \\\\ \\omega2 \\end{pmatrix} \\,\\, \\text{and} \\,\\, X = \\begin{pmatrix}\n  1 \\\\\n  X_1 \\\\\n  X_2\n  \\end{pmatrix}\n\\]\nSo we predict \\(y=1\\) if \\(\\omega_0 + \\omega_1 X_1 + \\omega_2 X_2 \\geq 0\\) which means that the equation of the decision boundary (a line here) is \\(X_2 = - \\frac{\\omega_1}{\\omega_2}X_1 - \\frac{\\omega_0}{\\omega_2}\\)\n11. Plot the decision boundary obtained with logistic regression.\nEvaluating logistic regression\n12. After learning the parameters, you can use the model to predict whether a particular student will be admitted. For a student with an Exam 1 score of 45 and an Exam 2 score of 85, you should expect to see an admission probability of 0.776. Another way to evaluate the quality of the parameters we have found is to see how well the learned model predicts on our training set. Write a function predict that will produce “1” or “0” predictions given a dataset and a learned parameter vector \\(\\omega\\).\n13. Calculate the confusion matrix, and use it to calculate the training accuracy of your classifier and the F1 score.\n14. In order to verify that your line (decision boundary) is well plotted, color the points on the last Figure with respect to the predicted response.\n15. Now make the same plot but color the points with respect to their real labels. From this figure, count the number of the false positive predictions.\nplotDecisionBoundary function\nFor the rest, use the following function (or modify it to adapt it) for plotting the decision boundary.\n\ndef plotDecisionBoundary(omega, X, y):\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n                         np.arange(y_min, y_max, 0.1))\n    X_plot = np.c_[xx.ravel(), yy.ravel()]\n    X_plot = np.hstack((np.ones((X_plot.shape[0], 1)), X_plot))\n    y_plot = np.dot(X_plot, omega).reshape(xx.shape)\n    \n    plt.figure()\n    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label=\"Admitted\")\n    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label=\"Not admitted\")\n    plt.contour(xx, yy, y_plot, levels=[0])\n    plt.xlabel(\"Exam 1 score\")\n    plt.ylabel(\"Exam 2 score\")\n    plt.legend()\n    plt.show()\n\n\nplotDecisionBoundary(res.x, X, y)\n\n\n\n0.20365864300942144\n\n\n[-24.13930679   0.19805053   0.19320881]"
  },
  {
    "objectID": "TD2.html#regularized-logistic-regression",
    "href": "TD2.html#regularized-logistic-regression",
    "title": "Lab2: Logistic Regression & Regularization",
    "section": "Regularized logistic regression",
    "text": "Regularized logistic regression\nIn this part of the exercise, you will implement regularized logistic regression to predict whether microchips from a fabrication plant passes quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly.\nSuppose you are the product manager of the factory and you have the test results for some microchips on two different tests. From these two tests, you would like to determine whether the microchips should be accepted or rejected. To help you make the decision, you have a dataset of test results on past microchips, from which you can build a logistic regression model.\n2.1. Load the data in file tp2data2.txt from from here . Separate the features from the labels in two differents objects.\n\n\n\n2.2. Visualize the data in a 2-dimensional plot. Color the point with respect to their labels. What do you notice ?\n\n\n\n\n\nFeature mapping\nOne way to fit the data better is to create more features from each data point. In the function mapFeature defined below, we will map the features into all polynomial terms of \\(x_1\\) and \\(x_2\\) up to the sixth power.\n\\[\\text{mapFeature}(x) = \\begin{bmatrix} 1 & x_1 & x_2 & x_1^2 & x_1 x_2 & x_2^2 & x_1^3 & \\dots & x_1 x_2^5 & x_2^6 \\end{bmatrix}^T\\]\n\ndef mapFeature(X1, X2, degree=6):\n    \"\"\"\n    Maps the two input features to quadratic features used in the regularization exercise.\n\n    Returns a new feature array with more features, comprising of\n    X1, X2, X1.^2, X2.^2, X1*X2, X1*X2.^2, etc..\n\n    Parameters\n    ----------\n    X1 : array_like\n        A vector of shape (m, 1), containing one feature for all examples.\n\n    X2 : array_like\n        A vector of shape (m, 1), containing a second feature for all examples.\n        Inputs X1, X2 must be the same size.\n\n    degree: int, optional\n        The polynomial degree.\n\n    Returns\n    -------\n    : array_like\n        A matrix of of m rows, and columns depend on the degree of polynomial.\n    \"\"\"\n    if X1.ndim > 0:\n        out = [np.ones(X1.shape[0])]\n    else:\n        out = [np.ones(1)]\n\n    for i in range(1, degree + 1):\n        for j in range(i + 1):\n            out.append((X1 ** (i - j)) * (X2 ** j))\n\n    if X1.ndim > 0:\n        return np.stack(out, axis=1)\n    else:\n        return np.array(out)\n\nAs a result of this mapping, our vector of two features (the scores on two QA tests) has been transformed into a 28-dimensional vector. A logistic regression classifier trained on this higher-dimension feature vector will have a more complex decision boundary and will appear nonlinear when drawn in our 2-dimensional plot.\nWhile the feature mapping allows us to build a more expressive classifier, it also more susceptible to overfitting.\nIn the next parts of the exercise, you will implement regularized logistic regression to fit the data and also see for yourself how regularization can help combat the overfitting problem.\n2.3. Apply the mapFeature() function on the dataset. Verify that you get a 28-dimensional vector.\n\n\n\nCost function and gradient\nRecall that the regularized cost function in logistic regression is\n\\[ J(\\omega) = \\frac{1}{m} \\sum_{i=1}^m \\left[ -y^{(i)}\\log \\left( h_\\omega \\left(x^{(i)} \\right) \\right) - \\left( 1 - y^{(i)} \\right) \\log \\left( 1 - h_\\omega \\left( x^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\omega_j^2 \\]\nNote that we do not regularize the parameters \\(\\omega_0\\). The gradient of the cost function is a vector where the \\(j^{th}\\) element is defined as follows:\n\\[ \\frac{\\partial J(\\omega)}{\\partial \\omega_0} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\omega \\left(x^{(i)}\\right) - y^{(i)} \\right) x_j^{(i)} \\qquad \\text{for } j =0 \\]\n\\[ \\frac{\\partial J(\\omega)}{\\partial \\omega_j} = \\left( \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\omega \\left(x^{(i)}\\right) - y^{(i)} \\right) x_j^{(i)} \\right) + \\frac{\\lambda}{m}\\omega_j \\qquad \\text{for } j \\ge 1 \\]\n2.4. Complete the function costFunctionReg() below. This function computes and returns the cost function and gradient for regularized logistic regression.\n\ndef costFunctionReg(omega, X, y, lambda_):\n    \"\"\"\n    Compute cost and gradient for logistic regression with regularization.\n    \n    Parameters\n    ----------\n    omega : array_like\n        Logistic regression parameters. A vector with shape (n, ). n is \n        the number of features including any intercept. If we have mapped\n        our initial features into polynomial features, then n is the total \n        number of polynomial features. \n    \n    X : array_like\n        The data set with shape (m x n). m is the number of examples, and\n        n is the number of features (after feature mapping).\n    \n    y : array_like\n        The data labels. A vector with shape (m, ).\n    \n    lambda_ : float\n        The regularization parameter. \n    \n    Returns\n    -------\n    J : float\n        The computed value for the regularized cost function. \n    \n    grad : array_like\n        A vector of shape (n, ) which is the gradient of the cost\n        function with respect to omega, at the current values of omega.\n    \n    Instructions\n    ------------\n    Compute the cost `J` of a particular choice of omega.\n    Compute the partial derivatives and set `grad` to the partial\n    derivatives of the cost w.r.t. each parameter in omega.\n    \"\"\"\n    # Initialize some useful values\n    m = y.size  # number of training examples\n\n    # You need to return the following variables correctly \n    J = 0\n    grad = np.zeros(omega.shape)\n\n    # ===================== YOUR CODE HERE ======================\n\n    \n    \n    # =============================================================\n    return J, grad\n\n\n\n\n2.5. Once you are done with the costFunctionReg, call it using the initial value of \\(\\omega\\) (initialized to all zeros), and also another test case where \\(\\omega\\) is all ones. The code is given below with the expected values. You should obtain the same values.\n\n# Initialize fitting parameters\n# X here has 28 columns\ninitial_omega = np.zeros(X.shape[1])\n\n# Set regularization parameter lambda to 1\n# DO NOT use `lambda` as a variable name in python\n# because it is a python keyword\nlambda_ = 1\n\n# Compute and display initial cost and gradient for regularized logistic\n# regression\ncost, grad = costFunctionReg(initial_omega, X, y, lambda_)\n\nprint('Cost at initial omega (zeros): {:.3f}'.format(cost))\nprint('Expected cost (approx)       : 0.693\\n')\n\nprint('Gradient at initial omega (zeros) - first five values only:')\nprint('\\t[{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}]'.format(*grad[:5]))\nprint('Expected gradients (approx) - first five values only:')\nprint('\\t[0.0085, 0.0188, 0.0001, 0.0503, 0.0115]\\n')\n\n\n# Compute and display cost and gradient\n# with all-ones omega and lambda = 10\ntest_omega = np.ones(X.shape[1])\ncost, grad = costFunctionReg(test_omega, X, y, 10)\n\nprint('------------------------------\\n')\nprint('Cost at test omega    : {:.2f}'.format(cost))\nprint('Expected cost (approx): 3.16\\n')\n\nprint('Gradient at test omega - first five values only:')\nprint('\\t[{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}]'.format(*grad[:5]))\nprint('Expected gradients (approx) - first five values only:')\nprint('\\t[0.3460, 0.1614, 0.1948, 0.2269, 0.0922]')\n\nLearning parameters using scipy.optimize.minimize\n\n2.6. Use optimize.minimize to learn the optimal parameters \\(\\omega\\).\n\n\n\nPlotting the decision boundary\n2.7. To visualize the model learned by this classifier use the function plotDecisionBoundary to plot the (non-linear) decision boundary that separates the positive and negative examples.\n\nIn plotDecisionBoundary, we plot the non-linear decision boundary by computing the classifier’s predictions on an evenly spaced grid and then and draw a contour plot where the predictions change from y = 0 to y = 1.\n\nYou should obtain something like this:\n\n\n<string>:37: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n\n\n\n\n\nExtra\nTry out different regularization parameters for the dataset to understand how regularization prevents overfitting.\nNotice the changes in the decision boundary as you vary \\(\\lambda\\). With a small \\(\\lambda\\), you should find that the classifier gets almost every training example correct, but draws a very complicated boundary, thus overfitting the data.\nCredits\n\nThis lab is hugely inspired from Andrew Ng’s Machine Learning course. Supplementary material from dibgerge’s github was used."
  },
  {
    "objectID": "TD3.html#regression-trees",
    "href": "TD3.html#regression-trees",
    "title": "Lab3: Decision Trees and Random Forests",
    "section": "Regression Trees",
    "text": "Regression Trees\nFor building trees for regression we are going to use the Housing dataset, (know also as Boston dataset). In the Housing dataset, there is 13 features and one target variable, described as follows:\n\n\ncrim: per capita crime rate by town.\n\nzn: proportion of residential land zoned for lots over 25,000 sq.ft.\n\nindus: proportion of non-retail business acres per town.\n\nchas: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n\nnox: nitrogen oxides concentration (parts per 10 million).\n\nrm: average number of rooms per dwelling.\n\nage: proportion of owner-occupied units built prior to 1940.\n\ndis: weighted mean of distances to five Boston employment centres.\n\nrad: index of accessibility to radial highways.\n\ntax: full-value property-tax rate per $10,000.\n\nptratio: pupil-teacher ratio by town.\n\nblack: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.\n\nlstat: lower status of the population (percent).\n\nAnd the target variable:\n\n\nmedv: median value of owner-occupied homes in $1000s.\n\nSingle tree\nTo demonstrate regression trees, we will use the Boston dataset that we used during the first two practical works, from the MASS package. Recall that medv is the response.\n1. Load the dataset and split it randomly in half.\nIn , load it from MASS package.\n\nlibrary(MASS)\nlibrary(caTools)\nset.seed(18)\nBoston_idx = sample(1:nrow(Boston), nrow(Boston) / 2) \n# You don't know what we just did?\n# open the documentation of the function sample by \n# writing ?sample in the R console.\n# Note that this is one of the ways to split it randomly and it is not necessary the best.\nBoston_train = Boston[Boston_idx,]\nBoston_test  = Boston[-Boston_idx,]\n\nIn Python, load it from sklearn’s datasets.\n\nfrom sklearn import datasets\n\n# Loading the Housing dataset\nboston= datasets.load_boston()\n\n# Create feature matrix\nX = boston.data\nprint(X.shape)\n\n# Create target vector\ny=boston.target\nprint(y.shape)\n\n# Then use train_test_split()\n\n2. Fit a regression tree to the training data using the rpart() function from the rpart package. Name the tree Boston_tree.\n\n\n\n3. Plot the obtained tree using the following code.\n\nplot(Boston_tree)\ntext(Boston_tree, pretty = 0)\ntitle(main = \"Regression Tree\")\n\n\n\n\n4. A better plot can be obtained using the rpart.plot2 package. Re-plot the tree using it. You can use the rpart.plot() function which by default, when the output is continuous, each node shows: the predicted value, and the percentage of observations in the node. You can also use the prp() function.\n\nlibrary(rpart.plot)\nrpart.plot(Boston_tree)\nprp(Boston_tree)\n\n\n\n\n\n\n\n5. Print the obtained tree and print its summary. Between the things that you can see in the summary, the CP (complexity parameter) table and the importance of each variable in the model. Print the CP table using the printcp() function to see the cross validation results. Plot a comparison figure using the plotcp() function.\nYou will notice the obtained tree is pruned. This is because rpart prunes the tree by default by performing 10-fold cross-validation.\n\n\n\n\n\n\nNote\n\n\n\nrpart keeps track of something called the complexity of a tree. The complexity measure is a combination of the size of a tree and the ability of the tree to separate the classes of the target variable. If the next best split in growing a tree does not reduce the tree’s overall complexity by a certain amount, rpart will terminate the growing process. This amount is specified by the complexity parameter, cp, in the call to rpart(). Setting cp to a negative amount (like -1) ensures that the tree will be fully grown. You can try it and then plot the tree.\nNotice that the default cp value may over prune the tree. As a rule of thumb, it’s best to prune a decision tree using the cp of smallest tree that is within one standard deviation of the tree with the smallest xerror. In the example above, it’s maybe best to prune the tree with a cp slightly greater than 0.03.\n\n\nNext we will compare this regression tree to a linear model and will use RMSE as our metric. RMSE is the Root Mean Square Error, which is the square root of the MSE.\n5. Write a  function that returns the RMSE of two vectors.\n\n\n\n6. Use the function predict() to predict the response on the test set. Then calculate the RMSE obtained with tree model.\n\n\n\n\n\n\n7. Fit a linear regression model on the training set. Then predict the response on the test set using the linear model. Calculate the RMSE and compare the performance of the tree and the linear regression model.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nHere the most obvious linear regression beats the tree! We’ll improve on this tree by considering ensembles of trees.\n\n\nYou can visually compare the performance of both models by plotting the Actual (reality) response values against the predicted values. The model with closer points are to the diagonal (y=x) line is the better one. You can try to reproduce the figure below.\n\n\n\n\n\n\n\n\nBy aggregating many decision trees, using methods like bagging, random forests, and boosting, the predictive performance of trees can be substantially improved. We will now use these concepts, called ensemble methods.\nBagging\nBagging, or Bootstrap aggregation, is a general-purpose procedure for reducing the variance of a statistical learning method, it is particularly useful and frequently used in the context of decision trees. The idea is to take many training sets from the population, build a separate prediction model using each training set, and average the resulting predictions. Generally we do not have access to multiple training sets. Instead, we can bootstrap, by taking repeated samples from the (single) training data set.\nTo apply bagging to regression trees, we simply construct \\(B\\) regression trees using B bootstrapped training sets, and average the resulting predictions. These trees are grown deep, and are not pruned. Hence each individual tree has high variance, but low bias. Averaging these \\(B\\) trees reduces the variance.\n8. Fit a bagged model, using the randomForest() function from the randomForest package.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nBagging is actually a special case of a random forest where mtry is equal to \\(p\\), the number of predictors.\n\n\n9. Predict the response on the test set using the bagging model. Calculate the RMSE. Is the performance of the model better than linear regression or a simple tree?\n\n\n\n\n\n\nNote that the “Mean of squared residuals” which is output by randomForest() is the Out of Bag3 estimate of the error. Here is its plot:\n\n\n\n\n\nRandom Forests\nNow try a random forest. For regression, on suggestion is to use mtry equal to \\(p/3\\).4\n10. Fit a random forest on the training set and compare its performance with the previous models by calculating the predictions and the RMSE.\n\n\n\n\n\n\n\n\n\n11. Use the function importance() from the randomForest package to see the most important predictors in the obtained random forest model. What are the three most important predictors? Did you find the same results when you selected the best predictors for the linear regression model during session 2?\n\n\n\n12. Plot the importance of the predictors to the model using the varImpPlot() function.\n\n\n\n\n\nBoosting\nLast and not least, let us try a boosted model, which by default will produce a nice variable importance plot as well as plots of the marginal effects of the predictors. To do so, we will use the gbm package5.\n10. Using the gbm() function like following, fit a boosted model on the training set. Then compare its performance with the previous models by calculating the predictions and the RMSE.\n\nlibrary(gbm)\n\nLoaded gbm 2.1.8\n\nBoston_boost = gbm(medv ~ ., data = Boston_train, distribution = \"gaussian\", \n                    n.trees = 5000, interaction.depth = 4, shrinkage = 0.01)\n\n\nBoston_boost_pred = predict(Boston_boost, newdata = Boston_test)\n\nUsing 5000 trees...\n\n\n\nrmse(Boston_boost_pred, Boston_test$medv)\n\n[1] 3.656622\n\n\n11. Show the summary of the boosted model. A figure of the variable importance will be shown.\n\nsummary(Boston_boost)\n\n\n\n\n            var     rel.inf\nlstat     lstat 44.28292729\nrm           rm 26.75013094\ndis         dis  5.69907146\ncrim       crim  4.99715134\nnox         nox  4.80328525\nblack     black  3.72041629\nage         age  3.15666045\nptratio ptratio  2.66396487\ntax         tax  2.11304970\nindus     indus  0.86895517\nrad         rad  0.73545313\nzn           zn  0.16493067\nchas       chas  0.04400344\n\n\nComparison\n12. Reproduce the following comparison: A table in which we show the obtained RMSE with each tested model, you can create a \\(5 \\times 2\\) data.frame in which you put the names of the models and the corresponding RMSE. To visualize the data frame in the compiled html report you can use the kable() function from the knitr package. Or, compare the models by plotting the Actual (reality) response values against the predicted values.\n\npar(mfrow=c(2,2))\nplot(Boston_tree_pred, Boston_test$medv, \n     xlab = \"Predicted\", ylab = \"Actual\", \n     main = \"Predicted vs Actual: Single Tree, Test Data\",\n     col = \"#cd0050\", pch = 20)\ngrid()\nabline(0, 1, col = \"dodgerblue\", lwd = 2)\n\nplot(Boston_bagging_pred, Boston_test$medv,\n     xlab = \"Predicted\", ylab = \"Actual\",\n     main = \"Bagging, Test Data\",\n     col = \"#cd0050\", pch = 20)\ngrid()\nabline(0, 1, col = \"dodgerblue\", lwd = 2)\n\nplot(Boston_forest_pred, Boston_test$medv,\n     xlab = \"Predicted\", ylab = \"Actual\",\n     main = \"Random Forest, Test Data\",\n     col = \"#cd0050\", pch = 20)\ngrid()\nabline(0, 1, col = \"dodgerblue\", lwd = 2)\n\nplot(Boston_boost_pred, Boston_test$medv,\n     xlab = \"Predicted\", ylab = \"Actual\",\n     main = \"Boosting, Test Data\",\n     col = \"#cd0050\", pch = 20)\ngrid()\nabline(0, 1, col = \"dodgerblue\", lwd = 2)"
  },
  {
    "objectID": "TD3.html#classification-trees",
    "href": "TD3.html#classification-trees",
    "title": "Lab3: Decision Trees and Random Forests",
    "section": "Classification Trees",
    "text": "Classification Trees\nA classification tree is very similar to a regression tree, except that the classification tree is used to predict a qualitative response rather than a quantitative one. Recall that for a regression tree, the predicted response for an observation is given by the mean response of the training observations that belong to the same terminal node. In contrast, for a classification tree, we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs.\nThe Toy Dataset\nIn order to better understand how a decision tree processes the feature space, we will first work on a simulated dataset.\n\n\n\n\nplt.figure(figsize=(5, 5))\n\nx1 = np.random.multivariate_normal([2,2], [[0.1,0],[0,0.1]], 50)\nx2 = np.random.multivariate_normal([-2,-2], [[0.1,0],[0,0.1]], 50)\nx3 = np.random.multivariate_normal([-3,3], [[0.1,0.1],[0,0.1]], 50)\nX1 = np.concatenate((x1,x2,x3), axis=0)\n\ny1 = np.random.multivariate_normal([-2,2], [[0.1,0],[0,0.1]], 50)\ny2 = np.random.multivariate_normal([2,-2], [[0.1,0],[0,0.1]], 50)\ny3 = np.random.multivariate_normal([-3,-3], [[0.01,0],[0,0.01]], 50)\nX2 = np.concatenate((y1,y2,y3), axis=0)\n\nplt.plot(X1[:,0],X1[:,1], 'x', color='blue', label='class 1')\nplt.plot(X2[:,0], X2[:,1], 'x', color='orange', label='class 2')\n\n\nplt.legend(loc=(0.4, 0.8), fontsize=12)\n\n\nWhat do you expect the decision boudaries to look like ?\nFill-in the following code to train a decision tree on this toy data and visualize it.\n\nChange the splitter to random, meaning that the algorithm will consider the feature along which to split randomly (rather than picking the optimal one), and then select the best among several random splitting point. Run the algorithm several times. What do you observe?\n\n# Training data\nX_demo = np.concatenate((X1, X2), axis=0)\ny_demo = np.concatenate((np.zeros(X1.shape[0]), np.ones(X2.shape[0])))\n\n# Train a DecisionTreeClassifier on the training data\nclf = # TODO\n\n# Create a mesh, i.e. a fine grid of values between the minimum and maximum\n# values of x1 and x2 in the training data\nplot_step = 0.02\nx_min, x_max = X_demo[:, 0].min() - 1, X_demo[:, 0].max() + 1\ny_min, y_max = X_demo[:, 1].min() - 1, X_demo[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                         np.arange(y_min, y_max, plot_step))\n\n# Label each point of the mesh with the trained DecisionTreeClassifier\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\n# Plot the contours corresponding to these labels \n# (i.e. the decision boundary of the DecisionTreeClassifier)\ncs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n\n# Plot the training data \nplt.plot(X1[:,0], X1[:,1], 'x', label='class 1')\nplt.plot(X2[:,0], X2[:,1], 'x', label='class 2')\nplt.legend()\n\nSpam Dataset\nIn this section, we will use the spam6 dataset, available here . A description of the dataset is given below.\n\n\n\n\n\n\nTODO\n\n\n\nYou must:\n\nImport the spam dataset and explore it. Be aware that it is preferable that the response column is of type factor.\nSplit the dataset into training and test sets (choose your own seed when using set.seed()).\nFit:\n\nA logistic regression model.\nA simple classification tree.\nBagging, Random Forests, and Boosting models.\n\n\nFor each model, predict the response on the test set and evaluate the performance of the model, using the prediction accuracy (create a function that returns the accuracy for two binary vectors).\n\n\n\n\n\n\nThis dataset consists of information from 4601 email messages, in a study to try to predict whether the email was junk email, or “spam”. For all 4601 email messages, the true outcome, spam or not, is available, along with 57 predictors as described below:\n\n48 quantitative predictors: the percentage of words in the email that match a given word. Examples include business, address, internet; etc.\n6 quantitative predictors: the percentage of characters in the email that match a given character. The characters are ; , ( , [ , ! , $ and #.\nThe average length of uninterrupted sequences of capital letters: crl.ave.\nThe length of the longest uninterrupted sequence of capital letters: crl.long.\nThe sum of the length of uninterrupted sequences of capital letters: crl.tot.\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that the spam dataset given here is already treated and ready to be explored. To achieve this stage, some steps are required to treat the raw data, like Tokenization, Stemming, and Lemmatization. In this dataset the most important words are already selected and other variables are added. Curious students can read more about these steps. Two famous  packages for text mining are tm and tidytext.\n\n\nTuning\nSo far in this lab, we fit bagging, boosting and random forest models, but did not tune any of them, we simply used certain, somewhat arbitrary, parameters. Actually, to make these models better the parameters should be tuned. The parameters include:\n\nBagging: Actually just a subset of Random Forest with mtry = \\(p\\).\nRandom Forest: mtry\n\nBoosting: n.trees, interaction.depth, shrinkage, n.minobsinnode\n\n\nThe caret package in R provides excellent functions to accomplish this. Note that with these tree-based ensemble methods there are two resampling solutions for tuning the model:\n\nOut of Bag\nCross-Validation\n\nUsing Out of Bag samples is advantageous with these methods as compared to Cross-Validation since it removes the need to refit the model and is thus much more computationally efficient. Unfortunately OOB methods cannot be used with gbm models. See the caret documentation: Short intro, Long intro for details.\n\n\n\nTumor classification data7\n\nThis data set comes from the world of bioinformatics. In this data set, each observation is a tumor, and it is described by the expression of 3,000 genes. The expression of a gene is a measure of how much of that gene is present in the biological sample. Because this affects how much of the protein this gene codes for is produced, and because proteins dictacte what cells can do, gene expression gives us valuable information about the tumor. In particular, the expression of the same gene in the same individual is different in different tissues (although the DNA is the same): this is why blood cells look different from skin cells. In our data set, there are two types of tumors: endometrium tumors and uterine tumors. Let us see if gene expression can be used to separate them!\nThe dataset is available here .\n\n# load the endometrium vs. uterus tumor data\nendometrium_data = pd.read_csv('datasets/small_Endometrium_Uterus.csv', sep=\",\")  # load data\nendometrium_data.head(n=5)  # adjust n to view more data\n\n   ID_REF  1554530_at  1553185_at  ...  1555097_a_at  1556371_at       Tissue\n0  117722        10.8     13233.7  ...          66.9        50.6  Endometrium\n1   76638        12.6      4986.8  ...           6.4        12.2  Endometrium\n2   88952        16.6      6053.8  ...          33.8        33.4  Endometrium\n3   76632         9.9      6109.1  ...          58.9        15.4  Endometrium\n4   88966        13.1      8430.9  ...          14.1        11.2  Endometrium\n\n[5 rows x 3002 columns]\n\n\n\n# Create the design matrix and target vector\nX = endometrium_data.drop(['ID_REF', 'Tissue'], axis=1).values\ny = pd.get_dummies(endometrium_data['Tissue']).values[:,1]\n\nCross Validation procedures\n\n## make folds\nfrom sklearn import model_selection\nskf = model_selection.StratifiedKFold(n_splits=5)\nskf.get_n_splits(X, y)\n\n5\n\nfolds = [(tr,te) for (tr,te) in skf.split(X, y)]\n\n\ndef cross_validate_clf(design_matrix, labels, classifier, cv_folds):\n    \"\"\" Perform a cross-validation and returns the predictions.\n    \n    Parameters:\n    -----------\n    design_matrix: (n_samples, n_features) np.array\n        Design matrix for the experiment.\n    labels: (n_samples, ) np.array\n        Vector of labels.\n    classifier:  sklearn classifier object\n        Classifier instance; must have the following methods:\n        - fit(X, y) to train the classifier on the data X, y\n        - predict_proba(X) to apply the trained classifier to the data X and return probability estimates \n    cv_folds: sklearn cross-validation object\n        Cross-validation iterator.\n        \n    Return:\n    -------\n    pred: (n_samples, ) np.array\n        Vectors of predictions (same order as labels).\n    \"\"\"\n    pred = np.zeros(labels.shape)\n    for tr, te in cv_folds:\n        classifier.fit(design_matrix[tr,:], labels[tr])\n        pos_idx = list(classifier.classes_).index(1)\n        pred[te] = (classifier.predict_proba(design_matrix[te,:]))[:, pos_idx]\n    return pred\n\n\ndef cross_validate_clf_optimize(design_matrix, labels, classifier, cv_folds):\n    \"\"\" Perform a cross-validation and returns the predictions.\n    \n    Parameters:\n    -----------\n    design_matrix: (n_samples, n_features) np.array\n        Design matrix for the experiment.\n    labels: (n_samples, ) np.array\n        Vector of labels.\n    classifier:  sklearn classifier object\n        Classifier instance; must have the following methods:\n        - fit(X, y) to train the classifier on the data X, y\n        - predict_proba(X) to apply the trained classifier to the data X and return probability estimates \n    cv_folds: sklearn cross-validation object\n        Cross-validation iterator.\n        \n    Return:\n    -------\n    pred: (n_samples, ) np.array\n        Vectors of predictions (same order as labels).\n    \"\"\"\n    pred = np.zeros(labels.shape)\n    for tr, te in cv_folds:\n        classifier.fit(design_matrix[tr,:], labels[tr])\n        print(classifier.best_params_)\n        pos_idx = list(classifier.best_estimator_.classes_).index(1)\n        pred[te] = (classifier.predict_proba(design_matrix[te,:]))[:, pos_idx]\n    return pred\n\n\n\nQuestion: Cross-validate 5 different decision trees (with default parameters) and print out their accuracy. Why do you get different values? Check the documentation for help.\n\n\nfrom sklearn import tree\nfrom sklearn import metrics\n\nypred_dt = [] # will hold the 5 arrays of predictions (1 per tree)\nfor tree_index in range(5):\n    # Initialize a DecisionTreeClassifier\n    clf = # TODO \n    \n    # Cross-validate this DecisionTreeClassifier on the toy data\n    pred_proba = cross_validate_clf(X, y, clf, folds)\n    \n    # Append the prediction to ypred_dt \n    ypred_dt.append(pred_proba)\n    \n    # Print the accuracy of DecisionTreeClassifier\n    print(\"%.3f\" % metrics.accuracy_score(y, np.where(pred_proba > 0.5, 1, 0)))\n\n\n\nQuestion: Compute the mean and standard deviation of the area under the ROC curve of these 5 trees. Plot the ROC curves of these 5 trees.\n\nUse the metrics module of scikit-learn.\n\nfpr_dt = [] # will hold the 5 arrays of false positive rates (1 per tree)\ntpr_dt = [] # will hold the 5 arrays of true positive rates (1 per tree)\nauc_dt = [] # will hold the 5 areas under the ROC curve (1 per tree)\n\nfor tree_index in range(5):\n    # Compute the ROC curve of the current tree\n    fpr_dt_tmp, tpr_dt_tmp, thresholds =  metrics.roc_curve(# TODO\n    # Compute the area under the ROC curve of the current tree\n    auc_dt_tmp = metrics.auc(fpr_dt_tmp, tpr_dt_tmp)\n    fpr_dt.append(fpr_dt_tmp)\n    tpr_dt.append(tpr_dt_tmp)\n    auc_dt.append(auc_dt_tmp)\n\n# Plot the first 4 ROC curves\nfor tree_index in range(4):\n    plt.plot(# TODO\n            \n# Plot the last ROC curve, with a label that gives the mean/std AUC\nplt.plot(fpr_dt[-1], tpr_dt[-1], '-', \n         label='DT (AUC = %0.2f +/- %0.2f)' % (np.mean(auc_dt), np.std(auc_dt)))\n\n# Plot the ROC curve\nplt.xlabel('False Positive Rate', fontsize=16)\nplt.ylabel('True Positive Rate', fontsize=16)\nplt.title('ROC curves', fontsize=16)\nplt.legend(loc=\"lower right\")\n\n\n\nQuestion: What parameters of DecisionTreeClassifier can you play with to define trees differently than with the default parameters? Cross-validate these using a grid search with model_selection.GridSearchCV. Plot the optimal decision tree on the previous plot. Did you manage to improve performance?\n\n\nfrom sklearn import model_selection\n\n# Define the grid of parameters to test\nparam_grid = # TODO\n\n# Initialize a GridSearchCV object that will be used to cross-validate\n# a DecisionTreeClassifier with these parameters.\n# What scoring function do you want to use?\nclf = model_selection.GridSearchCV( # TODO\n\n# Cross-validate the GridSearchCV object \nypred_dt_opt = cross_validate_clf_optimize(X, y, clf, folds)\n\n# Compute the ROC curve for the optimized DecisionTreeClassifier\nfpr_dt_opt, tpr_dt_opt, thresholds = metrics.roc_curve(y, ypred_dt_opt, pos_label=1)\nauc_dt_opt = metrics.auc(fpr_dt_opt, tpr_dt_opt)\n\n# Plot the ROC curves of the 5 decision trees from earlier\nfig = plt.figure(figsize=(5, 5))\n\nfor tree_index in range(4):\n    plt.plot(fpr_dt[tree_index], tpr_dt[tree_index], '-', color='blue') \nplt.plot(fpr_dt[-1], tpr_dt[-1], '-', color='blue', \n         label='DT (AUC = %0.2f (+/- %0.2f))' % (np.mean(auc_dt), np.std(auc_dt)))\n\n# Plot the ROC curve of the optimized DecisionTreeClassifier\nplt.plot(fpr_dt_opt, tpr_dt_opt, color='orange', label='DT optimized (AUC=%0.2f)' % auc_dt_opt)\n\nplt.xlabel('False Positive Rate', fontsize=16)\nplt.ylabel('True Positive Rate', fontsize=16)\nplt.title('ROC curves', fontsize=16)\nplt.legend(loc=\"lower right\", fontsize=12)\n\nBagging trees\nWe will resort to ensemble methods to try to improve the performance of single decision trees. Let us start with bagging trees: The different trees are to be built using a bootstrap sample of the data, that is to say, a sample built by randomly drawing n points with replacement from the original data, where n is the number of points in the training set.\nBagging is efficient when used with low bias and high variance weak learners. Indeed, by averaging such estimators, we lower the variance by obtaining a smoother estimator, which is still centered around the true density (low bias).\nBagging decision trees hence makes sense, as decision trees have: * low bias: intuitively, the conditions that are checked become multiplicative so the tree is continuously narrowing down on the data (the tree becomes highly tuned to the data present in the training set). * high variance: decision trees are very sensitive to where it splits and how it splits. Therefore, even small changes in input variable values might result in very different tree structure.\nNote: Bagging trees and random forests start being really powerful when using large number of trees (several hundreds). This is computationally more intensive, especially when the number of features is large, as in this lab. For the sake of computational time, we suggeste using small numbers of trees, but you might want to repeat this lab for larger number of trees at home.\n\n\nQuestion Cross-validate a bagging ensemble of 5 decision trees on the data. Plot the resulting ROC curve, compared to the 5 decision trees you trained earlier.\n\nUse ensemble.BaggingClassifier.\n\nfrom sklearn import ensemble\n\n# Initialize a bag of trees\nclf = # TODO\n\n# Cross-validate the bagging trees on the tumor data\nypred_bt = cross_validate_clf(X, y, clf, folds)\n\n# Compute the ROC curve of the bagging trees\nfpr_bt, tpr_bt, thresholds = metrics.roc_curve(y, ypred_bt, pos_label=1)\nauc_bt = metrics.auc(fpr_bt, tpr_bt)\n\n# Plot the ROC curve of the 5 decision trees from earlier\nfig = plt.figure(figsize=(5, 5))\n\nfor tree_index in range(4):\n    plt.plot(fpr_dt[tree_index], tpr_dt[tree_index], '-', color='blue') \nplt.plot(fpr_dt[-1], tpr_dt[-1], '-', color='blue', \n         label='DT (AUC = %0.2f (+/- %0.2f))' % (np.mean(auc_dt), np.std(auc_dt)))\n\n# Plot the ROC curve of the bagging trees\nplt.plot(fpr_bt, tpr_bt, color='orange', label='BT (AUC=%0.2f)' % auc_bt)\n\n\nplt.xlabel('False Positive Rate', fontsize=16)\nplt.ylabel('True Positive Rate', fontsize=16)\nplt.title('ROC curves', fontsize=16)\nplt.legend(loc=\"lower right\", fontsize=12)\n\n\nQuestion: How do the bagging trees perform compared to individual trees?\nQuestion Use cross_validate_optimize to optimize the number of decision trees to use in the bagging method. How many trees did you find to be an optimal choice?\n\n\n# Number of trees to use\nlist_n_trees = [5, 10, 20, 50, 80]\n\n# Start a ROC curve plot\nfig = plt.figure(figsize=(5, 5))\n    \nfor idx, n_trees in enumerate(list_n_trees):\n    # Initialize a bag of trees with n_trees trees\n    clf = # TODO\n    \n    # Cross-validate the bagging trees on the tumor data\n    ypred_bt_tmp = cross_validate_clf(X, y, clf, folds)\n    \n    # Compute the ROC curve \n    fpr_bt_tmp, tpr_bt_tmp, thresholds = metrics.roc_curve(y, ypred_bt_tmp, pos_label=1)\n    auc_bt_tmp = metrics.auc(fpr_bt_tmp, tpr_bt_tmp)\n\n    # Plot the ROC curve\n    plt.plot(fpr_bt_tmp, tpr_bt_tmp, '-', \n             label='BT %0.f trees (AUC = %0.2f)' % (n_trees, auc_bt_opt))\n\n# Plot the ROC curve of the optimal decision tree\nplt.plot(fpr_dt_opt, tpr_dt_opt, label='DT optimized (AUC=%0.2f)' % auc_dt_opt)\n\nplt.xlabel('False Positive Rate', fontsize=16)\nplt.ylabel('True Positive Rate', fontsize=16)\nplt.title('ROC curves', fontsize=16)\nplt.legend(loc=\"lower right\", fontsize=12)\n\nRandom Forest\nIn practice, simply bagging is typically not enough. In order to get a good reduction in variance, we require that the models being aggregated be uncorrelated, so that they make “different errors”. Bagging will usually get you highly correlated models that will make the same errors, and will therefore not reduce the variance of the combined predictor.\n\nQuestion What is the difference between bagging trees and random forests? How does it intuitively fix the problem of correlations between trees ?\nQuestion Cross-validate a random forest of 5 decision trees on the data. Plot the resulting ROC curve, compared to the bagging tree made of 5 decision trees.\n\nUse ensemble.RandomForestClassifier\n\n# Initialize a random forest with 5 trees\nclf = # TODO\n\n# Cross-validate the random forest on the tumor data\nypred_rf = # TODO\n\n# Compute the ROC curve of the random forest\nfpr_rf, tpr_rf, thresholds = # TODO\nauc_rf = # TODO\n\n# Plot the ROC curve of the 5 decision trees from earlier\nfig = plt.figure(figsize=(5, 5))\n\nfor tree_index in range(4):\n    plt.plot(fpr_dt[tree_index], tpr_dt[tree_index], '-', color='grey') \nplt.plot(fpr_dt[-1], tpr_dt[-1], '-', color='grey', \n         label='DT (AUC = %0.2f (+/- %0.2f))' % (np.mean(auc_dt), np.std(auc_dt)))\n\n# Plot the ROC curve of the bagging trees (5 trees)\nplt.plot(fpr_bt, tpr_bt, label='BT (AUC=%0.2f)' % auc_bt)\n\n# Plot the ROC curve of the random forest (5 trees)\nplt.plot(fpr_rf, tpr_rf, label='BT (AUC=%0.2f)' % auc_bt)\n\n\nplt.xlabel('False Positive Rate', fontsize=16)\nplt.ylabel('True Positive Rate', fontsize=16)\nplt.title('ROC curves', fontsize=16)\nplt.legend(loc=\"lower right\", fontsize=12)\n\n\nQuestion What are the main parameters of Random Forest which can be optimized ?\n\nQuestion Use cross_validate_clf_optimize to optimize\n\nthe number of decision trees\nthe number of features to consider at each split.\n\n\n\nHow many trees do you find to be an optimal choice? How does the optimal random forest compare to the optimal bagging trees? How do the training times of the random forest and the bagging trees compare?\n\n# Define the grid of parameters to test\nparam_grid = # TODO\n\n# Initialize a GridSearchCV object that will be used to cross-validate\n# a random forest with these parameters.\n# What scoring function do you want to use?\nclf = grid_search.GridSearchCV(# TODO\n\n# Cross-validate the GridSearchCV object \nypred_rf_opt = cross_validate_clf_optimize(X, y, clf, folds)\n\n# Compute the ROC curve for the optimized random forest\nfpr_rf_opt, tpr_rf_opt, thresholds = metrics.roc_curve(y, ypred_rf_opt, pos_label=1)\nauc_rf_opt = metrics.auc(fpr_rf_opt, tpr_rf_opt)\n\n# Plot the ROC curve of the optimized DecisionTreeClassifier\nfig = plt.figure(figsize=(5, 5))\n\nplt.plot(fpr_dt_opt, tpr_dt_opt, color='grey', \n         label='DT optimized (AUC=%0.2f)' % auc_dt_opt)\n    \n# Plot the ROC curve of the optimized random forest\nplt.plot(fpr_bt_opt, tpr_bt_opt, \n         label='BT optimized (AUC=%0.2f)' % auc_bt_opt)\n\n# Plot the ROC curve of the optimized bagging trees\nplt.plot(fpr_rf_opt, tpr_rf_opt, l\n         abel='RF optimized (AUC = %0.2f' % (auc_rf_opt))\n    \nplt.xlabel('False Positive Rate', fontsize=16)\nplt.ylabel('True Positive Rate', fontsize=16)\nplt.title('ROC curves', fontsize=16)\nplt.legend(loc=\"lower right\", fontsize=12)\n\n\n\nQuestion How do your tree-based classifiers compare to regularized logistic regression models? Plot the corresponding ROC curves.\n\n\nfrom sklearn import linear_model\n\n# Evaluate an optimized l1-regularized logistic regression\nparam_grid = {'C': np.logspace(-3, 3, 7)}\nclf = grid_search.GridSearchCV(linear_model.LogisticRegression(penalty='l1'), \n                               param_grid, scoring='roc_auc')\nypred_l1 = cross_validate_clf_optimize(X, y, clf, folds)\nfpr_l1, tpr_l1, thresholds_l1 = metrics.roc_curve(y, ypred_l1, pos_label=1)\nauc_l1 = metrics.auc(fpr_l1, tpr_l1)\nprint('nb features of best sparse model:', len(np.where(clf.best_estimator_.coef_!=0)[0]))\n\n# Evaluate an optimized l2-regularized logistic regression\nclf = grid_search.GridSearchCV(linear_model.LogisticRegression(penalty='l2'), \n                               param_grid, scoring='roc_auc')\nypred_l2 = cross_validate_clf_optimize(X, y, clf, folds)\nfpr_l2, tpr_l2, thresholds_l2 = metrics.roc_curve(y, ypred_l2, pos_label=1)\nauc_l2 = metrics.auc(fpr_l2, tpr_l2)\n\n\n# Plot the ROC curves\nfig = plt.figure(figsize=(5, 5))\n\nplt.plot(fpr_rf_opt, tpr_rf_opt, \n         label='RF optimized (AUC = %0.2f)' % (auc_rf_opt))\nplt.plot(fpr_bt_opt, tpr_bt_opt, \n         label='BT optimized (AUC = %0.2f)' % (auc_bt_opt))\nplt.plot(fpr_l1, tpr_l1,  \n         label='l1 optimized (AUC = %0.2f)' % (auc_l1))\nplt.plot(fpr_l2, tpr_l2,  \n         label='l2 optimized (AUC = %0.2f)' % (auc_l2))\n\nplt.xlabel('False Positive Rate', fontsize=16)\nplt.ylabel('True Positive Rate', fontsize=16)\nplt.title('ROC curves', fontsize=16)\nplt.legend(loc=\"lower right\", fontsize=12)"
  },
  {
    "objectID": "TD4.html#image-classification",
    "href": "TD4.html#image-classification",
    "title": "Lab4: Starting with Neural Networks",
    "section": "Image Classification",
    "text": "Image Classification\nIn the set of this exercises we will be using tf.keras (a high-level API to build and train models in TensorFlow) and GoogleColab.\n1. Open a new notebook in GoogleColab for python3. Run the following code for activating tensorflow version 2.0:\n\ntry:\n    # %tensorflow_version only exists in Colab.\n    %tensorflow_version 2.x\nexcept Exception: \n    pass\n\n2. Import tensorflow, keras, numpy and matplot using the following code:\n\nfrom __future__ import absolute_import , division , print_function , unicode_literals\n\n# TensorFlow and tf.keras\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Helper libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n3. Check your tensorflow version using:\n\nprint(tf.__version__)\n\nThe output should be 2. or higher.\n4. Import the cifra10 data set. The CIFAR-10 dataset consists of 60000 32 × 32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images: cifar10\n\ndata = keras.datasets.cifar10 \ncifar10_data = data.load_data()\n\n5. Before using a dataset, the datatype should be checked. Test type(cifar10 data) for verifying the variable type. len(cifar10 data) is another command for checking the data size.\n6. Load train and test images and labels with:\n\n(train_images , train_labels),(test_images , test_labels) =\ncifar10_data\n\nLoading the dataset returns four NumPy arrays:\n\nThe train_images and train_labels arrays are the training set, the data the model uses to learn.\nThe model is tested against the test set, the test_images, and test_labels arrays.\n\n7. The images are 32 × 32 NumPy arrays, with pixel values ranging from 0 to 255. You can check an example with:\n\nprint(train_images[0]) \nprint(train_images[0].shape)\n\nThe labels are an array of integers, ranging from 0 to 9. Check it with your own code. Each image is mapped to a single label. Since the class names are not included with the dataset, store them here to use later when plotting the images: (check the dataset link for more detailed information: cifat\n8. Before training the model, explore the datasets. Number of train and test points, their array size and etc.\n9. An interesting fact about the image is that you can plot the image. It is possible using the following:\n\nindex = 8 \nplt.figure()\nplt.imshow(train_images[index]) \nplt.colorbar()\nplt.grid(False) \nplt.show()\n\ntrain_labels[index]\n\n10. To verify that the data is in the correct format and that you’re ready to build and train the network, let’s display the first 25 images from the training set and display the class name below each image. To do so use the following commands:\n\nplt.subplot() \nplt.xticks([])\nplt.yticks([]) \nplt.imshow()\nplt.xlabel()\n\n11. You can check various images by changing the index value, or by calling test_images. You can see that the pixel values fall in the range of 0 to 255. normalise train and test sets using the following code:\n\ntrain_images = train_images / 255.0\n\nBuilding a neural network in general requires configuring the layers of the model, then compiling the model. The basic building block of a neural network is the layer. Layers extract representations from the data fed into them. Hopefully, these representations are meaningful for the problem at hand. Most of deep learning consists of chaining together simple layers. Most layers, such as tf.keras.layers.Dense, have parameters that are learned during training.\n12. First neural network definition with three layers and two activation functions\n\nmodel = keras.Sequential([ \n    keras.layers.Flatten(input_shape=(32, 32, 3)),\n    keras.layers.Dense(128, activation=’relu’), \n    keras.layers.Dense(10, activation=’softmax’)])\n\n\n\n\n\n\n\nFor more information on the keras code check keras website here\n\n\n\nThe first layer in this network, tf.keras.layers.Flatten, transforms the format of the images from a two-dimensional array (of 32 by 32 pixels) to a one- dimensional array (of 32 × 32 = 1024 pixels). Think of this layer as unstacking rows of pixels in the image and lining them up. This layer has no parameters to learn; it only reformats the data.\nAfter the pixels are flattened, the network consists of a sequence of two tf.keras.layers.Dense layers. These are densely connected, or fully con- nected, neural layers. The first Dense layer has 128 nodes (or neurons). The second (and last) layer is a 10-node softmax layer that returns an array of 10 probability scores that sum to 1. Each node contains a score that indicates the probability that the current image belongs to one of the 10 classes. In this exercise, we don’t explain the reasons of defining a neural network with this structure. For defining a network compatible with our data, we should define an input layer with the same size as the input data (images size) and an output corresponding the out put data (image labels).\n13. Before the model is ready for training, it needs a few more settings. These are added during the model’s compile step:\n\nLoss function: This measures how accurate the model is during training. You want to minimize this function to ”steer” the model in the right direction.\nOptimizer: This is how the model is updated based on the data it sees and its loss function.\nMetrics: Used to monitor the training and testing steps. The follow- ing example uses accuracy, the fraction of the images that are correctly classified.\n\n14. Training the neural network model requires the following steps:\n\nFeed the training data to the model. In this example, the training data is in the train images and train labels arrays.\nThe model learns to associate images and labels.\nYou ask the model to make predictions about a test set, in this example, the test images array. Verify that the predictions match the labels from the test labels array.\n\n\nmodel.compile(optimizer=’adam’,\n                loss=’sparse_categorical_crossentropy’,\n                metrics=[’accuracy’])\n\nTo start training, call the model.fit method, so called because it ”fits” the model to the training data:\n\nmodel.fit(train_images , train_labels , epochs=10)\n\n15. It is the moment for checking the model performance on the test dataset.\n\ntest_loss , test_acc = model.evaluate(test_images , test_labels ,\nverbose =2)\n\nCheck the test loss and accuracy in your code.\n16. With the model trained, we can use it to make predictions about some images.\n\npredictions = model.predict(test_images)\n\nThe model predicts a label for each image in the testing set. Print the first, second and third element of the predicted test sets. You can see that each element contains 10 values indicating a probability of each label. Choose the maximum one using np.argmax() function. Compare the predicted label of the first three elements with their predicted labels. How many are correct?\n17. Write a function for checking the predicted labels. The result should be similar to the above Figure with a label indicating the probability of the predicted label with blue color if the prediction is correct otherwise in the red color?\n18. Grab a single element from the test set such as test_images[5]. Send it to the model.predict() and check what will happen. Why? Correct it by your modification. (hint: you can use expand_dims())\n19. Respecting input and output sizes, try to change your model structure in exercises 12 and 13 and observe their affections on prediction precision.\n\n\n\n\n\n\nTip\n\n\n\nYou can choose another image classification dataset from Tensorflow available datasets https://www.tensorflow.org/datasets/catalog/overview and predict a classification function for it."
  },
  {
    "objectID": "TD4.html#sentiment-analysis",
    "href": "TD4.html#sentiment-analysis",
    "title": "Lab4: Starting with Neural Networks",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\nIn this exercise, you’ll build a neural network using Keras to classify texts into ones with positive and negative sentiments; sentiment analysis.\nImport the necessary Libraries\n\nimport numpy as np\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import models\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.datasets import imdb\n\n1. Load the IMDB dataset\n\n(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=10000)\ndata = np.concatenate((training_data, testing_data), axis=0)\ntargets = np.concatenate((training_targets, testing_targets), axis=0)\n\n2. Take only the first 10000 words from each data sample. This reduces the size of our final model during training.\n3. Build your Neural Network model\n4. Compile your model\n5. Fit your model, and get its final accuracy"
  },
  {
    "objectID": "TD4.html#regression",
    "href": "TD4.html#regression",
    "title": "Lab4: Starting with Neural Networks",
    "section": "Regression",
    "text": "Regression\nIn this exercise, you will create a regressor on the Boston housing Dataset, a task that you’ve previously accomplished using linear regression and decision trees. only this time, you’ll accomplish it using Keras neural networks.\nImport the Boston Housing dataset\n\nfrom sklearn.datasets import load_boston\nimport pandas as pd\nboston_dataset = load_boston()\ndf = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\ndf['MEDV'] = boston_dataset.target\n\n1. Visualize each feature and label in your data using a scatterplot. This will help in finding which features, if any, contain outliers. It will also assist in finding potential strong correlations between features.\n2. Split your data into training and testing\n3. Normalize your training and testing subsets\n4. Build your keras neural network model. Create a Sequential model, and make it only with 3 layers: an input (Dense) layer with 128 neurons, a hidden (Dense) layer with 64 neurons, both using a ReLU (Rectified Linear Unit) activation function, and a dense layer with a linear activation will be used as output layer.\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n#model =\n\n5. Compile your model and view its summary.\nTo compile your model, use the adam opimizer, and the mse (mean-squared-error) loss function, and the mae (mean average error) metric to report its performance.\n\nmodel.compile(optimizer='adam', loss='mse', metrics=['mae'])\nmodel.summary()\n\n6. Train your model\n7. Evaluate your model using model.evaluate\n\nmse_nn, mae_nn = model.evaluate(X_test, y_test)\nprint('Mean squared error on test data: ', mse_nn)\nprint('Mean absolute error on test data: ', mae_nn)\n\n8. Compare your model’s performance vs that of an sklearn Linear Regression model"
  }
]