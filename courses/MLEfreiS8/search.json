[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "In this course you will learn about the state of the art of Machine Learning and also gain practice implementing and deploying machine learning algorithms."
  },
  {
    "objectID": "discriminantanalysis.html",
    "href": "discriminantanalysis.html",
    "title": "\n1¬† Discriminant Analysis\n",
    "section": "",
    "text": "Slides\n\n\n\nSlides for this chapter: üìÑ\nDiscriminant analysis is a popular method for multiple-class classiÔ¨Åcation. We will start first by the Linear Discriminant Analysis (LDA)."
  },
  {
    "objectID": "discriminantanalysis.html#introduction",
    "href": "discriminantanalysis.html#introduction",
    "title": "\n1¬† Discriminant Analysis\n",
    "section": "\n1.1 Introduction",
    "text": "1.1 Introduction\nAs we saw previously, Logistic regression involves directly modeling \\(\\mathbb{P} (Y = k|X = x)\\) using the logistic function, for the case of two response classes. In logistic regression, we model the conditional distribution of the response \\(Y\\), given the predictor(s) \\(X\\). We now consider an alternative and less direct approach to estimating these probabilities. In this alternative approach, we model the distribution of the predictors \\(X\\) separately in each of the response classes (i.e.¬†given \\(Y\\)), and then use Bayes‚Äô theorem to flip these around into estimates for \\(\\mathbb{P} (Y = k|X = x)\\). When these distributions are assumed to be Normal, it turns out that the model is very similar in form to logistic regression.\nWhy not logistic regression? Why do we need another method, when we have logistic regression? There are several reasons:\n\nWhen the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suÔ¨Äer from this problem.\nIf \\(n\\) is small and the distribution of the predictors \\(X\\) is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.\nLinear discriminant analysis is popular when we have more than two response classes."
  },
  {
    "objectID": "discriminantanalysis.html#bayes-theorem",
    "href": "discriminantanalysis.html#bayes-theorem",
    "title": "\n1¬† Discriminant Analysis\n",
    "section": "\n1.2 Bayes‚Äô Theorem",
    "text": "1.2 Bayes‚Äô Theorem\nBayes‚Äô theorem is stated mathematically as the following equation:\n\\[ P(A | B) = \\frac{P(A \\cap B)}{P(B)} =  \\frac{P(B|A) P(A)}{P(B)}\\]\nwhere \\(A\\) and \\(B\\) are events and \\(P(B) \\neq 0\\).\n\n\n\\(P(A | B)\\), a conditional probability, is the probability of observing event \\(A\\) given that \\(B\\) is true. It is called the posterior probability.\n\n\\(P(A)\\), is called the prior, is the initial degree of belief in A.\n\n\\(P(B)\\) is the likelihood.\n\n\n\n\n\n\n\nTip\n\n\n\nThe posterior probability can be written in the memorable form as :\nPosterior probability \\(\\propto\\) Likelihood \\(\\times\\) Prior probability.\n\n\nExtended form:\nSuppose we have a partition \\(\\{A_i\\}\\) of the sample space, the even space is given or conceptualized in terms of \\(P(A_j)\\) and \\(P(B | A_j)\\). It is then useful to compute \\(P(B)\\) using the law of total probability:\n\\[ P(B) = \\sum_j P(B|A_j) P(A_j) \\]\n\\[ \\Rightarrow P(A_i|B) = \\frac{P(B|A_i) P(A_i)}{\\sum_j P(B|A_j) P(A_j)} \\]\nBayes‚Äô Theorem for Classification:\nSuppose that we wish to classify an observation into one of \\(K\\) classes, where \\(K \\geq 2\\). In other words, the qualitative response variable \\(Y\\) can take on \\(K\\) possible distinct and unordered values.\nLet \\(\\pi_k\\) represent the overall or prior probability that a randomly chosen observation comes from the \\(k\\)-th class; this is the probability that a given observation is associated with the \\(k\\)-th category of the response variable \\(Y\\).\nLet \\(f_k(X) \\equiv P(X = x|Y = k)\\) denote the density function of \\(X\\) for an observation that comes from the \\(k\\)-th class. In other words, \\(f_k(x)\\) is relatively large if there is a high probability that an observation in the \\(k\\)-th class has \\(X \\approx x\\), and \\(f_k(x)\\) is small if it is very unlikely that an observation in the \\(k\\)-th class has \\(X \\approx x\\). Then Bayes‚Äô theorem states that\n\\[\nP(Y=k|X=x) = \\frac{ \\pi_k f_k(x)}{\\sum_{c=1}^K \\pi_c f_c(x)}\n\\qquad(1.1)\\]\nAs we did in the last chapter, we will use the abbreviation \\(p_k(X) =P(Y = k|X)\\).\nThe equation above stated by Bayes‚Äô theorem suggests that instead of directly computing \\(p_k(X)\\) as we did in the logistic regression, we can simply plug in estimates of \\(\\pi_k\\) and \\(f_k(X)\\) into the equation. In general, estimating \\(\\pi_k\\) is easy (the fraction of the training observations that belong to the \\(k\\)-th class). But estimating \\(f_k(X)\\) tends to be more challenging.\n\nRecall that \\(p_k(x)\\) is the posterior probability that an observation \\(X=x\\) belongs to \\(k\\)-th class.\n\n\nIf we can find a way to estimate \\(f_k(X)\\), we can develop a classifier with the lowest possibe error rate out of all classifiers."
  },
  {
    "objectID": "discriminantanalysis.html#lda-for-p1",
    "href": "discriminantanalysis.html#lda-for-p1",
    "title": "\n1¬† Discriminant Analysis\n",
    "section": "\n1.3 LDA for \\(p=1\\)\n",
    "text": "1.3 LDA for \\(p=1\\)\n\nAssume that \\(p=1\\), which mean we have only one predictor. We would like to obtain an estimate for \\(f_k(x)\\) that we can plug into the Equation (1.1) in order to estimate \\(p_k(x)\\). We will then classify an observation to the class for which \\(p_k(x)\\) is greatest.\nIn order to estimate \\(f_k(x)\\), we will first make some assumptions about its form.\nSuppose we assume that \\(f_k(x)\\) is normal (Gaussian). In the one-dimensional setting, the normal density take the form\n\\[\nf_k(x)= \\frac{1}{\\sigma_k\\sqrt{2\\pi}} \\exp \\big( - \\frac{1}{2\\sigma_k^2 } (x-\\mu_k)^2\\big)\n\\qquad(1.2)\\]\nwhere \\(\\mu_k\\) and \\(\\sigma_k^2\\) are the mean and variance parameters for \\(k\\)-th class. Let us assume that \\(\\sigma_1^2 = \\ldots = \\sigma_K^2 = \\sigma^2\\) (which means there is a shared variance term across all \\(K\\) classes). Plugging Equation (1.2) into the Bayes formula in Equation (1.1) we get,\n\\[\np_k(x) = \\frac{  \\pi_k \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2} \\big(\\frac{x-\\mu_k}{\\sigma}\\big)^2 } }{  \\sum_{c=1}^K  \\pi_c \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2} \\big(\\frac{x-\\mu_c}{\\sigma}\\big)^2 } }\n\\qquad(1.3)\\]\n\n\n\n\n\n\nImportant\n\n\n\nNote that \\(\\pi_k\\) and \\(\\pi_c\\) denote the prior probabilities. And \\(\\pi\\) is the mathematical constant \\(\\pi \\approx 3.14159\\).\n\n\nTo classify at the value \\(X = x\\), we need to see which of the \\(p_k(x)\\) is largest. Taking logs, and discarding terms that do not depend on \\(k\\), we see that this is equivalent to assigning \\(x\\) to the class with the largest discriminant score:\n\\[\n\\delta_k(x) = x.\\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + \\log (\\pi_k)\n\\qquad(1.4)\\]\nNote that \\(\\delta_k(x)\\) is a linear function of \\(x\\).\n\n\n\n\n\n\nTip\n\n\n\n\nThe decision surfaces (e.g.¬†decision boundaries) for a linear discriminant classifiers are defined by the linear equations \\(\\delta_k(x) = \\delta_c(x)\\), for all classes \\(k\\neq c\\). It represents the set of values \\(x\\) for which the probability of belonging to classes \\(k\\) and \\(c\\) is the same, \\(0.5\\).\nExample: If \\(K=2\\) and \\(\\pi_1=\\pi_2\\), then the desicion boundary is at \\(x=\\frac{\\mu_1+\\mu2}{2}\\) (Prove it!).\nAn example where \\(\\mu_1=-1.5\\), \\(\\mu_2=1.5\\), \\(\\pi_1=\\pi_2=0.5\\) and \\(\\sigma^2=1\\) is shown in this following figure\n\n\n\n\n\nSee this  video  to understand more about decision boundary (Applied on logistic regression).\nAs we classify a new point according to which density is highest, when the priors are diÔ¨Äerent we take them into account as well, and compare \\(\\pi_k f_k(x)\\). On the right of the following figure, we favor the pink class (remark that the decision boundary has shifted to the left)."
  },
  {
    "objectID": "discriminantanalysis.html#estimating-the-parameters",
    "href": "discriminantanalysis.html#estimating-the-parameters",
    "title": "\n1¬† Discriminant Analysis\n",
    "section": "\n1.4 Estimating the parameters",
    "text": "1.4 Estimating the parameters\nTypically we don‚Äôt know these parameters; we just have the training data. In that case we simply estimate the parameters and plug them into the rule.\nLet \\(n\\) the total number of training observations, and \\(n_k\\) the number of training observations in the \\(k\\)-th class. The following estimates are used:\n\\[\\begin{align*}\n\\hat{\\pi}_k &= \\frac{n_k}{n} \\\\\n\\hat{\\mu}_k &= \\frac{1}{n_k} \\sum_{i: y_i=k} x_i \\\\\n\\hat{\\sigma}^2 &= \\frac{1}{n - K} \\sum_{k=1}^K \\sum_{i: y_i=k} (x_i-\\hat{\\mu}_k)^2 \\\\\n&= \\sum_{k=1}^K \\frac{n_k-1}{n - K} . \\hat{\\sigma}_k^2\n\\end{align*}\\]\nwhere \\(\\hat{\\sigma}_k^2 = \\frac{1}{n_k-1}\\sum_{i: y_i=k}(x_i-\\hat{\\mu}_k)^2\\) is the usual formula for the estimated variance in the -\\(k\\)-th class.\nThe linear discriminant analysis (LDA) classifier plugs these estimates in Equation (1.4) and assigns an observation \\(X=x\\) to the class for which\n\\[\n\\hat{\\delta}_k(x) = x.\\frac{\\hat{\\mu}_k}{\\hat{\\sigma}^2} - \\frac{\\hat{\\mu}_k^2}{2\\hat{\\sigma}^2} + \\log (\\hat{\\pi}_k)\n\\qquad(1.5)\\]\nis largest.\nThe discriminant functions in Equation (1.5) are linear functions of \\(x\\).\n\nRecall that we assumed that the observations come from a normal distribution with a common variance \\(\\sigma^2\\)."
  },
  {
    "objectID": "discriminantanalysis.html#lda-for-p-1",
    "href": "discriminantanalysis.html#lda-for-p-1",
    "title": "\n1¬† Discriminant Analysis\n",
    "section": "\n1.5 LDA for \\(p > 1\\)\n",
    "text": "1.5 LDA for \\(p > 1\\)\n\nLet us now suppose that we have multiple predictors. We assume that \\(X=(X_1,X_2,\\ldots,X_p)\\) is drawn from multivariate Gaussian distribution (assuming they have a common covariance matrix, e.g.¬†same variances as in the case of \\(p=1\\)). The multivariate Gaussian distribution assumes that each individual predictor follows a one-dimensional normal distribution as in Equation (1.2), with some correlation between each pair of predictors.\nTo indicate that a \\(p\\)-dimensional random variable \\(X\\) has a multivariate Gaussian distribution, we write \\(X \\sim \\mathcal{N}(\\mu,\\Sigma)\\). Where\n\\[\n\\mu = E(X) = \\begin{pmatrix}\n    \\mu_1 \\\\\n    \\mu_2 \\\\\n    \\vdots \\\\\n    \\mu_p\n\\end{pmatrix}\n\\]\nand,\n\\[\n\\Sigma = Cov(X) = \\begin{pmatrix}\n    \\sigma_1^2 & Cov[X_1, X_2]  & \\dots  & Cov[X_1, X_p] \\\\\n    Cov[X_2, X_1] & \\sigma_2^2  & \\dots  & Cov[X_2, X_p] \\\\\n    \\vdots & \\vdots &  \\ddots & \\vdots \\\\\n    Cov[X_p, X_1] & Cov[X_p, X_2]  & \\dots  & \\sigma_p^2\n\\end{pmatrix}  \n\\]\n\n\\(\\Sigma\\) is the \\(p\\times p\\) covariance matrix of \\(X\\).\n\nFormally, the multivariate Gaussian density is deÔ¨Åned as\n\\[\nf(x) = \\frac{1}{(2\\pi)^{p/2} |\\Sigma|^{1/2}} \\exp \\bigg( - \\frac{1}{2} (x-\\mu)^T \\Sigma^{-1} (x-\\mu) \\bigg)\n\\]\nPlugging the density function for the \\(k\\)-th class, \\(f_k(X=x)\\), into Equation (1.1) reveals that the Bayes classifier assigns an observation \\(X=x\\) to the class for which\n\\[\n\\delta_k(x) = x^T \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\Sigma^{-1}  \\mu_k + \\log \\pi_k\n\\qquad(1.6)\\]\nis largest. This is the vector/matrix version of (1.4).\nAn example is shown in the following figure. Three equally-sized Gaussian classes are shown with class-specific mean vectors and a common covariance matrix (\\(\\pi_1=\\pi_2=\\pi_3=1/3\\)). The three ellipses represent regions that contain \\(95\\%\\) of the probability for each of the three classes. The dashed lines are the Bayes decision boundaries.\n\n\n\nRecall that the decision boundaries represent the set of values \\(x\\) for which \\(\\delta_k(x)=\\delta_c(x)\\); i.e.¬†for \\(k\\neq c\\).\n\\[ x^T \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\Sigma^{-1}  \\mu_k = x^T \\Sigma^{-1} \\mu_c - \\frac{1}{2} \\mu_c^T \\Sigma^{-1}  \\mu_c  \\]\nNote that there are three lines representing the Bayes decision boundaries because there are three pairs of classes among the three classes. That is, one Bayes decision boundary separates class 1 from class 2, one separates class 1 from class 3, and one separates class 2 from class 3. These three Bayes decision boundaries divide the predictor space into three regions. The Bayes classiÔ¨Åer will classify an observation according to the region in which it is located.\nOnce again, we need to estimate the unknown parameters \\(\\mu_1,\\ldots,\\mu_k,\\) and \\(\\pi_1,\\ldots,\\pi_k,\\) and \\(\\Sigma\\); the formulas are similar to those used in the one-dimensional case. To assign a new observation \\(X = x\\), LDA plugs these estimates into Equation (1.6) and classiÔ¨Åes to the class for which \\(\\delta_k(x)\\) is largest.\nNote that in Equation (1.6) \\(\\delta_k(x)\\) is a linear function of \\(x\\); that is, the LDA decision rule depends on \\(x\\) only through a linear combination of its elements (e.g.¬†the decision boundaries are linear). This is the reason for the word linear in LDA."
  },
  {
    "objectID": "discriminantanalysis.html#making-predictions",
    "href": "discriminantanalysis.html#making-predictions",
    "title": "\n1¬† Discriminant Analysis\n",
    "section": "\n1.6 Making predictions",
    "text": "1.6 Making predictions\nOnce we have estimates \\(\\hat{\\delta}_k(x)\\), we can turn these into estimates for class probabilities:\n\\[ \\hat{P}(Y=k|X=x)= \\frac{e^{\\hat{\\delta}_k(x)}}{\\sum_{c=1}^K e^{\\hat{\\delta}_c(x)}} \\]\nSo classifying to the largest \\(\\hat{\\delta}_k(x)\\) amounts to classifying to the class for which \\(\\hat{P}(Y=k|X=x)\\) is largest.\nWhen \\(K=2\\), we classify to class 2 if \\(\\hat{P}(Y=2|X=x) \\geq 0.5\\), else to class \\(1\\)."
  },
  {
    "objectID": "discriminantanalysis.html#other-forms-of-discriminant-analysis",
    "href": "discriminantanalysis.html#other-forms-of-discriminant-analysis",
    "title": "\n1¬† Discriminant Analysis\n",
    "section": "\n1.7 Other forms of Discriminant Analysis",
    "text": "1.7 Other forms of Discriminant Analysis\n\\[P(Y=k|X=x) = \\frac{ \\pi_k f_k(x)}{\\sum_{c=1}^K \\pi_c f_c(x)}\\]\nWe saw before that when \\(f_k(x)\\) are Gaussian densities, whith the same covariance matrix \\(\\Sigma\\) in each class, this leads to Linear Discriminant Analysis (LDA).\nBy altering the forms for \\(f_k(x)\\), we get different classifiers.\n\nWith Gaussians but different \\(\\Sigma_k\\) in each class, we get Quadratic Discriminant Analysis (QDA).\nWith \\(f_k(x) = \\prod_{j=1}^p f_{jk}(x_j)\\) (conditional independence model) in each class we get Naive Bayes. (For Gaussian, this mean the \\(\\Sigma_k\\) are diagonal, e.g.¬†\\(Cov(X_i,X_j)=0 \\,\\, \\forall \\, \\, 1\\leq i,j \\leq p\\)).\nMany other forms by proposing specific density models for \\(f_k(x)\\), including nonparametric approaches.\n\n\n1.7.1 Quadratic Discriminant Analysis (QDA)\nLike LDA, the QDA classiÔ¨Åer results from assuming that the observations from each class are drawn from a Gaussian distribution, and plugging estimates for the parameters into Bayes‚Äô theorem in order to perform prediction.\nHowever, unlike LDA, QDA assumes that each class has its own covariance matrix. Under this assumption, the Bayes classiÔ¨Åer assigns an observation \\(X = x\\) to the class for which\n\\[\\begin{align*}\n\\delta_k(x) &= - \\frac{1}{2} (x-\\mu)^T \\Sigma_k^{-1} (x-\\mu) - \\frac{1}{2} \\log |\\Sigma_k| + \\log \\pi_k \\\\\n            &= - \\frac{1}{2} x^T \\Sigma_k^{-1} x + \\frac{1}{2} x^T \\Sigma_k^{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\Sigma_k^{-1} \\mu_k - \\frac{1}{2} \\log |\\Sigma_k| + \\log \\pi_k\n\\end{align*}\\]\nis largest.\nUnlike in LDA, the quantity \\(x\\) appears as a quadratic function in QDA. This is where QDA gets its name.\n\n\n\n\n\n\nImportant\n\n\n\nThe decision boundary in QDA is non-linear. It is quadratic (a curve).\n\n\n\n1.7.2 Naive Bayes\nWe use Naive Bayes classifier if the features are independant in each class. It is useful when \\(p\\) is large (unklike LDA and QDA).\nNaive Bayes assumes that each \\(\\Sigma_k\\) is diagonal, so\n\\[\\begin{align*}\n\\delta_k(x) &\\propto \\log \\bigg[\\pi_k \\prod_{j=1}^p f_{kj}(x_j) \\bigg] \\\\\n            &= -\\frac{1}{2} \\sum_{j=1}^p \\frac{(x_j-\\mu_{kj})^2}{\\sigma_{kj}^2} + \\log \\pi_k\n\\end{align*}\\]\nIt can used for mixed feature vectors (qualitative and quantitative). If \\(X_j\\) is qualitative, we replace \\(f_{kj}(x_j)\\) by probability mass function (histogram) over discrete categories."
  },
  {
    "objectID": "discriminantanalysis.html#lda-vs-logistic-regression",
    "href": "discriminantanalysis.html#lda-vs-logistic-regression",
    "title": "\n1¬† Discriminant Analysis\n",
    "section": "\n1.8 LDA vs Logistic Regression",
    "text": "1.8 LDA vs Logistic Regression\nthe logistic regression and LDA methods are closely connected. Consider the two-class setting with \\(p =1\\) predictor, and let \\(p_1(x)\\) and \\(p_2(x)=1‚àíp_1(x)\\) be the probabilities that the observation \\(X = x\\) belongs to class 1 and class 2, respectively. In the LDA framework, we can see from Equation (1.4) (and a bit of simple algebra) that the log odds is given by\n\\[ \\log \\bigg(\\frac{p_1(x)}{1-p_1(x)}\\bigg) = \\log \\bigg(\\frac{p_1(x)}{p_2(x)}\\bigg) = c_0 + c_1 x\\]\nwhere \\(c_0\\) and \\(c_1\\) are functions of \\(\\mu_1, \\mu_2,\\) and \\(\\sigma^2\\).\nOn the other hand, we know that in logistic regression\n\\[ \\log \\bigg(\\frac{p_1}{1-p_1}\\bigg) = \\beta_0 + \\beta_1 x\\]\nBoth of the equations above are linear functions of \\(x\\). Hence both logistic regression and LDA produce linear decision boundaries. The only diÔ¨Äerence between the two approaches lies in the fact that \\(\\beta_0\\) and \\(\\beta_1\\) are estimated using maximum likelihood, whereas \\(c_0\\) and \\(c_1\\) are computed using the estimated mean and variance from a normal distribution. This same connection between LDA and logistic regression also holds for multidimensional data with \\(p> 1\\).\n\n\n\n\n\n\nTip\n\n\n\n\nLogistic regression uses the conditional likelihood based on \\(P(Y|X)\\) (known as discriminative learning).\nLDA uses the full likelihood based on \\(P(X,Y )\\) (known as generative learning).\nDespite these differences, in practice the results are often very similar.\n\nRemark: Logistic regression can also fit quadratic boundaries like QDA, by explicitly including quadratic terms in the model.\n\n\n\n‚óº"
  },
  {
    "objectID": "Lab-DA.html",
    "href": "Lab-DA.html",
    "title": "Lab on Discriminant Analysis",
    "section": "",
    "text": "You are free to apply this lab in  or Python. The codes given in the lab and the main instructions are  codes. It is up to you to adapt them if you use Python.\nIf you use Python, verify that scikit-learn is installed and verify its version (it should at least 0.21).\nIn , we are going to use the lda() and qda() functions from MASS library.\nIn Python, you can use :\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nDuring this session we are going to analyse the Social_Network_Ads dataset üî¢. This dataset contains informations of users of a social network and if they bought a specified product. We are going to model the variable Purchased in function of Age and EstimatedSalary. We will fit using the models Logistic Regression, LDA, QDA, and Naive Bayes."
  },
  {
    "objectID": "Lab-DA.html#logistic-regression",
    "href": "Lab-DA.html#logistic-regression",
    "title": "Lab on Discriminant Analysis",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n1. First, let‚Äôs do the pre-processing steps and fit a logistic regression model. Please read and understand very well the following code (read the comments!). Then copy what is necessary for your report (but remove my comments!).\n\n\n\n\n\n\nTip\n\n\n\nYou can download the dataset from here üî¢.\n\n\n\n# Loading the dataset.. I have putted it into a folder called \"datasets\"\ndataset <- read.csv('http://www.mghassany.com/MLcourseEfrei/datasets/Social_Network_Ads.csv')\n\n# Describing and Exploring the dataset\n\nstr(dataset) # to show the structure of the dataset. \nsummary(dataset) # will show some statistics of every column. \n# Remark what it shows when the column is a numerical or categorical variable.\n# Remark that it has no sense for the variable User.ID\n\nboxplot(Age ~ Purchased, data=dataset, col = \"blue\", main=\"Boxplot Age ~ Purchased\")\n# You know what is a boxplot right? I will let you interpret it.\nboxplot(EstimatedSalary ~ Purchased, data=dataset,col = \"red\",\n main=\"Boxplot EstimatedSalary ~ Purchased\")\n# Another boxplot\n\naov(EstimatedSalary ~Purchased, data=dataset)\n# Anova test, but we need to show the summary of \n# it in order to see the p-value and to interpret.\n\nsummary(aov(EstimatedSalary ~Purchased, data=dataset))\n# What do you conclude ?\n# Now another anova test for the variable Age\nsummary(aov(Age ~Purchased, data=dataset))\n\n# There is a categorical variable in the dataset, which is Gender.\n# Of course we cannot show a boxplot of Gender and Purchased.\n# But we can show a table, or a mosaic plot, both tell the same thing.\ntable(dataset$Gender,dataset$Purchased)\n# Remark for the function table(), that\n# in lines we have the first argument, and in columns we have the second argument.\n# Don't forget this when you use table() to show a confusion matrix!\nmosaicplot(~ Purchased + Gender, data=dataset,\n  main = \"MosaicPlot of two categorical variables: Puchased & Gender\",\n  color = 2:3, las = 1)\n\n# since these 2 variables are categorical, we can apply\n# a Chi-square test. The null hypothesis is the independance between\n# these variables. You will notice that p-value = 0.4562 which is higher than 0.05 (5%)\n# so we cannot reject the null hypothesis. \n# conclusion: there is no dependance between Gender and Purchased (who\n# said that women buy more than men? hah!)\n\nchisq.test(dataset$Purchased, dataset$Gender)\n\n# Let's say we want to remove the first two columns as we are not going to use them.\n# But, we can in fact use a categorical variable as a predictor in logistic regression.\n# It will treat it the same way as in regression. Check Appendix C.\n# Try it by yourself if you would like to.\ndataset = dataset[3:5]\nstr(dataset) # show the new structure of dataset\n\n\n# splitting the dataset into training and testing sets\nlibrary(caTools)\nset.seed(123) # CHANGE THE VALUE OF SEED. PUT YOUR STUDENT'S NUMBER INSTEAD OF 123.\nsplit = sample.split(dataset$Purchased, SplitRatio = 0.75)\ntraining_set = subset(dataset, split == TRUE)\ntest_set = subset(dataset, split == FALSE)\n\n# scaling\n# So here, we have two continuous predictors, Age and EstimatedSalary.\n# There is a very big difference in their scales (units).\n# That's why we scale them. But it is not always necessary.\n\ntraining_set[-3] <- scale(training_set[-3]) #only first two columns\ntest_set[-3] <- scale(test_set[-3])\n\n# Note that, we replace the columns of Age and EstimatedSalary in the training and\n# test sets but their scaled versions. I noticed in a lot of reports that you scaled\n# but you did not do the replacing.\n# Note too that if you do it column by column you will have a problem because \n# it will replace the column by a matrix, you need to retransform it to a vector then.\n# Last note, to call the columns Age and EstimatedSalary we can it like I did or \n# training_set[c(1,2)] or training_set[,c(1,2)] or training_set[,c(\"Age\",\"EstimatedSalary\")]\n\n\n# logistic regression\n\nclassifier.logreg <- glm(Purchased ~ Age + EstimatedSalary , family = binomial, data=training_set)\nclassifier.logreg\nsummary(classifier.logreg)\n\n# prediction\npred.glm = predict(classifier.logreg, newdata = test_set[,-3], type=\"response\")\n# Do not forget to put type response. \n# By the way, you know what you get when you do not put it, right?\n\n# Now let's assign observations to classes with respect to the probabilities\npred.glm_0_1 = ifelse(pred.glm >= 0.5, 1,0)\n# I created a new vector, because we need the probabilities later for the ROC curve.\n\n# show some values of the vectors\nhead(pred.glm)\nhead(pred.glm_0_1)\n\n# confusion matrix\ncm = table(test_set[,3], pred.glm_0_1)\ncm\n# First line to store it into cm, second line to show the matrix! \n\n# You remember my note about table() function and the order of the arguments?\ncm = table(pred.glm_0_1, test_set[,3])\ncm\n\n# You can show the confusion matrix in a mosaic plot by the way\nmosaicplot(cm,col=sample(1:8,2)) # colors are random between 8 colors.\n\n# ROC\nrequire(ROCR)\nscore <- prediction(pred.glm,test_set[,3]) # we use the predicted probabilities not the 0 or 1\nperformance(score,\"auc\") # y.values\nplot(performance(score,\"tpr\",\"fpr\"),col=\"green\")\nabline(0,1,lty=8)\n\nSo now we have a logistic regression model stored in classifier.logreg. It is a model of Purchased in function of Age and EstimatedSalary."
  },
  {
    "objectID": "Lab-DA.html#linear-discriminant-analysis-lda",
    "href": "Lab-DA.html#linear-discriminant-analysis-lda",
    "title": "Lab on Discriminant Analysis",
    "section": "Linear Discriminant Analysis (LDA)",
    "text": "Linear Discriminant Analysis (LDA)\nLet us apply linear discriminant analysis (LDA) now. First we will make use of the lda() function in the package MASS. Second, you are going to create the model and predict the classes by yourself without using the lda() function. And we will visualize the decision boundary of LDA.\n2. Fit a LDA model of Purchased in function of Age and EstimatedSalary. Name the model classifier.lda.\n\nlibrary(MASS)\nclassifier.lda <- lda(Purchased~Age+EstimatedSalary, data=training_set)\n\n3. Call classifier.lda and understand what does it compute.\n\n\n\nPlus: If you enter the following you will be returned with a list of summary information concerning the computation:\n\nclassifier.lda$prior\nclassifier.lda$means\n\n4. On the test set, predict the probability of purchasing the product by the users using the model classifier.lda. Remark that when we predict using LDA, we obtain a list instead of a matrix, do str() for your predictions to see what do you get.\nRemark: we get the predicted class here, without being obligated to round the predictions as we did for logistic regression.\n\n\n\n5. Compute the confusion matrix and compare the predictions results obtained by LDA to the ones obtained by logistic regression. What do you remark? (Hint: compare the accuracy)\n\n\n\n6. Now let us plot the decision boundary obtained with LDA. You saw in the course that decision boundary for LDA represent the set of values \\(x\\) where \\(\\delta_k(x) = \\delta_c(x)\\). Recall that\n\\[\\delta_k(X) = x^T \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\Sigma^{-1}  \\mu_k + \\log \\pi_k\\]\nHere in our case, we have 2 classes (\\(K=2\\)) and 2 predictors (\\(p=2\\)). So the decision boundary (which is linear in the case of LDA, and line in our case since \\(p=2\\)) will verify the equation \\(\\delta_0(x) = \\delta_1(x)\\) Since we have two classes ‚Äú0‚Äù and ‚Äú1‚Äù. In the case of LDA this leads to linear boundary and is easy to be plotted. But in more complicated cases it is difficult to manually simplify the equations and plot the decision boundary. Anyway, there is a smart method to plot (but a little bit costy) the decision boundary in R using the function contour(), the corresponding code is the following (you must adapt it and use it to plot your decision boundary):\n\n# create a grid corresponding to the scales of Age and EstimatedSalary\n# and fill this grid with lot of points\nX1 = seq(min(training_set[, 1]) - 1, max(training_set[, 1]) + 1, by = 0.01)\nX2 = seq(min(training_set[, 2]) - 1, max(training_set[, 2]) + 1, by = 0.01)\ngrid_set = expand.grid(X1, X2)\n# Adapt the variable names\ncolnames(grid_set) = c('Age', 'EstimatedSalary')\n\n# plot 'Estimated Salary' ~ 'Age'\nplot(test_set[, 1:2],\n     main = 'Decision Boundary LDA',\n     xlab = 'Age', ylab = 'Estimated Salary',\n     xlim = range(X1), ylim = range(X2))\n\n# color the plotted points with their real label (class)\npoints(test_set[1:2], pch = 21, bg = ifelse(test_set[, 3] == 1, 'green4', 'red3'))\n\n# Make predictions on the points of the grid, this will take some time\npred_grid = predict(classifier.lda, newdata = grid_set)$class\n\n# Separate the predictions by a contour\ncontour(X1, X2, matrix(as.numeric(pred_grid), length(X1), length(X2)), add = TRUE)"
  },
  {
    "objectID": "Lab-DA.html#quadratic-discriminant-analysis-qda",
    "href": "Lab-DA.html#quadratic-discriminant-analysis-qda",
    "title": "Lab on Discriminant Analysis",
    "section": "Quadratic Discriminant Analysis (QDA)",
    "text": "Quadratic Discriminant Analysis (QDA)\nTraining and assessing a QDA model in R is very similar in syntax to training and assessing a LDA model. The only difference is in the function name qda()\n7. Fit a QDA model of Purchased in function of Age and EstimatedSalary. Name the model classifier.qda.\n\n# qda() is a function of library(MASS)\nclassifier.qda <- qda(Purchased~., data = training_set)\n\n8. Make predictions on the test_set using the QDA model classifier.qda. Show the confusion matrix and compare the results with the predictions obtained using the LDA model classifier.lda.\n\n\n\n9. Plot the decision boundary obtained with QDA. Color the points with the real labels."
  },
  {
    "objectID": "Lab-DA.html#comparison",
    "href": "Lab-DA.html#comparison",
    "title": "Lab on Discriminant Analysis",
    "section": "Comparison",
    "text": "Comparison\n10. In order to compare the methods we used, plot on the same Figure the ROC curve for each classifier we fitted and compare the correspondant AUC. What was the best model for this dataset? Can you justify it?\nRemark: If you use the ROCR package:\n\nFor Logistic regression, use the predicted probabilities in the prediction() (and not the round values ‚Äú0‚Äù or ‚Äú1‚Äù).\nFor LDA and QDA, put pred.lda$posterior[,2] in the prediction() function (those are the posterior probabilities that observations belong to class ‚Äú1‚Äù)."
  },
  {
    "objectID": "Lab-DA.html#lda-from-scratch",
    "href": "Lab-DA.html#lda-from-scratch",
    "title": "Lab on Discriminant Analysis",
    "section": "LDA from scratch",
    "text": "LDA from scratch\n11. Now let us build a LDA model for our data set without using the lda() function. You are free to do it by creating a function or without creating one. Go back to question 6 and see what did you obtain by using lda(). It computes the prior probability of group membership and the estimated group means for each of the two groups. Additional information that is not provided, but may be important, is the single covariance matrix that is being used for the various groupings.\n\n\n\n\n\n\nTip\n\n\n\nIn LDA, we compute for every observation \\(x\\) its discriminant score \\(\\delta_k(x)\\). Then we attribute \\(x\\) to the class that has the highest \\(\\delta\\). Recall that\n\\[\\delta_k(x) = x^T \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\Sigma^{-1}  \\mu_k + \\log \\pi_k\\]\nSo to compute \\(\\delta_k(x)\\) we need to estimate \\(\\pi_k\\), \\(\\mu_k\\) and \\(\\Sigma\\).\nNote that \\[x=\\begin{pmatrix}\n            X_1 \\\\\n            X_2\n            \\end{pmatrix}\\] and here \\(X_1\\)=Age and \\(X_2\\)=EstimatedSalary.\n\n\nSo let us do it step by step, first we will do the estimates:\n11.1 Subset the training set into two sets: class0 where Purchased = 0 and class1 where Purchased = 1).\n11.2 Compute \\(\\pi_0\\) and \\(\\pi_1\\).\n\\[\\pi_i = N_i / N, \\,\\, \\text{where} \\,\\, N_i \\,\\, \\text{is the number of data points in group i}\\]\n11.3 Compute \\(\\mu_0\\) and \\(\\mu_1\\).\n\\[\\mu_0 = \\begin{pmatrix}\n   \\mu_0(X_1) \\\\\n   \\mu_0(X_2)\n   \\end{pmatrix} \\,\\, \\text{and} \\,\\, \\mu_1 = \\begin{pmatrix}\n   \\mu_1(X_1) \\\\\n   \\mu_1(X_2)\n   \\end{pmatrix}\\]\nwhere, for example, \\(\\mu_0(X_1)\\) is the mean of the variable \\(X_1\\) in the group \\(0\\) (the subset class0).\n11.4 Compute \\(\\Sigma\\). In the case of two classes like here, it is computed by calculating the following:\n\\[\\Sigma = \\frac{(N_0-1)\\Sigma_0 + (N_1-1)\\Sigma_1}{N_0+N_1-2}\\]\nwhere \\(\\Sigma_i\\) is the estimated covariance matrix for specific group \\(i\\).\nRemark: Recall that in LDA we use the same \\(\\Sigma\\). But in QDA we do not.\n11.5. Now that we have computed all the needed estimates, we can calculate \\(\\delta_0(x)\\) and \\(\\delta_1(x)\\) for any observation \\(x\\). And we will attribute \\(x\\) to the class with the highest \\(\\delta\\). First, try it for \\(x\\) where \\(x^T=(1,1.5)\\), what is class prediction for this spesific \\(x\\)?\n11.6. Compute the discriminant scores \\(\\delta\\) for the test set (a matrix \\(100\\times 2\\)), predict the classes and compare your results with the results obtained with the lda() function.\n\n\n\n\n‚óº"
  },
  {
    "objectID": "dimreduction.html",
    "href": "dimreduction.html",
    "title": "\n2¬† Dimensionality Reduction\n",
    "section": "",
    "text": "Slides\n\n\n\nSlides for this chapter: üìÑ\n\n\n\n\n\n‚óº"
  },
  {
    "objectID": "Lab-Dim-Red.html",
    "href": "Lab-Dim-Red.html",
    "title": "Lab on Dimensionality Reduction",
    "section": "",
    "text": "You are free to apply this lab in  or Python.\nIn Python, use sklearn.decomposition.PCA for PCA and sklearn.manifold.TSNE for t-SNE.\nIn , you are free to use princomp(), prcomp() or factominer::PCA()."
  },
  {
    "objectID": "Lab-Dim-Red.html#the-dataset",
    "href": "Lab-Dim-Red.html#the-dataset",
    "title": "Lab on Dimensionality Reduction",
    "section": "The dataset",
    "text": "The dataset\nEmployement in European countries in the late 70s\nThe purpose of this case study is to reveal the structure of the job market and economy in different developed countries. The final aim is to have a meaningful and rigorous plot that is able to show the most important features of the countries in a concise form.\nThe eurojob dataset üî¢ contains the data employed in this case study. It contains the percentage of workforce employed in 1979 in 9 industries for 26 European countries. The industries measured are:\n\nAgriculture (Agr)\nMining (Min)\nManufacturing (Man)\nPower supply industries (Pow)\nConstruction (Con)\nService industries (Ser)\nFinance (Fin)\nSocial and personal services (Soc)\nTransport and communications (Tra)"
  },
  {
    "objectID": "Lab-Dim-Red.html#pca",
    "href": "Lab-Dim-Red.html#pca",
    "title": "Lab on Dimensionality Reduction",
    "section": "PCA",
    "text": "PCA\n1. Import the eurojob dataset üî¢ .\nIf the dataset is imported correctly, then it should look like this:\n\n\n\nThe eurojob dataset.\n\nCountry\nAgr\nMin\nMan\nPow\nCon\nSer\nFin\nSoc\nTra\n\n\n\nBelgium\n3.3\n0.9\n27.6\n0.9\n8.2\n19.1\n6.2\n26.6\n7.2\n\n\nDenmark\n9.2\n0.1\n21.8\n0.6\n8.3\n14.6\n6.5\n32.2\n7.1\n\n\nFrance\n10.8\n0.8\n27.5\n0.9\n8.9\n16.8\n6.0\n22.6\n5.7\n\n\nWGerm\n6.7\n1.3\n35.8\n0.9\n7.3\n14.4\n5.0\n22.3\n6.1\n\n\nIreland\n23.2\n1.0\n20.7\n1.3\n7.5\n16.8\n2.8\n20.8\n6.1\n\n\nItaly\n15.9\n0.6\n27.6\n0.5\n10.0\n18.1\n1.6\n20.1\n5.7\n\n\nLuxem\n7.7\n3.1\n30.8\n0.8\n9.2\n18.5\n4.6\n19.2\n6.2\n\n\nNether\n6.3\n0.1\n22.5\n1.0\n9.9\n18.0\n6.8\n28.5\n6.8\n\n\nUK\n2.7\n1.4\n30.2\n1.4\n6.9\n16.9\n5.7\n28.3\n6.4\n\n\nAustria\n12.7\n1.1\n30.2\n1.4\n9.0\n16.8\n4.9\n16.8\n7.0\n\n\nFinland\n13.0\n0.4\n25.9\n1.3\n7.4\n14.7\n5.5\n24.3\n7.6\n\n\nGreece\n41.4\n0.6\n17.6\n0.6\n8.1\n11.5\n2.4\n11.0\n6.7\n\n\nNorway\n9.0\n0.5\n22.4\n0.8\n8.6\n16.9\n4.7\n27.6\n9.4\n\n\nPortugal\n27.8\n0.3\n24.5\n0.6\n8.4\n13.3\n2.7\n16.7\n5.7\n\n\nSpain\n22.9\n0.8\n28.5\n0.7\n11.5\n9.7\n8.5\n11.8\n5.5\n\n\nSweden\n6.1\n0.4\n25.9\n0.8\n7.2\n14.4\n6.0\n32.4\n6.8\n\n\nSwitz\n7.7\n0.2\n37.8\n0.8\n9.5\n17.5\n5.3\n15.4\n5.7\n\n\nTurkey\n66.8\n0.7\n7.9\n0.1\n2.8\n5.2\n1.1\n11.9\n3.2\n\n\nBulgaria\n23.6\n1.9\n32.3\n0.6\n7.9\n8.0\n0.7\n18.2\n6.7\n\n\nCzech\n16.5\n2.9\n35.5\n1.2\n8.7\n9.2\n0.9\n17.9\n7.0\n\n\nEGerm\n4.2\n2.9\n41.2\n1.3\n7.6\n11.2\n1.2\n22.1\n8.4\n\n\nHungary\n21.7\n3.1\n29.6\n1.9\n8.2\n9.4\n0.9\n17.2\n8.0\n\n\nPoland\n31.1\n2.5\n25.7\n0.9\n8.4\n7.5\n0.9\n16.1\n6.9\n\n\nRomania\n34.7\n2.1\n30.1\n0.6\n8.7\n5.9\n1.3\n11.7\n5.0\n\n\nUSSR\n23.7\n1.4\n25.8\n0.6\n9.2\n6.1\n0.5\n23.6\n9.3\n\n\nYugoslavia\n48.7\n1.5\n16.8\n1.1\n4.9\n6.4\n11.3\n5.3\n4.0\n\n\n\n\n\n2. Describe the dataset and make some hypotheses. You can for example:\n\nCalculate the measurements of each variable\nCalculate and visualize the correlation matrix\nShow the scatterplot matrix\netc..\n\n3. Apply PCA to the dataset. Show the variation explained by each of the principal components and the cumulative variation. Comment.\n\n\n\n\n\n\nImportant\n\n\n\nDon‚Äôt forget to standardize the dataset, or to use the eigendecomposition of the correlation matrix instead of the variance-covariance matrix (no need to standardize in this case).\n\n\n4. In the following plot, you see a scatterplot matrix of the principal components. What does the green lines correspond to? what do you notice?\n\n\n\n\n\n\nThe PCs are uncorrelated, but not independent (uncorrelated does not imply independent).\n\n5. Plot the following:\n\nThe scree plot.\nThe graph of individuals.\nThe graph of variables.\nThe biplot graph.\nThe contributions of the variables to the first 2 principal components.\n\nInterpret the results (at least 3 interpretations).\nPCA from scratch\n6. Implement PCA on the eurojob dataset:\n\nStandardize the data.\nObtain the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix.\n\nExtra: Verify that the variance-covariance matrix of the standardized data is equal to the correlation matrix for the unstandardized data, and that both yield the same igenvectors and eigenvalue pairs\nSort eigenvalues in descending order and choose the \\(k\\) eigenvectors that correspond to the \\(k\\) largest eigenvalues, where \\(k\\) is the number of dimensions of the new feature subspace (\\(k \\le p\\)).\nConstruct the projection matrix \\(\\mathbf{A}\\) from the selected \\(k\\) eigenvectors.\nTransform the original dataset \\(X\\) via \\(\\mathbf{A}\\) to obtain a \\(k\\)-dimensional feature subspace \\(\\mathbf{Y}\\).\nVisualize the graph of individuals. Compare with the graph obtained in question 5.\n\n\nEigendecomposition - Computing Eigenvectors and Eigenvalues\nThe eigenvectors and eigenvalues of a covariance (or correlation) matrix represent the ‚Äúcore‚Äù of a PCA: The eigenvectors (principal components) determine the directions of the new feature space, and the eigenvalues determine their magnitude. In other words, the eigenvalues explain the variance of the data along the new feature axes."
  },
  {
    "objectID": "Lab-Dim-Red.html#t-sne",
    "href": "Lab-Dim-Red.html#t-sne",
    "title": "Lab on Dimensionality Reduction",
    "section": "t-SNE",
    "text": "t-SNE\nIn this part, we are going to use a sample from the digits dataset. You can download the sample from here \nThe MNIST dataset contains tens of thousands of handwritten digits ranging from zero to nine. Each image is of size 28√ó28 pixels.\nThe following image displays a couple of handwritten digits from the dataset:\n\n\n\n\n\n\n\n\nIt is required to flatten the images from 28√ó28 to 1√ó784 (which is already done in the given csv).\n\nLoad the dataset and describe it.\nShow some numbers like in the image above.\nApply PCA and t-SNE on the dataset and visualize in 2D plot the observations. Label the points by coloring them or showing the corresponding letter. Compare the results.\nWhat is the effect of the perplexity parameter when using t-SNE?\n\n\nIf you use R, use the Rtsne package.\n\n\n\n\n\n‚óº"
  },
  {
    "objectID": "kmeans.html",
    "href": "kmeans.html",
    "title": "\n3¬† Kmeans & Hierarchical Clustering\n",
    "section": "",
    "text": "Previously we considered supervised learning methods such as regression and classification, where we typically have access to a set of \\(p\\) features \\(X_1,X_2,\\ldots,X_p\\), measured on \\(n\\) observations, and a response \\(Y\\) also measured on those same \\(n\\) observations (what we call labels). The goal was then to predict \\(Y\\) using \\(X_1,X_2,\\ldots,X_p\\). From now on we will instead focus on unsupervised learning, a set of statistical tools where we have only a set of features \\(X_1,X_2,\\ldots,X_p\\) measured on \\(n\\) observations. We are not interested in prediction, because we do not have an associated response variable \\(Y\\). Rather, the goal is to discover interesting things about the measurements on \\(X_1,X_2,\\ldots,X_p\\). Is there an informative way to visualize the data? Can we discover subgroups among the variables or among the observations? Unsupervised learning refers to a diverse set of techniques for answering questions such as these. In this chapter, we will focus on a particular type of unsupervised learning: Principal Components Analysis (PCA), a tool used for data visualization or data pre-processing before supervised techniques are applied. In the next chapters, we will talk about clustering, another particular type of unsupervised learning. Clustering is a broad class of methods for discovering unknown subgroups in data.\nUnsupervised learning is often much more challenging than supervised learning. The exercise tends to be more subjective, and there is no simple goal for the analysis, such as prediction of a response. Unsupervised learning is often performed as part of an exploratory data analysis. It is hard to assess the results obtained from unsupervised learning methods. If we fit a predictive model using a supervised learning technique, then it is possible to check our work by seeing how well our model predicts the response \\(Y\\) on observations not used in fitting the model. But in unsupervised learning, there is no way to check our work because we don‚Äôt know the true answer: the problem is unsupervised."
  },
  {
    "objectID": "kmeans.html#clustering",
    "href": "kmeans.html#clustering",
    "title": "\n3¬† Kmeans & Hierarchical Clustering\n",
    "section": "\n3.2 Clustering",
    "text": "3.2 Clustering\nClustering (or Cluster analysis) is the collection of techniques designed to find subgroups or clusters in a dataset of variables \\(X_1,\\ldots,X_p\\). Depending on the similarities between the observations, these are partitioned in homogeneous groups as separated as possible between them. Clustering methods can be classified into these main categories:\n\n\nPartition methods: Given a fixed number of cluster \\(k\\), these methods aim to assign each observation of \\(X_1,\\ldots,X_p\\) to a unique cluster, in such a way that the within-cluster variation is as small as possible (the clusters are as homogeneous as possible) while the between cluster variation is as large as possible (the clusters are as separated as possible).\n\nDistribution models: These clustering models are based on the notion of how probable is it that all data points in the cluster belong to the same distribution (For example: Normal, Poisson, etc..). A popular example of these models is Expectation-maximization algorithm using multivariate Normal distributions.\n\nHierarchical methods: These methods construct a hierarchy for the observations in terms of their similitudes. This results in a tree-based representation of the data in terms of a dendrogram, which depicts how the observations are clustered at different levels ‚Äì from the smallest groups of one element to the largest representing the whole dataset.\n\nDensity Models: These models search the data space for areas of varied density of data points in the data space. It isolates various different density regions and assign the data points within these regions in the same cluster. Popular examples of density models are DBSCAN and OPTICS.\n\n\n\n\n\nPerformance comparison of different clustering methods on different datasets\n\n\n\n\n\n\nIn this chapter we will see the basics of the partition methods, and one of the most well-known clustering techniques, namely \\(k\\)-means clustering."
  },
  {
    "objectID": "kmeans.html#introduction",
    "href": "kmeans.html#introduction",
    "title": "\n3¬† Kmeans & Hierarchical Clustering\n",
    "section": "\n3.3 Introduction",
    "text": "3.3 Introduction\nClustering (or Cluster analysis) is the process of partitioning a set of data objects (observations) into subsets. Each subset is a cluster, such that objects in a cluster are similar to one another, yet dissimilar to objects in other clusters.\nThe set of clusters resulting from a cluster analysis can be referred to as a clustering. In this context, different clustering methods may generate different clusterings on the same data set. The partitioning is not performed by humans, but by the clustering algorithm. Hence, clustering is useful in that it can lead to the discovery of previously unknown groups within the data.\n\n\n\n\n\n\nTip\n\n\n\nDifferent clustering methods may generate different clusterings on the same data set.\n\n\nExample: Imagine a Director of Customer Relationships at an Electronics magazine, and he has five managers working for him. He would like to organize all the company‚Äôs customers into five groups so that each group can be assigned to a different manager. Strategically, he would like that the customers in each group are as similar as possible. Moreover, two given customers having very different business patterns should not be placed in the same group. His intention behind this business strategy is to develop customer relationship campaigns that specifically target each group, based on common features shared by the customers per group. Unlike in classification, the class label of each customer is unknown. He needs to discover these groupings. Given a large number of customers and many attributes describing customer profiles, it can be very costly or even infeasible to have a human study the data and manually come up with a way to partition the customers into strategic groups. He needs a clustering tool to help.\nClustering has been widely used in many applications such as business intelligence, image pattern recognition, Web search, biology, and security. In business intelligence, clustering can be used to organize a large number of customers into groups, where customers within a group share strong similar characteristics. In image recognition, clustering can be used to discover clusters or ‚Äúsubclasses‚Äù in handwritten character recognition systems, for example. Clustering has also found many applications in Web search. For example, a keyword search may often return a very large number of hits (i.e., pages relevant to the search) due to the extremely large number of web pages. Clustering can be used to organize the search results into groups and present the results in a concise and easily accessible way. Moreover, clustering techniques have been developed to cluster documents into topics (remember the google news example?), which are commonly used in information retrieval practice.\nClustering is also called data segmentation in some applications because clustering partitions large data sets into groups according to their similarity.\nAs a branch of statistics, clustering has been extensively studied, with the main focus on distance-based cluster analysis. Clustering tools were proposed like \\(k\\)-means, fuzzy \\(c\\)-means, and several other methods.\nMany clustering algorithms have been introduced in the literature. Since clusters can formally be seen as subsets of the data set, one possible classification of clustering methods can be according to whether the subsets are fuzzy or crisp (hard).\nHard clustering\nHard clustering methods are based on classical set theory, and require that an object either does or does not belong to a cluster. Hard clustering means partitioning the data into a specified number of mutually exclusive subsets. The most common hard clustering method is \\(k\\)-means.\nFuzzy clustering\nFuzzy clustering methods, however, allow the objects to belong to several clusters simultaneously, with different degrees of membership. In many situations, fuzzy clustering is more natural than hard clustering. The most known technique of fuzzy clustering is the fuzzy \\(c\\)-means."
  },
  {
    "objectID": "kmeans.html#k-means",
    "href": "kmeans.html#k-means",
    "title": "\n3¬† Kmeans & Hierarchical Clustering\n",
    "section": "\n3.4 \\(k\\)-Means",
    "text": "3.4 \\(k\\)-Means\nIf you have ever watched a group of tourists with a couple of tour guides who hold umbrellas up so that everybody can see them and follow them, then you have seen a dynamic version of the \\(k\\)-means algorithm. \\(k\\)-means is even simpler, because the data (playing the part of the tourists) does not move, only the tour guides move.\nSuppose that we want to divide our input data into \\(K\\) categories, where we know the value of \\(K\\). We allocate \\(K\\) cluster centres (also called prototypes or centroids) to our input space, and we would like to position these centres so that there is one cluster centre in the middle of each cluster. However, we don‚Äôt know where the clusters are, let alone where their ‚Äòmiddle‚Äô is, so we need an algorithm that will find them. Learning algorithms generally try to minimize some sort of error, so we need to think of an error criterion that describes this aim. There are two things that we need to define:\nA distance measure: In order to talk about distances between points, we need some way to measure distances. It is often the normal Euclidean distance, but there are other alternatives like Manhattan distance, Correlation distance, Chessboard distance and other.\nThe Euclidean distance: Let \\(x=(x_1,x_2)\\) and \\(y=(y_1,y_2)\\) two observations in a two-dimensional space. The Euclidean distance \\(d_{x,y}\\) between \\(x\\) and \\(y\\) is\n\\[\\begin{align*}  \nd_{x,y}^2 &= (x_1-y_1)^2+(x_2 - y_2)^2  \\\\\nd_{x,y} &= \\sqrt{(x_1-y_1)^2+(x_2 - y_2)^2}\n\\end{align*}\\]\nThe mean average: Once we have a distance measure, we can compute the central point of a set of data points, which is the mean average. Actually, this is only true in Euclidean space, which is the one we are used to, where everything is nice and flat.\nWe can now think about a suitable way of positioning the cluster centres: we compute the mean point of each cluster, \\(\\textbf{v}_k\\), \\(i=1,\\ldots,K\\), and put the cluster centre there. This is equivalent to minimizing the Euclidean distance (which is the sum-of-squares error) from each data point to its cluster centre. Then we decide which points belong to which clusters by associating each point with the cluster centre that it is closest to. This changes as the algorithm iterates. We start by positioning the cluster centres randomly though the input space, since we don‚Äôt know where to put them, and we update their positions according to the data. We decide which cluster each data point belongs to by computing the distance between each data point and all of the cluster centres, and assigning it to the cluster that is the closest. For all the point that are assigned to a cluster, we then compute the mean of them, and move the cluster centre to that place. We iterate the algorithm until the cluster centres stop moving.\nIt is convenient at this point to define some notation to describe the assignment of data points to clusters. For each data point \\(x_i\\), we introduce a corresponding set of binary indicator variables \\(u_{ki} \\in {0,1}\\), where \\(k=1,\\ldots,K\\) describing which of the \\(K\\) clusters the data point \\(x_i\\) is assigned to, so that if data point \\(x_i\\) is assigned to cluster \\(k\\) then \\(u_{ki}= 1\\), and \\(u_{ji}= 0\\) for \\(j \\neq k\\). This is known as the \\(1\\)-of-\\(K\\) coding scheme. We can then define an objective function (and sometimes called a distortion measure), given by\n\\[J= \\sum_{i=1}^{N} \\sum_{k=1}^{K} u_{ki} \\| x_{i}- \\mathbf{v}_{k} \\|^2\\]\nwhich represents the sum of the squares of the distances of each data point to its assigned vector \\(\\mathbf{v}_{k}\\). The goal is to find values for the \\(\\{u_{ki}\\}\\) and the \\(\\{\\mathbf{v}_{k}\\}\\) so as to minimize \\(J\\). We can do this through an iterative procedure in which each iteration involves two successive steps corresponding to successive optimizations with respect to the \\(u_{ki}\\) and the \\(\\mathbf{v}_{k}\\). The algorithm of \\(k\\)-means is described in the following algorithm:\n\n\n\n\n\n\nThe \\(k\\)-means algorithm\n\n\n\n\n\nData:\n\\(\\textbf{X}=\\{x_{ij}, i=1,\\ldots,n, j=1,\\ldots,p\\}\\)\n\n\n\nResult:\nCluster centres (Prototypes)\n\n\n\nInitialization:\n\nChoose a value for \\(K\\).\nChoose \\(K\\) random positions in the input space.\nAssign the prototypes \\(\\mathbf{v}_{k}\\) to those positions\n\n\n\n\nLearning: repeat\n\n\nfor each data point \\(x_i\\) do\n\ncompute the distance to each prototype: \\[d_{ki}= \\text{min}_k d(x_i,\\mathbf{v}_k)\\]\n\nassign the data point to the nearest prototype with distance \\[u_{ki}= \\left\\lbrace \\begin{array}{ll}  1   & \\mbox{if} \\quad k = argmin_j d(x_i,\\mathbf{v}_j) \\\\  0 & \\mbox{otherwise} \\end{array} \\right.\\]\n\n\nfor each prototype do\n\nmove the position of the prototype to the mean of the points in that cluster: \\[\\mathbf{v}_k = \\frac{\\sum_i u_{ki} x_i}{\\sum_i u_{ki}}\\]\n\n\nUntil the prototypes stop moving.\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe \\(k\\)-means algorithm produces\n\nA final estimate of cluster centroids (i.e.¬†their coordinates).\nAn assignment of each point to their respective cluster.\n\n\n\nThe denominator in the expression \\(\\mathbf{v}_k = \\frac{\\sum_i u_{ki} x_i}{\\sum_i u_{ki}}\\) is equal to the number of points assigned to cluster \\(k\\), and so this result has a simple interpretation, namely set \\(\\mathbf{v}_k\\) equal to the mean of all of the data points \\(x_i\\) assigned to cluster \\(k\\). For this reason, the procedure is known as the \\(k\\)-means algorithm.\nThe two phases of re-assigning data points to clusters and re-computing the cluster means are repeated in turn until there is no further change in the assignments (or until some maximum number of iterations is exceeded). Because each phase reduces the value of the objective function \\(J\\), convergence of the algorithm is assured. However, it may converge to a local rather than global minimum of \\(J\\).\nThe \\(k\\)-means algorithm is illustrated using the Old Faithful data set 1 in following figure.\n\n\n\n\nIllustration of the \\(k\\)-means algorithm using the re-scaled Old Faithful data set, where \\(k=2\\). We can see how the \\(k\\)-means algorithm works. (a) The first thing \\(k\\)-means has to do is assign an initial set of centroids. (b) The next stage in the algorithm assigns every point in the dataset to the closest centroid. (c) The next stage is to re-calculate the centroids based on the new cluster assignments of the data points. (d) Now we have completed one full cycle of the algorithm we can continue and re-assign points to their (new) closest cluster centroid. (e) And we can update the centroid positions one more time based on the re-assigned points. (g)(h)(f) The algorithm stops when we obtain the same results in consecutive iterations.\n\n\n\n\nThe \\(k\\)-means algorithm is illustrated using the Iris data set in following interactive figure2. (Try to modify the X and Y variables and the numbers of chosen clusters and see the result)\n\n\n\n\n\n\n\n3.4.1 \\(k\\)-means in \n\nWe will use an example with simulated data to demonstrate how the \\(k\\)-means algorithm works. Here we simulate some data from three clusters and plot the dataset below.\n\nset.seed(1234)\nx <- rnorm(12,mean=rep(1:3,each=4),sd=0.2)\ny <- rnorm(12,mean=rep(c(1,2,1),each=4),sd=0.2)\nplot(x,y,col=\"blue\",pch=19,cex=2)\ntext(x+0.05,y+0.05,labels=as.character(1:12))\n\n\n\nSimulated dataset\n\n\n\n\nThe kmeans() function in R implements the \\(k\\)-means algorithm and can be found in the stats package, which comes with R and is usually already loaded when you start R. Two key parameters that you have to specify are x, which is a matrix or data frame of data, and centers which is either an integer indicating the number of clusters or a matrix indicating the locations of the initial cluster centroids. The data should be organized so that each row is an observation and each column is a variable or feature of that observation.\n\ndataFrame <- data.frame(x,y)\nkmeansObj <- kmeans(dataFrame,centers=3)\nnames(kmeansObj)\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nYou can see which cluster each data point got assigned to by looking at the cluster element of the list returned by the kmeans() function.\n\nkmeansObj$cluster\n\n [1] 3 1 1 3 2 2 2 2 2 2 2 2\n\n\nHere is a plot of the \\(k\\)-means clustering solution.\n\nplot(x,y,col=kmeansObj$cluster,pch=19,cex=2)\npoints(kmeansObj$centers,col=1:3,pch=3,cex=3,lwd=3)\n\n\n\n\\(k\\)-means clustering solution\n\n\n\n\n\n3.4.2 Cluster Validity, Choosing the Number of Clusters\nThe result of a clustering algorithm can be very different from each other on the same data set as the other input parameters of an algorithm can extremely modify the behavior and execution of the algorithm. The aim of the cluster validity is to find the partitioning that best fits the underlying data. Usually 2D data sets are used for evaluating clustering algorithms as the reader easily can verify the result. But in case of high dimensional data the visualization and visual validation are not trivial tasks therefore some formal methods are needed.\nThe process of evaluating the results of a clustering algorithm is called cluster validity assessment. Two measurement criteria have been proposed for evaluating and selecting an optimal clustering scheme:\n\nCompactness: The member of each cluster should be as close to each other as possible. A common measure of compactness is the variance.\nSeparation: The clusters themselves should be widely separated. There are three common approaches measuring the distance between two different clusters: distance between the closest member of the clusters, distance between the most distant members and distance between the centres of the clusters.\n\nThere are three different techniques for evaluating the result of the clustering algorithms, and several Validity measures are proposed: Validity measures are scalar indices that assess the goodness of the obtained partition. Clustering algorithms generally aim at locating well separated and compact clusters. When the number of clusters is chosen equal to the number of groups that actually exist in the data, it can be expected that the clustering algorithm will identify them correctly. When this is not the case, misclassifications appear, and the clusters are not likely to be well separated and compact. Hence, most cluster validity measures are designed to quantify the separation and the compactness of the clusters.\n\n\n\n\n\n\nTip\n\n\n\nCheck this answer on stackoverflow containing  code for several methods of computing an optimal value of \\(k\\) for \\(k\\)-means cluster analysis: here."
  },
  {
    "objectID": "kmeans.html#hierarchical-clustering",
    "href": "kmeans.html#hierarchical-clustering",
    "title": "\n3¬† Kmeans & Hierarchical Clustering\n",
    "section": "\n3.5 Hierarchical Clustering",
    "text": "3.5 Hierarchical Clustering\nIn the previous part we introduced \\(k\\)-means. One potential disadvantage of it is that it requires us to pre-specify the number of clusters \\(k\\). Hierarchical clustering is an alternative approach which does not require that we commit to a particular choice of \\(k\\). Hierarchical clustering has an added advantage over \\(k\\)-means clustering in that it results in an attractive tree-based representation of the observations, called a dendrogram.\nThe most common type of hierarchical clustering is the agglomerative clustering (or bottom-up clustering). It refers to the fact that a dendrogram (generally depicted as an upside-down tree) is built starting from the leaves and combining clusters up to the trunk.\n\n3.5.1 Dendrogram\nSuppose that we have the simulated data in the following figure:\n\n\n\n\nFigure 3.1: Simulated data of 45 observations generated from a three-class model.\n\n\n\n\nThe data in the figure above consists of 45 observations in two-dimensional space. The data were generated from a three-class model; the true class labels for each observation are shown in distinct colors. However, suppose that the data were observed without the class labels, and that we wanted to perform hierarchical clustering of the data. Hierarchical clustering (with complete linkage, to be discussed later) yields the result shown in Figure¬†3.2. How can we interpret this dendrogram?\n\n\n\n\nFigure 3.2: Dendrogram\n\n\n\n\nIn the dendrogram of Figure¬†3.2, each leaf of the dendrogram represents one of the 45 observations. However, as we move up the tree, some leaves begin to fuse into branches. These correspond to observations that are similar to each other. As we move higher up the tree, branches themselves fuse, either with leaves or other branches. The earlier (lower in the tree) fusions occur, the more similar the groups of observations are to each other. On the other hand, observations that fuse later (near the top of the tree) can be quite different. In fact, this statement can be made precise: for any two observations, we can look for the point in the tree where branches containing those two observations are first fused. The height of this fusion, as measured on the vertical axis, indicates how different the two observations are. Thus, observations that fuse at the very bottom of the tree are quite similar to each other, whereas observations that fuse close to the top of the tree will tend to be quite different.\nAn example of interpreting a dendrogram is presented in Figure¬†3.3\n\n\n\n\nFigure 3.3: An illustration of how to properly interpret a dendrogram with nine observations in two-dimensional space. Left: a dendrogram generated using Euclidean distance and complete linkage. Observations 5 and 7 are quite similar to each other, as are observations 1 and 6. However, observation 9 is no more similar to observation 2 than it is to observations 8, 5, and 7, even though observations 9 and 2 are close together in terms of horizontal distance. This is because observations 2, 8, 5, and 7 all fuse with observation 9 at the same height, approximately 1.8. Right: the raw data used to generate the dendrogram can be used to confirm that indeed, observation 9 is no more similar to observation 2 than it is to observations 8, 5, and 7.\n\n\n\n\nNow that we understand how to interpret the dendrogram of Figure¬†3.2, we can move on to the issue of identifying clusters on the basis of a dendrogram. In order to do this, we make a horizontal cut across the dendrogram, as shown in the following Figure¬†3.4 where we cut the dendrogram at a height of nine results in two clusters.\n\n\n\n\nFigure 3.4: The dendrogram from the simulated dataset, cut at a height of nine (indicated by the dashed line). This cut results in two distinct clusters, shown in different colors.\n\n\n\n\nThe distinct sets of observations beneath the cut can be interpreted as clusters. In Figure¬†3.5, cutting the dendrogram at a height of five results in three clusters.\n\n\n\n\nFigure 3.5: The dendrogram from the simulated dataset, cut at a height of five (indicated by the dashed line). This cut results in three distinct clusters, shown in different colors.\n\n\n\n\nThe term hierarchical refers to the fact that clusters obtained by cutting the dendrogram at a given height are necessarily nested within the clusters obtained by cutting the dendrogram at any greater height.\n\n3.5.2 The Hierarchical Clustering Algorithm\nThe hierarchical clustering dendrogram is obtained via an extremely simple algorithm. We begin by defining some sort of dissimilarity measure between each pair of observations. Most often, Euclidean distance is used. The algorithm proceeds iteratively. Starting out at the bottom of the dendrogram, each of the n observations is treated as its own cluster. The two clusters that are most similar to each other are then fused so that there now are \\(n‚àí1\\) clusters. Next the two clusters that are most similar to each other are fused again, so that there now are \\(n ‚àí 2\\) clusters. The algorithm proceeds in this fashion until all of the observations belong to one single cluster, and the dendrogram is complete.\nFigure¬†3.6 depicts the first few steps of the algorithm.\n\n\n\n\nFigure 3.6: An illustration of the first few steps of the hierarchical clustering algorithm, with complete linkage and Euclidean distance. Top Left: initially, there are nine distinct clusters {1}, {2}, ‚Ä¶, {9}. Top Right: the two clusters that are closest together, {5} and {7}, are fused into a single cluster. Bottom Left: the two clusters that are closest together, {6} and {1},are fused into a single cluster. Bottom Right: the two clusters that are closest together using complete linkage, {8} and the cluster {5, 7}, are fused into a single cluster.\n\n\n\n\nTo summarize, the hierarchical clustering algorithm is given in the following Algorithm:\n\n\n\n\n\nHierarchical Clustering:\n\n\n\n1- Initialisation: Begin with \\(n\\) observations and a measure (such as Euclidean distance) of all the \\(C^2_n = n(n‚àí1)/2\\) pairwise dissimilarities. Treat each observation as its own cluster.\n\n\n\n2- For \\(i=n,n-1,\\ldots,2\\):\n\n\n(a) Examine all pairwise inter-cluster dissimilarities among the \\(i\\) clusters and identify the pair of clusters that are least dissimilar (that is, most similar). Fuse these two clusters. The dissimilarity between these two clusters indicates the height in the dendrogram at which the fusion should be placed.\n\n\n(b) Compute the new pairwise inter-cluster dissimilarities among the \\(i ‚àí 1\\) remaining clusters.\n\n\n\nThis algorithm seems simple enough, but one issue has not been addressed. Consider the bottom right panel in Figure¬†3.6. How did we determine that the cluster {5, 7} should be fused with the cluster {8}? We have a concept of the dissimilarity between pairs of observations, but how do we define the dissimilarity between two clusters if one or both of the clusters contains multiple observations? The concept of dissimilarity between a pair of observations needs to be extended to a pair of groups of observations. This extension is achieved by developing the notion of linkage, which defines the dissimilarity between two groups of observations. The four most common types of linkage: complete, average, single, and centroid are briefly are described like follows:\n\nComplete: Maximal intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the largest of these dissimilarities.\nSingle: Minimal intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the smallest of these dissimilarities. Single linkage can result in extended, trailing clusters in which single observations are fused one-at-a-time\nAverage: Mean intercluster dissimilarity. Compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B, and record the average of these dissimilarities.\nCentroid: Dissimilarity between the centroid for cluster A (a mean vector of length p) and the centroid for cluster B. Centroid linkage can result in undesirable inversions.\n\nAverage and complete linkage are generally preferred over single linkage, as they tend to yield more balanced dendrograms. Centroid linkage is often used in genomics.\nThe dissimilarities computed in Step 2(b) of the hierarchical clustering algorithm will depend on the type of linkage used, as well as on the choice of dissimilarity measure. Hence, the resulting dendrogram typically depends quite strongly on the type of linkage used, as is shown in Figure¬†3.7.\n\n\n\n\nFigure 3.7: Average, complete, and single linkage applied to an example data set. Average and complete linkage tend to yield more balanced clusters.\n\n\n\n\n\n3.5.3 Hierarchical clustering in \n\nLet‚Äôs illustrate how to perform hierarchical clustering on dataset  Ligue1 2017-2018 .\n\n# Load the dataset\nligue1 <- read.csv(\"datasets/ligue1_17_18.csv\", row.names=1,sep=\";\")\n\n# Work with standardized data\nligue1_scaled <- data.frame(scale(ligue1))\n\n# Compute dissimilary matrix - in this case Euclidean distance\nd <- dist(ligue1_scaled)\n\n# Hierarchical clustering with complete linkage\ntreeComp <- hclust(d, method = \"complete\")\nplot(treeComp)\n\n\n\n# With average linkage\ntreeAve <- hclust(d, method = \"average\")\nplot(treeAve)\n\n\n\n# With single linkage\ntreeSingle <- hclust(d, method = \"single\")\nplot(treeSingle) # Chaining\n\n\n\n# Set the number of clusters after inspecting visually\n# the dendrogram for \"long\" groups of hanging leaves\n# These are the cluster assignments\ncutree(treeComp, k = 2) \n\n     Paris-SG        Monaco          Lyon     Marseille        Rennes \n            1             1             1             1             2 \n     Bordeaux Saint-Etienne          Nice        Nantes   Montpellier \n            2             2             2             2             2 \n        Dijon      Guingamp        Amiens        Angers    Strasbourg \n            2             2             2             2             2 \n         Caen         Lille      Toulouse        Troyes          Metz \n            2             2             2             2             2 \n\ncutree(treeComp, k = 3) \n\n     Paris-SG        Monaco          Lyon     Marseille        Rennes \n            1             1             1             1             2 \n     Bordeaux Saint-Etienne          Nice        Nantes   Montpellier \n            2             2             2             2             2 \n        Dijon      Guingamp        Amiens        Angers    Strasbourg \n            3             3             3             3             3 \n         Caen         Lille      Toulouse        Troyes          Metz \n            3             3             3             3             3 \n\ncutree(treeComp, k = 4) \n\n     Paris-SG        Monaco          Lyon     Marseille        Rennes \n            1             2             2             2             3 \n     Bordeaux Saint-Etienne          Nice        Nantes   Montpellier \n            3             3             3             3             3 \n        Dijon      Guingamp        Amiens        Angers    Strasbourg \n            4             4             4             4             4 \n         Caen         Lille      Toulouse        Troyes          Metz \n            4             4             4             4             4 \n\n\n\n‚óº"
  },
  {
    "objectID": "Lab-kmeans.html",
    "href": "Lab-kmeans.html",
    "title": "Lab on \\(k\\)-means and Hierarchical clustering",
    "section": "",
    "text": "You are free to apply this lab in  or Python.\nIn Python, you can use sklearn.cluster module in which you can find the most famous clustering techniques. For this lab, we will need KMeans() and AgglomerativeClustering(). You can read the interesting documentation here.\n\nkmeans() and hclust() are implemented by default in  base."
  },
  {
    "objectID": "Lab-kmeans.html#markdown",
    "href": "Lab-kmeans.html#markdown",
    "title": "Lab on \\(k\\)-means and Hierarchical clustering",
    "section": "Markdown",
    "text": "Markdown\nIn this part we will learn how to create a report in Rstudio using Rmarkdown files. Then we will apply the \\(k\\)-means clustering algorithm using the standard function kmeans() in . While in Python, you can instead use Jupyter to create a notebook. You can modify the type of the cells to markdown and then execute them to get an html output.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMarkdown is a lightweight markup language with plain text formatting syntax designed so that it can be converted to HTML and many other formats (pdf, docx, etc..).\nClick here  to see an example of a markdown (.md) syntaxes and the result in HTML. The markdown syntaxes are on right and their HTML result is on left. You can modify the source text to see the result.\n\n\n\n\n\n\nTip\n\n\n\nThere is some markdown online editors you can use, like dillinger.io/. See the Markdown source file and the HTML preview. Play with the source text to see the result in the preview.\n\n\nR Markdown\nR Markdown is a variant of Markdown that has embedded  code chunks, to be used with the knitr package to make it easy to create reproducible web-based reports.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst, in Rstudio create a new R Markdown file. A default template will be opened. There is some  code in R chunks. Click on knit, save your file and see the produced output. The output is a html report containing the results of the  codes. If your file is named report.Rmd, your report is named report.html.\n\nMake sure to have the latest version of Rstudio. (Recent versions of Rstudio have also a visual editor, see here)\nIf you have problems creating a R Markdown file (problem in installing packages, etc..) close your Rstudio and reopen it with administrative tools and retry.\n\nYou can find all the informations about R Markdown on this site: rmarkdown.rstudio.com.\nYou may also find the following resources helpful:\n\nThe R Markdown Reference Guide\nThe R Markdown Cheatsheet\nThe YAML header\nIn Rstudio, start by creating a R Markdown file. When you create it a default template will be opened with the following first lines:\n---\ntitle: \"Untitled\"\noutput: html_document\n---\nThese lines are the YAML header in which you choose the settings of your report (title, author, date, appearance, etc..)\nFor example, you can use the following YAML header:\n---\ntitle: \"Machine Learning Lab\"\nsubtitle: \"Clustering\"\nauthor: LastName FirstName\ndate: \"`#r format(Sys.time())`\" # remove the # to show the date\noutput:\n  html_document:\n    toc: true\n    toc_depth: 2\n    theme: flatly # you can modify the theme, check the available themes on Rstudio's website\n---\n\n\n\n\n\n\nImportant\n\n\n\nVery Important Remark: Click on the settings button of Rstudio‚Äôs text editor and choose to Chunk Output in Console.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo compile your report, click on knit or use the keyboard shortcut ctrl + shift + k (cmd + shift + k on mac)."
  },
  {
    "objectID": "Lab-kmeans.html#k-means-clustering",
    "href": "Lab-kmeans.html#k-means-clustering",
    "title": "Lab on \\(k\\)-means and Hierarchical clustering",
    "section": "\n\\(k\\)-means clustering",
    "text": "\\(k\\)-means clustering\nIn this lab, we will use the dataset Ligue1 2017/2018.\n1. Download the dataset: Ligue1 2017/2018 üî¢ and import it. Be aware that the teams‚Äô names are in the first column. You can put the argument row.names to 1 in R.\n\n\n\n\n# You can import directly from my website (instead of downloading it..)\nligue1 <- read.csv(\"http://www.mghassany.com/MLcourseEfrei/datasets/ligue1_17_18.csv\", row.names=1, sep=\";\")\n\n2. Print the first two rows of the dataset and the total number of features in this dataset.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn R, you can create an awesome HTML table by using the function kable from the knitr library. For example, if you want to show the first 5 lines and 5 columns of your dataset, you can use knitr::kable(ligue1[1:5,1:5]). Give it a try and see the result on your html report!\n\n\npointsCards\n3. We will first consider a smaller dataset to easily understand the results of \\(k\\)-means. Create a new dataset in which you consider only Points and Yellow.cards from the original dataset. Name it pointsCards\n\n\n\n4. Apply \\(k\\)-means on pointsCards. Chose \\(k=2\\) clusters and put the number of iterations to 20. Store your results into km. (Remark: kmeans() uses a random initialization of the clusters, so the results may vary from one call to another. Use set.seed() to have reproducible outputs).\n\n\n\n5. Print and describe what is inside km.\n\n\n\n6. What are the coordinates of the centers of the clusters (called also prototypes or centroids) ?\n\n\n\n7. Plot the data (Yellow.cards vs Points). Color the points corresponding to their cluster.\n\n\n\n8. Add to the previous plot the clusters centroids and add the names of the observations.\n\n\n\n9. Re-run \\(k\\)-means on pointsCards using 3 and 4 clusters and store the results into km3 and km4 respectively. Visualize the results like in question 7 and 8.\n\n\n\n\n\n\nImportant\n\n\n\nHow many clusters \\(k\\) do we need in practice? There is not a single answer: the advice is to try several and compare. Inspecting the ‚Äòbetween_SS / total_SS‚Äô for a good trade-off between the number of clusters and the percentage of total variation explained usually gives a good starting point for deciding on \\(k\\) (criterion to select \\(k\\) similar to PCA).\nThere is several methods of computing an optimal value of \\(k\\) with  code on following stackoverflow answer: here .\n\n\n10. Visualize the ‚Äúwithin groups sum of squares‚Äù of the \\(k\\)-means clustering results (use the code in the link above).\n\n\n\n\n\n\n11. Modify the code of the previous question in order to visualize the ‚Äòbetween_SS / total_SS‚Äô. Interpret the results.\n\n\n\nLigue 1\nSo far, you have only taken the information of two variables for performing clustering. Now you will apply kmeans() on the original dataset ligue1. Using PCA, we can visualize the clustering performed with all the available variables in the dataset.\nBy default, kmeans() does not standardize the variables, which will affect the clustering result. As a consequence, the clustering of a dataset will be different if one variable is expressed in millions or in tenths. If you want to avoid this distortion, use scale to automatically center and standardize the dataset (the result will be a matrix, so you need to transform it to a data frame again).\n12. Scale the dataset and transform it to a data frame again. Store the scaled dataset into ligue1_scaled.\n\n\n\n13. Apply kmeans() on ligue1 and on ligue1_scaled using 3 clusters and 20 iterations. Store the results into km.ligue1 and km.ligue1.scaled respectively (do not forget to set a seed)\n\n\n\n14. How many observations there are in each cluster of km.ligue1 and km.ligue1.scaled ? (you can use table()). Do you obtain the same results when you perform kmeans() on the scaled and unscaled data?\n\n\n\nPCA\n15. Apply PCA on ligue1 dataset and store you results in pcaligue1. Do we need to apply PCA on the scaled dataset? Justify your answer.\n\n\n\n16. Plot the observations and the variables on the first two principal components (biplot). Interpret the results.\n\n\n\n17. Visualize the teams on the first two principal components and color them with respect to their cluster.\n\n# You can use the following code in R, based on `factoextra` library.\nfviz_cluster(km.ligue1, data = ligue1, # km.ligue1 is where you stored your kmeans results\n              palette = c(\"red\", \"blue\", \"green\"), # 3 colors since 3 clusters\n              ggtheme = theme_minimal(),\n              main = \"Clustering Plot\"\n)\n\n18. Recall that the figure of question 17 is a visualization with PC1 and PC2 of the clustering done with all the variables, not on PC1 and PC2. Now apply the kmeans() clustering taking only the first two PCs instead the variables of original dataset. Visualize the results and compare with the question 17.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nBy applying \\(k\\)-means only on the PCs we obtain different and less accurate result, but it is still an insightful way.\n\n\nImplementing k-means\nIn this part, you will perform \\(k\\)-means clustering manually, with \\(k=2\\), on a small example with \\(n=6\\) observations and \\(p=2\\) features. The observations are as follows.\n\n\n\nObservation\n\\(X_1\\)\n\\(X_2\\)\n\n\n\n1\n1\n4\n\n\n2\n1\n3\n\n\n3\n0\n4\n\n\n4\n5\n1\n\n\n5\n6\n2\n\n\n6\n4\n0\n\n\n\n\n19. Plot the observations.\n20. Randomly assign a cluster label to each observation. You can use the sample() command in  to do this. Report the cluster labels for each observation.\n21. Compute the centroid for each cluster.\n22. Create a function that calculates the Euclidean distance for two observations.\n23. Assign each observation to the centroid to which it is closest, in terms of Euclidean distance. Report the cluster labels for each observation.\n24. Repeat 21 and 23 until the answers obtained stop changing.\n25. In your plot from 19, color the observations according to the cluster labels obtained."
  },
  {
    "objectID": "Lab-kmeans.html#hierarchical-clustering",
    "href": "Lab-kmeans.html#hierarchical-clustering",
    "title": "Lab on \\(k\\)-means and Hierarchical clustering",
    "section": "Hierarchical clustering",
    "text": "Hierarchical clustering\nDistances dist()\n\nTo calculate the distance in  we use the dist() function. Here is a tutorial of how use it.\n\n# Generate a matrix M of values from 1 to 15 with 5 rows and 3 columns\nM <- matrix(1:15,5,3)\nM\n\n     [,1] [,2] [,3]\n[1,]    1    6   11\n[2,]    2    7   12\n[3,]    3    8   13\n[4,]    4    9   14\n[5,]    5   10   15\n\n\n\n# - Compute the distance between rows of M.\n# - The default distance is the euclidian distance.\n# - Since there are 3 columns, it is the euclidian\n#        distance between tri-dimensional points.\ndist(M)\n\n         1        2        3        4\n2 1.732051                           \n3 3.464102 1.732051                  \n4 5.196152 3.464102 1.732051         \n5 6.928203 5.196152 3.464102 1.732051\n\n\n\n# To compute the Manhattan distance \ndist(M, method= \"manhattan\")\n\n   1  2  3  4\n2  3         \n3  6  3      \n4  9  6  3   \n5 12  9  6  3\n\n\nDendrogram hclust()\n\n\n# First we construct the dendrogram \ndendro <- hclust(dist(M))\n\n# Then we plot it\nplot(dendro)\n\n\n\n\nHierarchical clustering on Iris dataset\n\n\n\n\n\n\n\n\n1. Download the iris dataset from here üî¢ and load it.\n\n\n\n2. Choose randomly 40 observations of the iris dataset and store the sample dataset into sampleiris.\n\n\n\n3. Calculate the euclidean distances between the flowers. Store the results in a matrix called D. (Remark: the last column of the dataset is the class labels of the flowers)\n\n\n\n4. Construct a dendrogram on the iris dataset using the method average. Store the result in dendro.avg.\n\n\n\n5. Plot the dendrogram.\n\n\n\n6. Plot again the dendrogram using the following command:\n\nplot(dendro.avg, hang=-1, label=sampleiris$class)\n\n7. To cut the dendrogram and obtain a clustering use the cutree. You can choose the number of clusters you wish to obtain, or you can cut by choosing the height from the dendrogram figure. Cut the dendrogram in order to obtain 3 clusters. Store the results into vector groups.avg.\n\n\n\n8. Visualize the cut tree using the function rect.hclust(). You can choose the colors of the rectangles too!\n\n\n\n9. Compare the obtained results obtained with Hierarchical clustering and the real class labels of the flowers (function table()). Interpret the results.\n\n\n\nBonus: You can cut the tree manually (on demand!). To do so, plot a dendrogram first then use the function identify(). On the figure, click on the clusters you wish to obtain. Then hit Escape to finish.\n10. Now apply the Hierarchical clustering on the iris dataset (the 150 observations). Choose 3 clusters and compare the results with the real class labels. Compare different methods of Hierarchical clustering (average, complete and single linkages).\n\n‚óº"
  },
  {
    "objectID": "em.html",
    "href": "em.html",
    "title": "\n4¬† Gaussian Mixture Models & EM\n",
    "section": "",
    "text": "In the previous chapter we saw the \\(k\\)-means algorithm which is considered as a hard clustering technique, such that each point is allocated to only one cluster. In \\(k\\)-means, a cluster is described only by its centroid. This is not too flexible, as we may have problems with clusters that are overlapping, or ones that are not of circular shape.\nIn this chapter, we will introduce a model-based clustering technique, which is Expectation Maximization (EM). We will apply it using Gaussian Mixture Models (GMM).\nWith EM Clustering, we can go a step further and describe each cluster by its centroid (mean), covariance (so that we can have elliptical clusters), and weight (the size of the cluster). The probability that a point belongs to a cluster is now given by a multivariate Gaussian probability distribution (multivariate - depending on multiple variables). That also means that we can calculate the probability of a point being under a Gaussian ‚Äòbell‚Äô, i.e.¬†the probability of a point belonging to a cluster. A comparison between the performances of \\(k\\)-means and EM clustering on an artificial dataset is shown in Figure¬†4.1.\nWe start this chapter by reminding what is a Gaussian distribution, then introduce the Mixture of Gaussians and finish by explaining the Expectation-Maximization algorithm."
  },
  {
    "objectID": "em.html#the-gaussian-distribution",
    "href": "em.html#the-gaussian-distribution",
    "title": "\n4¬† Gaussian Mixture Models & EM\n",
    "section": "\n4.1 The Gaussian distribution",
    "text": "4.1 The Gaussian distribution\nThe Gaussian, also known as the normal distribution, is a widely used model for the distribution of continuous variables. In the case of a single variable \\(x\\), the Gaussian distribution can be written in the form\n\\[\n\\mathcal{N}(x|m,\\sigma^2)=\\frac{1}{(2 \\pi \\sigma^2 )^{1/2}} \\exp \\left\\lbrace - \\frac{1}{2 \\sigma^2} (x-m)^2\\right\\rbrace\n\\qquad(4.1)\\]\nwhere \\(m\\) is the mean and \\(\\sigma^2\\) is the variance.\nFor a \\(D\\)-dimensional vector \\(X\\), the multivariate Gaussian distribution take the form\n\\[\n\\mathcal{N}(X|\\mu,\\Sigma)=\\frac{1}{(2 \\pi)^{D/2}} \\frac{1}{|\\Sigma|^{1/2}} \\exp \\left\\lbrace - \\frac{1}{2} (X-\\mu)^T \\Sigma^{-1} (X-\\mu) \\right\\rbrace\n\\qquad(4.2)\\]\nwhere \\(\\mu\\) is a \\(D\\)-dimensional mean vector, \\(\\Sigma\\) is a \\(D\\times D\\) covariance matrix, and \\(|\\Sigma|\\) denotes the determinant of \\(\\Sigma\\).\nThe Gaussian distribution arises in many different contexts and can be motivated from a variety of different perspectives. For example, when we consider the sum of multiple random variables. The central limit theorem (due to Laplace) tells us that, subject to certain mild conditions, the sum of a set of random variables, which is of course itself a random variable, has a distribution that becomes increasingly Gaussian as the number of terms in the sum increases."
  },
  {
    "objectID": "em.html#mixture-of-gaussians",
    "href": "em.html#mixture-of-gaussians",
    "title": "\n4¬† Gaussian Mixture Models & EM\n",
    "section": "\n4.2 Mixture of Gaussians",
    "text": "4.2 Mixture of Gaussians\nWhile the Gaussian distribution has some important analytical properties, it suffers from significant limitations when it comes to modeling real data sets. Consider the example shown in Figure¬†4.2 applied on the ‚ÄôOld Faithful‚Äô data set, this data set comprises 272 measurements of the eruption of the Old Faithful geyser at Yellowstone National Park in the USA. Each measurement comprises the duration of the eruption in minutes (horizontal axis) and the time in minutes to the next eruption (vertical axis). We see that the data set forms two dominant clumps, and that a simple Gaussian distribution is unable to capture this structure, whereas a linear superposition of two Gaussians gives a better characterization of the data set. Such superpositions, formed by taking linear combinations of more basic distributions such as Gaussians, can be formulated as probabilistic models known as mixture distributions.\n\n\n\n\nFigure 4.2: Plots of the ‚Äôold faithful‚Äô data in which the blue curves show contours of constant probability density. On the left is a single Gaussian ditribution which has been fitted to the data using maximum likelihood. On the right the distribution is given by a linear combination of two Gaussians which has been fitted to the data by maximum likelihood using the EM technique, and which gives a better representation of the data\n\n\n\n\nIn Figure¬†4.3 we see that a linear combination of Gaussians can give rise to very complex densities. By using a sufficient number of Gaussians, and by adjusting their means and covariances as well as the coefficients in the linear combination, almost any continuous density can be approximated to arbitrary accuracy.\n\n\n\n\nFigure 4.3: Example of a Gaussian mixture distribution in one dimension showing three Gaussians (each scaled by a coefficient) in blue and their sum in red.\n\n\n\n\nWe therefore consider a superposition of \\(K\\) Gaussian densities of the form\n\\[p(x)= \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x| \\mu_k, \\Sigma_k)\n\\qquad(4.3)\\]\nwhich is called a mixture of Gaussians. Each Gaussian density \\(\\mathcal{N}(x| \\mu_k, \\Sigma_k)\\) is called a component of the mixture and has its own mean \\(\\mu_k\\) and covariance \\(\\Sigma_k\\).\nThe parameters \\(\\pi_k\\) are called mixing coefficients. They verify the conditions\n\\[\\sum_{k=1}^{K} \\pi_k = 1 \\quad \\text{and} \\quad 0 \\leq \\pi_k \\leq 1\\]\nIn order to find an equivalent formulation of the Gaussian mixture involving an explicit latent variable, we introduce a \\(K\\)-dimensional binary random variable \\(z\\) having a 1-of-\\(K\\) representation in which a particular element \\(z_k\\) is equal to 1 and all other elements are equal to 0. The values of \\(z_k\\) therefore satisfy \\(z_k \\in \\{0,1\\}\\) and \\(\\sum_k z_k =1\\), and we see that there are \\(K\\) possible states for the vector \\(z\\) according to which element is nonzero. The marginal distribution over \\(z\\) is specified in terms of the mixing coefficients \\(\\pi_k\\) , such that \\[p(z_k=1)=\\pi_k\\]\n\n\n\n\n\n\nLatent variable\n\n\n\nA latent variable is a variable that is not directly measurable, but its value can be inferred by taking other measurements.\nThis happens a lot in machine learning, robotics, statistics and other fields. For example, you may not be able to directly quantify intelligence (it‚Äôs not a countable thing like the number of brain cells you have), but we think it exists and we can run experiments that may tell us about intelligence. So your intelligence is a latent variable that affects your performance on multiple tasks even though it can not be directly measured (link).\n\n\nThe conditional distribution of \\(x\\) given a particular value for \\(z\\) is a Gaussian\n\\[p(x|z_k=1)= \\mathcal{N}(x| \\mu_k, \\Sigma_k)\\]\nThe joint distribution is given by \\(p(z)p(x|z)\\), and the marginal distribution of \\(x\\) is then obtained by summing the joint distribution over all possible states of \\(z\\) to give\n\\[p(x)= \\sum_z p(z)p(x|z) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x| \\mu_k, \\Sigma_k)\\]\nNow, we are able to work with the joint distribution \\(p(x|z)\\) instead of the marginal distribution \\(p(x)\\). This leads to significant simplification, most notably through the introduction of the Expectation-Maximization (EM) algorithm.\nAnother quantity that play an important role is the conditional probability of \\(z\\) given \\(x\\). We shall use \\(r(z_k)\\) to denote \\(p(z_k = 1|x)\\), whose value can be found using Bayes‚Äô theorem\n\\[\n\\begin{align}\nr(z_k)= p(z_k = 1|x) &= \\frac{ p(z_k = 1) p(x|z_k=1)}{\\displaystyle \\sum_{j=1}^{K} p(z_j = 1) p(x|z_j=1)} \\notag \\\\\n&= \\frac{\\pi_k \\mathcal{N}(x| \\mu_k, \\Sigma_k)}{\\displaystyle \\sum_{j=1}^{K} \\pi_j \\mathcal{N}(x| \\mu_j, \\Sigma_j)}\n\\end{align}\n\\qquad(4.4)\\]\nWe shall view \\(\\pi_k\\) as the prior probability of \\(z_k = 1\\), and the quantity \\(r(z_k)\\) as the corresponding posterior probability once we have observed \\(x\\). As we shall see in next section, \\(r(z_k)\\) can also be viewed as the responsibility that component \\(k\\) takes for ‚Äôexplaining‚Äô the observation \\(x\\).\n\n\n\n\n\n\nTip\n\n\n\nDoesn‚Äôt this reminds you of the Equation (1.1) when we used Bayes‚Äô theorm for Classification? where \\(p_k(x)\\) was the posterior probability that an observation \\(X=x\\) belongs to \\(k\\)-th class. The difference is that here the data is unlabled (we have no class), so we create a latent (hidden, unobserved) variable \\(z\\) that will play a similar role.\n\n\nIn Figure¬†4.4 the role of the responsibilities is illustrated on a sample of 500 points drawn from a mixture of three Gaussians.\n\n\n\n\nFigure 4.4: Example of 500 points drawn from a mixture of 3 Gaussians. (a) Samples from the joint distribution \\(p(z)p(x|z)\\) in which the three states of \\(z\\), corresponding to the three components of the mixture, are depicted in red, green, and blue, and (b) the corresponding samples from the marginal distribution \\(p(x)\\), which is obtained by simply ignoring the values of \\(z\\) and just plotting the \\(x\\) values. The data set in (a) is said to be complete, whereas that in (b) is incomplete. (c) The same samples in which the colours represent the value of the responsibilities \\(r(z_{nk})\\) associated with data point \\(x_n\\), obtained by plotting the corresponding point using proportions of red, blue, and green ink given by \\(r(z_{nk})\\) for \\(k = 1,2,3\\), respectively.\n\n\n\n\nSo the form of the Gaussian mixture distribution is governed by the parameters \\(\\pi\\), \\(\\mu\\) and \\(\\Sigma\\), where we have used the notation \\(\\pi=\\{\\pi_1,\\ldots,\\pi_K\\}\\), \\(\\mu=\\{\\mu_1,\\ldots,\\mu_K\\}\\) and \\(\\Sigma=\\{\\Sigma_1,\\ldots,\\Sigma_K\\}\\). One way to set the values of these parameters is to use maximum likelihood. The log of the likelihood function is given by\n\\[\\ln p(X|\\pi,\\mu,\\Sigma)=\\sum_{n=1}^{N} \\ln \\left\\lbrace \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x_n | \\mu_k, \\Sigma_k) \\right\\rbrace\\]\nWe immediately see that the situation is now much more complex than with a single Gaussian, due to the presence of the summation over \\(k\\) inside the logarithm. As a result, the maximum likelihood solution for the parameters no longer has a closed-form analytical solution. One approach to maximizing the likelihood function is to use iterative numerical optimization techniques. Alternatively we can employ a powerful framework called Expectation Maximization, which will be discussed in this chapter."
  },
  {
    "objectID": "em.html#em-for-gaussian-mixtures",
    "href": "em.html#em-for-gaussian-mixtures",
    "title": "\n4¬† Gaussian Mixture Models & EM\n",
    "section": "\n4.3 EM for Gaussian Mixtures",
    "text": "4.3 EM for Gaussian Mixtures\nSuppose we have a data set of observations \\(\\{x_1, \\ldots, x_N\\}\\), which gives a data set \\(X\\) of size \\(N \\times D\\), and we wish to model this data using a mixture of Gaussians. Similarly, the corresponding latent variable are denoted by a \\(N \\times K\\) matrix \\(Z\\) with rows \\(z_n^K\\).\n\n\n\n\n\n\nTip\n\n\n\nRecall that the objective is to estimate the parameters \\(\\pi\\), \\(\\mu\\) and \\(\\Sigma\\) in order to estimate the posterior probabilities (named also responsibilities, called \\(\\, r(z_k)\\) in this chapter). To do so, we find the estimators that maximize the log of the likelihood function.\n\n\nIf we assume that the data points are i.i.d. (independent and identically distributed), then we can calculate the log of the likelihood function, which is given by\n\\[\\ln p(X|\\pi,\\mu,\\Sigma)=\\sum_{n=1}^{N} \\ln \\left\\lbrace \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x_n | \\mu_k, \\Sigma_k) \\right\\rbrace \\qquad(4.5)\\]\nAn elegant and powerful method for finding maximum likelihood solutions for this models with latent variables is called the Expectation Maximization algorithm, or EM algorithm.\nSetting the derivatives of \\(\\ln p(X|\\pi,\\mu,\\Sigma)\\) in (4.5) respectively with respect to the \\(\\mu_k,\\Sigma_k\\) and \\(\\pi_k\\) to zero, we obtain\n\\[\\mu_k = \\frac{1}{N_k} \\sum_{n=1}^{N} r(z_{nk}) x_n  \\qquad(4.6)\\]\nwhere we define \\[N_k= \\sum_{n=1}^{N}r(z_{nk})\\]\nWe can interpret \\(N_k\\) as the effective number of points assigned to cluster \\(k\\). Note carefully the form of this solution. We see that the mean \\(\\mu_k\\) for the \\(k\\)-th Gaussian component is obtained by taking a weighted mean of all of the points in the data set, in which the weighting factor for data point \\(x_n\\) is given by the posterior probability \\(r(z_{nk})\\) that component \\(k\\) was responsible for generating \\(x_n\\).\nAs for \\(\\sigma_k\\) we obtain\n\\[\\Sigma_k= \\frac{1}{N_k} \\sum_{n=1}^{N} r(z_{nk}) (x_n - \\mu_k)(x_n - \\mu_k)^T\n\\qquad(4.7)\\]\nwhich has the same form as the corresponding result for a single Gaussian fitted to the data set, but again with each data point weighted by the corresponding posterior probability and with the denominator given by the effective number of points associated with the corresponding component.\nFinally, for the mixing coefficients \\(\\pi_k\\) we obtain\n\\[\n\\pi_k=\\frac{N_k}{N}\n\\qquad(4.8)\\]\nso that the mixing coefficient for the \\(k\\)-th component is given by the average responsibility which that component takes for explaining the data points.\nWe first choose some initial values for the means, covariances, and mixing coefficients. Then we alternate between the following two updates that we shall call the E step and the M step. In the expectation step, or E step, we use the current values for the parameters to evaluate the posterior probabilities, or responsibilities, given by (4.4). We then use these probabilities in the maximization step, or M step, to re-estimate the means, covariances, and mixing coefficients using the results in Equations (4.6), (4.7) and (4.8). The algorithm of EM for mixtures of Gaussians is shown in the following Algorithm:\n\n\n\n\n\n\nThe EM for Gaussian mixtures\n\n\n\n\n\nData:\n\n\\(\\mathbf{X}= \\{x_{kd}, \\,\\,\\,\\, k=1,\\ldots,N, d=1,\\ldots,D\\}\\) where \\(D\\) is the dimension of the feature space. \\(Z\\) the latent variables matrix.\n\n\n\nResult:\nPosterior probabilities \\(r(z_{nk})\\) and the model parameters \\(\\mu,\\Sigma\\) and \\(\\pi\\).\n\n\n\nInitialization:\n\nChoose a value for \\(K\\), \\(1 < K < N\\).\nInitialize the means \\(\\mu_k\\), the covariances \\(\\Sigma_k\\) and mixing coefficients \\(\\pi_k\\) randomly.\nEvaluate the initial value of the log likelihood.\n\n\n\n\nLearning: repeat\n\n\nE step:\n\nEvaluate the responsibilities using the current parameter values: \\[r(z_{nk})= \\frac{\\pi_k \\mathcal{N}(x| \\mu_k, \\Sigma_k)}{\\displaystyle \\sum_{j=1}^{K} \\pi_j \\mathcal{N}(x| \\mu_j, \\Sigma_j)}\\]\n\n\nM step:\n\nRe-estimate the parameters using the current responsibilities: \\[\\mu_k = \\frac{1}{N_k} \\sum_{n=1}^{N} r(z_{nk}) x_n\\] \\[\\Sigma_k= \\frac{1}{N_k} \\sum_{n=1}^{N} r(z_{nk}) (x_n - \\mu_k)(x_n - \\mu_k)^T\\] \\[\\pi_k=\\frac{N_k}{N}\\] \\[\\text{where} \\quad N_k= \\sum_{n=1}^{N}r(z_{nk})\\]\nEvaluate the log likelihood: \\[\\ln p(X|\\pi,\\mu,\\Sigma)=\\sum_{n=1}^{N} \\ln \\left\\lbrace \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x_n | \\mu_k, \\Sigma_k) \\right\\rbrace\\] Until convergence of either the parameters or the log likelihood. If the convergence criterion is not satisfied return to E step.\n\n\n\n\n\nThe EM algorithm for a mixture of two Gaussians applied to the rescaled Old Faithful data set is illustrated in Figure¬†4.5. In plot (a) we see the initial configuration, the Gaussian component are shown as blue and red circles. Plot (b) shows the result of the initial E step where we update the responsibilities. Plot (c) shows the M step where we update the parameters. Plots (d), (e), and (f) show the results after 2, 5, and 20 complete cycles of EM, respectively. In plot (f) the algorithm is close to convergence.\n\n\n\n\nFigure 4.5: Illustration of the EM algorithm using the Old Faithful dataset. A mixture of two Gaussians is used.\n\n\n\n\n\n\n\n\n\n\n\n\nSummary\n\n\n\nSummary of this chapter:\n\nGaussian Mixture Models (GMM) take a Gaussian and add another Gaussian(s).\nThis allows to model more complex data.\nWe fit a GMM with the Expectation-Maximization (EM) algorithm.\nExpectation-Maximization (EM) algorithm is a series of steps to find good parameter estimates when there are latent variables.\nEM steps:\n\nInitialize the parameter estimates.\nGiven the current parameter estimates, find the minimum log likelihood for \\(Z\\) (data + latent variables).\nGivent the current data, find better parameter estimates.\nRepeat steps 2 & 3.\n\n\n\n\n\n\n‚óº"
  },
  {
    "objectID": "Lab-EM.html#expectation-maximization-em",
    "href": "Lab-EM.html#expectation-maximization-em",
    "title": "Lab on Gaussian Mixture Models (GMM) and Expectation Maximization (EM) technique",
    "section": "\nExpectation-Maximization (EM)",
    "text": "Expectation-Maximization (EM)\nGMM vs \\(k\\)-means\nIn this section, we will use two artificial (simulated) datasets in which we know the ground truth (true labels) in order to compare the performances of \\(k\\)-means and GMM. To fit a GMM using EM technique in  or Python you need to use specified packages. Please refer to the note above.\n1. Download and import Data1 üî¢ and Data2 üî¢. Plot both of the datasets on the same window. Color the observations with respect to the ground truth, like in Figure¬†1.\n\n\n\n\n\n\n\nFigure 1: Data1 is plotted on left. Data2 on right. Colors shown with respect to ground truth.\n\n\n\n\n2. Apply \\(k\\)-means on both datasets with 4 clusters. Plot both of the dataset on the same window and color the observations with respect to \\(k\\)-means results. Interpret the results.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nOne way to think about the \\(k\\)-means model is that it places a circle (or, in higher dimensions, a hyper-sphere) at the center of each cluster, with a radius defined by the most distant point in the cluster. This radius acts as a hard cutoff for cluster assignment within the training set: any point outside this circle is not considered a member of the cluster. You can try to visualize the circles on your plots.\n\n\n3. Now fit a GMM model on the datasets. To do so in , load the mclust library. Then you can use the function Mclust() on your data (this function will choose automatically the number of mixtures, basing on BIC criterion). Use the clustering results from your GMM model to visualize the results on both of the datasets, color the observations with respect to the clusters obtained from the GMM model. Interpret the results.\n\n\n\nIn the following questions from this section, explore the mclust library and what it offers. Apply its functions on Data2.\n\n\n\n\n\n\nTip\n\n\n\nmclust is a contributed  package for model-based clustering, classification, and density estimation based on finite normal mixture modelling. It provides functions for parameter estimation via the EM algorithm for normal mixture models with a variety of covariance structures, and functions for simulation from these models. Also included are functions that combine model-based hierarchical clustering, EM for mixture estimation and the Bayesian Information Criterion (BIC) in comprehensive strategies for clustering, density estimation and discriminant analysis. Additional functionalities are available for displaying and visualizing fitted models along with clustering, classification, and density estimation results.\n\n\n4. Show the summary of the GMM model you fitted on Data2. Explain what it shows.\n5. mclust package offers some visualization. To plot your two-dimensional data, use the standard plot function applied on your model. Apply the following code, given that the model is named gmm_model, and interpret what it shows.\n\n\n\n\nplot(gmm_model, what = \"classification\")\nplot(gmm_model, what = \"uncertainty\")\n\n6. mclust package uses the Bayesian Information Criterion (BIC) to choose the best number of mixtures. To see the values of BIC for different number of mixtures use the following code.\n\nplot(gmm_model, what = \"BIC\")\n\nInformation criteria are based on penalised forms of the log-likelihood. As the likelihood increases with the addition of more components, a penalty term for the number of estimated parameters is subtracted from the log-likelihood. The BIC is a popular choice in the context of GMMs, and takes the form\n\\[ \\text{BIC} \\approx 2 \\ell (X|\\hat{\\theta}) - \\nu \\log (n)\\]\nwhere \\(\\theta\\) is the set of parameters (in GMM it is \\(\\theta=\\{\\mu,\\Sigma,\\pi\\})\\), and \\(\\ell (X|\\hat{\\theta})\\) is the log-likelihood at the Maximum Likelihood Estimators \\(\\hat{\\theta}\\) for the model, \\(n\\) is the sample size, and \\(\\nu\\) is the number of estimated parameters. We select the model that maximises BIC.\n\n\n\n\n\n\nTip\n\n\n\nWhat you see on the figure showing the BIC values are different parameterisations of the within-group covariance matrix \\(\\Sigma_k\\). In GMM, clusters are ellipsoidal, centered at the mean vector \\(\\mu_k\\), and with other geometric features, such as volume, shape and orientation, determined by the covariance matrix \\(\\Sigma_k\\).\n\n\n\n7. Though GMM is often categorized as a clustering algorithm, fundamentally it is an algorithm for density estimation. That is to say, the result of a GMM fit to some data is technically not a clustering model, but a generative probabilistic model describing the distribution of the data. Density estimation plays an important role in applied statistical data analysis and theoretical research. A density estimate based on GMM can be obtained using the function densityMclust(). Apply it on Data2 and visualize the estimated densities (show an ‚Äúimage‚Äù and a ‚Äúperspective‚Äù plot of the bivariate density estimate).\nEM on 1D\nIn this part you must fit a GMM model on a one dimensional simulated data.\n8. Create a data table of 300 observations in which you have two columns:\n\nThe first column contains generated data. Those data are generated from three Gaussian distributions with different parameters.\nThe second column corresponds to the groud truth (every observation was generated from which Gaussian).\nHint: functions you may need are rnorm(), rep(), rbind() or cbind().\nYou must of course set a seed (your sutdent_pk). An example of 9 generated values from three Gaussians is shown in the following table:\n\n\n\n\n\nX\nsource\n\n\n\n-5.6264538\n1\n\n\n-4.8163567\n1\n\n\n-5.8356286\n1\n\n\n1.5952808\n2\n\n\n0.3295078\n2\n\n\n-0.8204684\n2\n\n\n5.4874291\n3\n\n\n5.7383247\n3\n\n\n5.5757814\n3\n\n\n\n\n\n9. Show your generated data on one axe (this kind of figures are called stripchart), color them with respect to ground truth, you must obtain something like:\n\n\n\n\n\n10. Plot the histogram corresponding to your generated data. Interpret it.\n11. Fit a GMM model on your generated data. Print the summary and visualize your results. Explain your results.\n\n\n\n12. Apply a density estimate on your generated data and visualize it. Interpret the obtained figure."
  },
  {
    "objectID": "Lab-EM.html#em-from-scratch",
    "href": "Lab-EM.html#em-from-scratch",
    "title": "Lab on Gaussian Mixture Models (GMM) and Expectation Maximization (EM) technique",
    "section": "EM from scratch",
    "text": "EM from scratch\nIn this second part of this PW you will build a GMM model from scratch, you must develop the EM technique to fit the model.\n2.1 Generate a two-dimensional dataset from a \\(k\\)-component Gaussian mixture density with different means and different covariance matrices. It is up to you to choose the mixing proportions \\(\\{\\pi_1,\\ldots,\\pi_k\\}\\).\n2.2 Implement the EM algorithm to fit a GMM on your generated data:\n\nInitialize the mixing proportions and the covariance matrices (e.g., you can initialize with equal mixing proportions and Identity covariance matrices).\nInitialize the means ‚Äúrandomly‚Äù (by your own choice of \\(k\\)).\nIn the EM training loop, store the value of the observed-data log-likelihood at each iteration.\nAt convergence, plot the log-likelihood curve.\n\n2.3 Create a function that selects the number of mixture components by computing the values of BIC criterion for \\(k\\) varying from 1 to 10.\n2.4 On your generated data, compare your results obtained with the algorithm you developed and the ground truth (in terms of the chosen number of mixture components; and in terms of error rate).\n2.5 Apply the algorithm you developed on Iris üî¢ dataset.\n\n\n\n\n\n\nTip\n\n\n\nTo visualize your results on Iris dataset, you can use PCA projection and coloring with respect to clustering results.\nThe package mclust provides also a dimensionality reduction technique in function MclustDR().\n\n\n\n‚óº"
  },
  {
    "objectID": "density-based.html",
    "href": "density-based.html",
    "title": "5¬† Density-based Clustering",
    "section": "",
    "text": "Documents for this chapter\n\n\n\n\nTable of contents üìÑ from ‚ÄúData Clustering Algorithms and Applications‚Äù book for authors C Aggarwal and C Reddy.\nCluster analysis lecture notes üìÑ from Prof.¬†Erich Schuberts.\nClustering lecture notes üìÑ from Prof.¬†Emilie Chouzenoux and Prof.¬†Fr√©d√©ric PASCAL.\nDBSCAN chapter üìÑ from ‚ÄúThe Unsupervised Learning‚Äù book for authors A Jones, C Kruger and B Johnston.\nDensity-bsed Methods üìÑ from ‚ÄúData Mining Concepts and Techniques‚Äù book for authors J Han, M Kamber and J Pei.\nDBSCAN: Density-Based Clustering üìÑ from ‚ÄúPractical Guide To Cluster Analysis in R‚Äù book for author A Kassambara.\nCluster validation: Internal versus External indexes."
  },
  {
    "objectID": "density-based.html#lab",
    "href": "density-based.html#lab",
    "title": "5¬† Density-based Clustering",
    "section": "5.2 Lab",
    "text": "5.2 Lab\nDuring an interview, you are asked to create the DBSCAN algorithm from scratch using a generated two-dimensional dataset. To do this, you will need to convert the theory behind neighborhood searching into production code, with a recursive call that adds neighbors. As explained in the documents, you will use a distance scan in space surrounding a specified point to add these neighbors.\nGiven what you‚Äôve learned about DBSCAN and distance metrics from the documents, build an implementation of DBSCAN from scratch in R or Python. You are free to use R or Python libraries to evaluate the distances.\nThese steps will help you to complete the activity:\n\nGenerate a random cluster dataset.\nVisualize the data.\nCreate functions from scratch that allow you to call DBSCAN on a dataset.\nUse your created DBSCAN implementation to find clusters in the generated dataset. Feel free to use hyperparameters as you see fit, tuning them based on their performance.\nVisualize the clustering performance of your DBSCAN implementation from scratch.\n\nThe desired outcome of this lab is for you to implement how DBSCAN works from the ground up before you use the fully packaged implementation in scikit-learn for example. Taking this approach to any machine learning algorithm from scratch is important, as it helps you ‚Äúearn‚Äù the ability to use easier implementations, while still being able to discuss DBSCAN in depth in the futur.\nOnce your implementation is done, you can complete your work by1:\n\nExtend your algorithm to multidimensional datasets (>2).\nImplementing a tool to help tuning the hyperparameters of DBSCAN.\nImplementing a tool to compare the performances of DBSCAN with k-means and Hierarchical Clustering.\nImplementing other density based algorithms: HDBSCAN or OPTICS.\n\n\n‚óº"
  },
  {
    "objectID": "Lab-image-segmentation.html",
    "href": "Lab-image-segmentation.html",
    "title": "\n6¬† Multi-spectral Image Segmentation\n",
    "section": "",
    "text": "You are free to use  or Python.\nOr you can use both.\nIf you want to use both, try Rstudio‚Äôs reticulate package, it lets you use python in Rstudio. You can then write a Rmarkdown file to produce your report, containing chunks from both languages."
  },
  {
    "objectID": "Lab-image-segmentation.html#data",
    "href": "Lab-image-segmentation.html#data",
    "title": "\n6¬† Multi-spectral Image Segmentation\n",
    "section": "\n6.1 Data",
    "text": "6.1 Data\nFor this study, you will need two data files:\n\nmars.csv üî¢\nmask.csv üî¢\n\nThe data in the mars.csv file represent a hyper-spectral image of the surface of Mars. Visible and near-infrared imagery is a key tele-detection technique, used to study planets thanks to satellites with on-board spectrometers. In march 2014, the OMEGA equipment collected more than 310 Go of raw images. It mapped Mars surface with a resolution between 300 and 3000 meters, depending on the height of the space ship1. For each pixel, the spectral response between 0.36 and 5.2 ¬µm was collected and sampled accross 255 channels. The objective is to characterize the geological composition of Mars surface and in particular to distinguish between different classes of silicates, minerals, oxydes, carbonates and frozen regions.\nThese data are represented by an image of 300 x 128 pixels. A vector of 255 spectral values (variables, characteristics, features) is associated to each of the 38400 pixels (instances, individuals).\n\n\n\n\n\n\nObjectives\n\n\n\nAccording to experts, there are \\(K=5\\) geological classes to identify on the map. The objective is to operate an automated unsupervised classification of the pixels, in order to segment the image into 5 distinct categories (colors). Before conducting the segmentation, an exploratory approach is conducted to better understand the data and decide how it should be pre-processed. Then, different clustering algorithms are applied and compared on the MARS dataset."
  },
  {
    "objectID": "Lab-image-segmentation.html#data-exploration",
    "href": "Lab-image-segmentation.html#data-exploration",
    "title": "\n6¬† Multi-spectral Image Segmentation\n",
    "section": "\n6.2 Data exploration",
    "text": "6.2 Data exploration\n\n6.2.1 Read the data and understand the formatting\n\nRead the ‚Äúmars.csv‚Äù data.\nDisplay a summary of the data.\nHow many rows and columns do the data have? What does it mean in terms of number of data points? Dimensionality of the data?\n\n6.2.2 Data preprocessing\n\n\nDraw the histograms of six different dimensions (wave length) selected at random. They represent the distributions of the values of a given dimension across the different pixels.\n\nWhat can you say regarding the discriminative power of single dimensions?\nThe symmetry of the data?\n\n\n\nDraw the box plots of all different wave lengths. They should all appear on the same graph.\n\nWhat can you say regarding the spread of the different dimensions?\nThe symmetry of the data?\nOutlier values?\n\n\nConduct the required data transformations/preprocessing. Visualize the new distribution of the transformed data (histograms, box plots), what can you say?"
  },
  {
    "objectID": "Lab-image-segmentation.html#dimensionality-reduction",
    "href": "Lab-image-segmentation.html#dimensionality-reduction",
    "title": "\n6¬† Multi-spectral Image Segmentation\n",
    "section": "\n6.3 Dimensionality reduction",
    "text": "6.3 Dimensionality reduction\n\n6.3.1 PCA\n\nConduct a Principal Component Analysis decomposition of the MARS data.\nVisualize the explained variance as a function of the number of dimensions selected. (In Python, We can use the pca.explained_variance_ratio argument from the scikit learn PCA implementation)\nHow many dimensions should be used to conduct a cluster analysis?\nPlot the Variable factor map (Features representation) for the first two PCA dimensions. What can you say?\nPlot the Individuals factor map (Individuals‚Äô representation) for the first two PCA dimensions. What can you say?\n\n6.3.2 t-SNE\n\nApply the t-SNE method on the MARS data. What can you say?"
  },
  {
    "objectID": "Lab-image-segmentation.html#clustering",
    "href": "Lab-image-segmentation.html#clustering",
    "title": "\n6¬† Multi-spectral Image Segmentation\n",
    "section": "\n6.4 Clustering",
    "text": "6.4 Clustering\n\n6.4.1 K-means\n\nConduct one of the methods to choose the number of clusters K to apply K-means on these data? What K should be chosen?\nIn practice, the experts tell us that the number of different geological compositions in this image is 5. Apply K-means on the selected PCA dimensions, with this value of K.\nDisplay the clusters in the PCA Individual factor map.\nDisplay the 300x128 image where the pixels represent the clusters. (We can use the reshape() function.)\nPlot the curves representing the values of the wave lengths of the cluster centers.\nRepeat the process using the complete data instead of the data reduced with PCA. Compare the two results using external metrics (normalized mutual information, Fowlkes Mallows). Are the clusters obtained similar?\nRepeat the process using the data reduced with t-SNE. Compare with the previous results.\n\n6.4.2 Agglomerative Clustering\n\nApply Agglomerative Clustering on the MARS dataset, visualize the results.\n\n6.4.3 Gaussian Mixture Models\n\nApply Gaussian Mixture Models on the MARS dataset, visualize the results.\n\n6.4.4 DBSCAN\n\nApply DBSCAN on the MARS dataset, using your previously implemented algorithm from scratch, visualize the results.\nCompare your results with sickit-learn‚Äôs dbscan (or R‚Äôs dbscan function)."
  },
  {
    "objectID": "Lab-image-segmentation.html#comparison-of-clustering-algorithms",
    "href": "Lab-image-segmentation.html#comparison-of-clustering-algorithms",
    "title": "\n6¬† Multi-spectral Image Segmentation\n",
    "section": "\n6.5 Comparison of clustering algorithms",
    "text": "6.5 Comparison of clustering algorithms\nThe ground truth of the pixels values was provided by domain experts and can be found in the mask.csv file.\n\nUsing external validation metrics, evaluate which clustering algorithms, parameters, data preprocess ing, etc. gives the best clustering results. In particular, we can study the influence of using PCA, the number of classes, the linkage methods, etc."
  },
  {
    "objectID": "Lab-image-segmentation.html#bonus-supervised-learning-using-discriminative-analysis",
    "href": "Lab-image-segmentation.html#bonus-supervised-learning-using-discriminative-analysis",
    "title": "\n6¬† Multi-spectral Image Segmentation\n",
    "section": "\n6.6 Bonus: Supervised learning using Discriminative Analysis",
    "text": "6.6 Bonus: Supervised learning using Discriminative Analysis\nTaking into account the ground truth of the pixels values in the mask.csv file we can create a supervised model to learn the class of each pixel.\n\nFit a supervised model using LDA and QDA and evalute its performance.\n\n\n‚óº"
  }
]