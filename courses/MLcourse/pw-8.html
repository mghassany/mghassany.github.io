<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>PW 8 | Machine Learning</title>
<meta name="author" content="Mohamad Ghassany">
<meta name="description" content="In the first part of this PW you will: Learn how to fit a Gaussian Mixture Model (GMM) using the mclust package. Compare \(k\)-means and GMM on artificial data (2-D data). Fit a GMM on univariate...">
<meta name="generator" content="bookdown 0.24.10 with bs4_book()">
<meta property="og:title" content="PW 8 | Machine Learning">
<meta property="og:type" content="book">
<meta property="og:description" content="In the first part of this PW you will: Learn how to fit a Gaussian Mixture Model (GMM) using the mclust package. Compare \(k\)-means and GMM on artificial data (2-D data). Fit a GMM on univariate...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PW 8 | Machine Learning">
<meta name="twitter:description" content="In the first part of this PW you will: Learn how to fit a Gaussian Mixture Model (GMM) using the mclust package. Compare \(k\)-means and GMM on artificial data (2-D data). Fit a GMM on univariate...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<link href="libs/vembedr-0.1.5/css/vembedr.css" rel="stylesheet">
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script><script src="libs/plotly-binding-4.10.0/plotly.js"></script><script src="libs/typedarray-0.1/typedarray.min.js"></script><link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet">
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script><link href="libs/plotly-htmlwidgets-css-2.5.1/plotly-htmlwidgets.css" rel="stylesheet">
<script src="libs/plotly-main-2.5.1/plotly-latest.min.js"></script><script type="text/x-mathjax-config">
    const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
    for (let popover of popovers){
      const div = document.createElement('div');
      div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
      div.innerHTML = popover.getAttribute('data-content');
      
      // Will this work with TeX on its own line?
      var has_math = div.querySelector("span.math");
      if (has_math) {
        document.body.appendChild(div);
      	MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
      	MathJax.Hub.Queue(function(){
          popover.setAttribute('data-content', div.innerHTML);
      	})
      }
    }
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="css/ims-style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Machine Learning</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="introduction.html">Introduction</a></li>
<li class="book-part">Supervised Learning</li>
<li class="book-part">Regression</li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">1</span> Linear Regression</a></li>
<li><a class="" href="practical-work-1.html">Practical Work 1</a></li>
<li><a class="" href="multiple-linear-regression.html"><span class="header-section-number">2</span> Multiple Linear Regression</a></li>
<li><a class="" href="pw-2.html">PW 2</a></li>
<li class="book-part">Classification</li>
<li><a class="" href="logistic-regression.html"><span class="header-section-number">3</span> Logistic Regression</a></li>
<li><a class="" href="pw-3.html">PW 3</a></li>
<li><a class="" href="discriminant-analysis.html"><span class="header-section-number">4</span> Discriminant Analysis</a></li>
<li><a class="" href="pw-4.html">PW 4</a></li>
<li><a class="" href="decision-trees-random-forests.html"><span class="header-section-number">5</span> Decision Trees &amp; Random Forests</a></li>
<li><a class="" href="pw-5.html">PW 5</a></li>
<li class="book-part">Dimensionality Reduction</li>
<li><a class="" href="principal-components-analysis.html"><span class="header-section-number">6</span> Principal Components Analysis</a></li>
<li><a class="" href="pw-6.html">PW 6</a></li>
<li class="book-part">Unsupervised Learning</li>
<li><a class="" href="kmeans-hierarchical-clustering.html"><span class="header-section-number">7</span> Kmeans &amp; Hierarchical Clustering</a></li>
<li><a class="" href="pw-7.html">PW 7</a></li>
<li><a class="" href="gaussian-mixture-models-em.html"><span class="header-section-number">8</span> Gaussian Mixture Models &amp; EM</a></li>
<li><a class="active" href="pw-8.html">PW 8</a></li>
<li class="book-part">Hackathon</li>
<li><a class="" href="hackathon.html">Hackathon</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="final-grades.html"><span class="header-section-number">A</span> Final Grades</a></li>
<li><a class="" href="app-introRStudio.html"><span class="header-section-number">B</span> Introduction to RStudio</a></li>
<li><a class="" href="app-ht.html"><span class="header-section-number">C</span> Review on hypothesis testing</a></li>
<li><a class="" href="use-qual.html"><span class="header-section-number">D</span> Use of qualitative predictors</a></li>
<li><a class="" href="model-selection.html"><span class="header-section-number">E</span> Model Selection</a></li>
<li><a class="" href="references-and-credits.html"><span class="header-section-number">F</span> References and Credits</a></li>
<li><a class="" href="other-references.html"><span class="header-section-number">G</span> Other References</a></li>
<li><a class="" href="main-references-credits.html">Main References &amp; Credits</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="pw-8" class="section level1 unnumbered">
<h1>PW 8<a class="anchor" aria-label="anchor" href="#pw-8"><i class="fas fa-link"></i></a>
</h1>
<!-- https://www.python-course.eu/expectation_maximization_and_gaussian_mixture_models.php -->
<!-- mclust -->
<!-- https://chamroukhi.users.lmno.cnrs.fr/teaching.php -->
<!-- https://chamroukhi.users.lmno.cnrs.fr/Teaching/ApprentissageM2/tp-EM-GMM.pdf -->
<!-- From my Firefox session: -->
<!-- In Depth: Gaussian Mixture Models | Python Data Science Handbook -->
<!-- https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html -->
<!-- A quick tour of mclust -->
<!-- https://cran.r-project.org/web/packages/mclust/vignettes/mclust.html -->
<!-- mclust: an R package for normal mixture modeling -->
<!-- https://www.stat.washington.edu/mclust/ -->
<!-- mclust Version 4 for R: Normal Mixture Modeling for Model-Based Clustering, Classification, and Density Estimation - tr597.pdf -->
<!-- https://www.stat.washington.edu/sites/default/files/files/reports/2012/tr597.pdf -->
<!-- tp-EM-GMM.pdf -->
<!-- https://chamroukhi.users.lmno.cnrs.fr/Teaching/ApprentissageM2/tp-EM-GMM.pdf -->
<!-- Machine Learning with Python: Expectation Maximization and Gaussian Mixture Models in Python -->
<!-- https://www.python-course.eu/expectation_maximization_and_gaussian_mixture_models.php -->
<!-- Fitting a Mixture Model Using the Expectation-Maximization Algorithm in R -->
<!-- http://tinyheero.github.io/2016/01/03/gmm-em.html -->
<!-- Using Mixture Models for Clustering -->
<!-- http://tinyheero.github.io/2015/10/13/mixture-model.html -->
<!-- Practical session on Clustering methods -->
<!-- http://perso.mines-paristech.fr/fabien.moutarde/ES_MachineLearning/Practical_clustering/Practical_clustering.htm -->
<!-- Mines ParisTech machine learning practical session: Model selection and clustering, 13 May 2011. -->
<!-- http://members.cbio.mines-paristech.fr/~thocking/mines-course/2011-05-13-clustering/2011-05-13-clustering.html -->
<p>In the first part of this PW you will:</p>
<ul>
<li>Learn how to fit a Gaussian Mixture Model (GMM) using the <code>mclust</code> <svg aria-hidden="true" role="img" viewbox="0 0 581 512" style="height:1em;width:1.13em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:steelblue;overflow:visible;position:relative;"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"></path></svg> package.</li>
<li>Compare <span class="math inline">\(k\)</span>-means and GMM on artificial data (2-D data).</li>
<li>Fit a GMM on univariate (1-D) simulated data.</li>
</ul>
<p>In the second part of this PW you will build an algorithm from <strong>scratch</strong>. Your algorithm must fit a GMM model using the <strong>E</strong>xpectation-<strong>M</strong>aximization (EM) technique on any multi dimensional dataset.</p>
<div id="report-template" class="section level2 unnumbered">
<h2>Report template<a class="anchor" aria-label="anchor" href="#report-template"><i class="fas fa-link"></i></a>
</h2>
<p>For this session, write your report in a RMarkdown script, in which you must use the following YAML header settings (replace the default YAML header by this one, edit the author’s name and show the date):</p>
<pre><code>---
title: "Week 8"
subtitle: "Gaussian Mixture Models &amp; EM"
author: LastName FirstName
date: "`#r format(Sys.time())`" # remove the # to show the date
output:
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    theme: cerulean
    highlight: espresso
---</code></pre>
</div>
<div id="em-using-mclust" class="section level2" number="8.4">
<h2>
<span class="header-section-number">8.4</span> EM using <code>mclust</code><a class="anchor" aria-label="anchor" href="#em-using-mclust"><i class="fas fa-link"></i></a>
</h2>
<div id="gmm-vs-k-means" class="section level3 unnumbered">
<h3>GMM vs <span class="math inline">\(k\)</span>-means<a class="anchor" aria-label="anchor" href="#gmm-vs-k-means"><i class="fas fa-link"></i></a>
</h3>
<p>In this section, we will use two artificial (simulated) datasets in which we know the ground truth (true labels) in order to compare the performances of <span class="math inline">\(k\)</span>-means and GMM. To fit a GMM using EM technique you need to install and use the package <code>mclust</code>.</p>
<p><strong>1.</strong> Download and import <a target="_blank" href="datasets/data1.csv"> Data1 <i class="fa fa-table" aria-hidden="true"></i></a> and <a target="_blank" href="datasets/data2.csv"> Data2 <i class="fa fa-table" aria-hidden="true"></i></a>. Plot both of the datasets on the same window. Color the observations with respect to the ground truth, like in Figure <a href="pw-8.html#fig:fig1">8.6</a>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:fig1"></span>
<img src="Machine-Learning_files/figure-html/fig1-1.png" alt="Data1 is plotted on left. Data2 on right. Colors shown with respect to ground truth." width="70%"><p class="caption">
Figure 8.6: Data1 is plotted on left. Data2 on right. Colors shown with respect to ground truth.
</p>
</div>
<p><strong>2.</strong> Apply <span class="math inline">\(k\)</span>-means on both datasets with 4 clusters. Plot both of the dataset on the same window and color the observations with respect to <span class="math inline">\(k\)</span>-means results. Interpret the results.</p>
<div class="rmdtip">
<p>
One way to think about the <span class="math inline"><span class="math inline">\(k\)</span></span>-means model is that it places a circle
(or, in higher dimensions, a hyper-sphere) at the center of each
cluster, with a radius defined by the most distant point in the cluster.
This radius acts as a hard cutoff for cluster assignment within the
training set: any point outside this circle is not considered a member
of the cluster. You can try to visualize the circles on your plots.
</p>
</div>
<p><strong>3.</strong> Now fit a GMM model on the datasets. To do so, load the <code>mclust</code> library. Then you can use the function <code>Mclust()</code> on your data (this function will choose automatically the number of mixtures, basing on BIC criterion). Use the clustering results from your GMM model to visualize the results on both of the datasets, color the observations with respect to the clusters obtained from the GMM model. Interpret the results.</p>
<p><strong>In the following questions from this section, explore the <code>mclust</code> library and what it offers. Apply its functions on Data2.</strong></p>
<div class="rmdtip">
<p><code>mclust</code> is a contributed <svg aria-hidden="true" role="img" viewbox="0 0 581 512" style="height:1em;width:1.13em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:steelblue;overflow:visible;position:relative;"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"></path></svg> package for model-based clustering, classification, and density estimation based on finite normal mixture modelling. It provides functions for parameter estimation via the EM algorithm for normal mixture models with a variety of covariance structures, and functions for simulation from these models. Also included are functions that combine model-based hierarchical clustering, EM for mixture estimation and the Bayesian Information Criterion (BIC) in comprehensive strategies for clustering, density estimation and discriminant analysis. Additional functionalities are available for displaying and visualizing fitted models along with clustering, classification, and density estimation results.</p>
</div>
<p><strong>4.</strong> Show the summary of the GMM model you fitted on Data2. Explain what it shows.</p>
<p><strong>5.</strong> <code>mclust</code> package offers some visualization. To plot your two-dimensional data, use the standard plot function applied on your model. Apply the following code, given that the model is named <code>gmm_model</code>, and interpret what it shows.</p>
<div class="sourceCode" id="cb81"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">gmm_model</span>, what <span class="op">=</span> <span class="st">"classification"</span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">gmm_model</span>, what <span class="op">=</span> <span class="st">"uncertainty"</span><span class="op">)</span></code></pre></div>
<p><strong>6.</strong> <code>mclust</code> package uses the Bayesian Information Criterion (BIC) to choose the best number of mixtures. To see the values of BIC for different number of mixtures use the following code.</p>
<div class="sourceCode" id="cb82"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">gmm_model</span>, what <span class="op">=</span> <span class="st">"BIC"</span><span class="op">)</span></code></pre></div>
<p>Information criteria are based on penalised forms of the log-likelihood. As the likelihood increases with the addition of more components, a penalty term for the number of estimated parameters is subtracted from the log-likelihood. The BIC is a popular choice in the context of GMMs, and takes the form</p>
<p><span class="math display">\[ \text{BIC} \approx 2 \ell (X|\hat{\theta}) - \nu \log (n)\]</span></p>
<p>where <span class="math inline">\(\theta\)</span> is the set of parameters (in GMM it is <span class="math inline">\(\theta=\{\mu,\Sigma,\pi\})\)</span>, and <span class="math inline">\(\ell (X|\hat{\theta})\)</span> is the log-likelihood at the Maximum Likelihood Estimators <span class="math inline">\(\hat{\theta}\)</span> for the model, <span class="math inline">\(n\)</span> is the sample size, and <span class="math inline">\(\nu\)</span> is the number of estimated parameters. We select the model that maximises BIC.</p>
<div class="rmdtip">
<p>
What you see on the figure showing the BIC values are different
parameterisations of the within-group covariance matrix <span class="math inline"><span class="math inline">\(\Sigma_k\)</span></span>. In GMM, clusters are
ellipsoidal, centered at the mean vector <span class="math inline"><span class="math inline">\(\mu_k\)</span></span>, and with other geometric features,
such as volume, shape and orientation, determined by the covariance
matrix <span class="math inline"><span class="math inline">\(\Sigma_k\)</span></span>.
</p>
<div class="inline-figure">
<img src="img/gmm_mclust_sigma.png">
</div>
</div>
<p><strong>7.</strong> Though GMM is often categorized as a clustering algorithm, fundamentally it is an algorithm for density estimation. That is to say, the result of a GMM fit to some data is technically not a clustering model, but a generative probabilistic model describing the distribution of the data. Density estimation plays an important role in applied statistical data analysis and theoretical research. A density estimate based on GMM can be obtained using the function <code>densityMclust()</code>. Apply it on Data2 and visualize the estimated densities (show an “image” and a “perspective” plot of the bivariate density estimate).</p>
</div>
<div id="em-on-1d" class="section level3 unnumbered">
<h3>EM on 1D<a class="anchor" aria-label="anchor" href="#em-on-1d"><i class="fas fa-link"></i></a>
</h3>
<p>In this part you must fit a GMM model on a one dimensional simulated data.</p>
<p><strong>8.</strong> Create a data table of 300 observations in which you have two columns:</p>
<ul>
<li>The first column contains generated data. Those data are generated from three Gaussian distributions with different parameters.</li>
<li>The second column corresponds to the groud truth (every observation was generated from which Gaussian).</li>
<li>Hint: functions you may need are <code><a href="https://rdrr.io/r/stats/Normal.html">rnorm()</a></code>, <code><a href="https://rdrr.io/r/base/rep.html">rep()</a></code>, <code><a href="https://rdrr.io/r/base/cbind.html">rbind()</a></code> or <code><a href="https://rdrr.io/r/base/cbind.html">cbind()</a></code>.</li>
<li>You must of course <strong>set a seed</strong> (your sutdent_pk).
An example of 9 generated values from three Gaussians is shown in the following table:</li>
</ul>
<center>
<table class="table table-sm">
<thead><tr>
<th style="text-align:right;">
X
</th>
<th style="text-align:right;">
source
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:right;">
-5.63
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
-4.82
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
-5.84
</td>
<td style="text-align:right;">
1
</td>
</tr>
<tr>
<td style="text-align:right;">
1.59
</td>
<td style="text-align:right;">
2
</td>
</tr>
<tr>
<td style="text-align:right;">
0.33
</td>
<td style="text-align:right;">
2
</td>
</tr>
<tr>
<td style="text-align:right;">
-0.82
</td>
<td style="text-align:right;">
2
</td>
</tr>
<tr>
<td style="text-align:right;">
5.49
</td>
<td style="text-align:right;">
3
</td>
</tr>
<tr>
<td style="text-align:right;">
5.74
</td>
<td style="text-align:right;">
3
</td>
</tr>
<tr>
<td style="text-align:right;">
5.58
</td>
<td style="text-align:right;">
3
</td>
</tr>
</tbody>
</table>
</center>
<p><strong>9.</strong> Show your generated data on one axe (this kind of figures are called stripchart), color them with respect to ground truth, you must obtain something like:</p>
<div class="inline-figure"><img src="Machine-Learning_files/figure-html/unnamed-chunk-267-1.png" width="70%" style="display: block; margin: auto;"></div>
<p><strong>10.</strong> Plot the histogram corresponding to your generated data. Interpret it.</p>
<p><strong>11.</strong> Fit a GMM model on your generated data. Print the summary and visualize your results. Explain your results.</p>
<p><strong>12.</strong> Apply a density estimate on your generated data and visualize it. Interpret the obtained figure.</p>
</div>
</div>
<div id="em-from-scratch" class="section level2" number="8.5">
<h2>
<span class="header-section-number">8.5</span> EM from scratch<a class="anchor" aria-label="anchor" href="#em-from-scratch"><i class="fas fa-link"></i></a>
</h2>
<p>In this second part of this PW you will build a GMM model from scratch, you must develop the EM technique to fit the model.</p>
<p><strong>2.1</strong> Generate a two-dimensional dataset from a <span class="math inline">\(k\)</span>-component Gaussian mixture density with different means and different covariance matrices. It is up to you to choose the mixing proportions <span class="math inline">\(\{\pi_1,\ldots,\pi_k\}\)</span>.</p>
<p><strong>2.2</strong> Implement the EM algorithm to fit a GMM on your generated data:</p>
<ul>
<li>Initialize the mixing proportions and the covariance matrices (e.g., you can initialize with equal mixing proportions and Identity covariance matrices).</li>
<li>Initialize the means “randomly” (by your own choice of <span class="math inline">\(k\)</span>).</li>
<li>In the EM training loop, store the value of the observed-data log-likelihood at each iteration.</li>
<li>At convergence, plot the log-likelihood curve.</li>
</ul>
<p><strong>2.3</strong> Create a function that selects the number of mixture components by computing the values of BIC criterion for <span class="math inline">\(k\)</span> varying from 1 to 10.</p>
<p><strong>2.4</strong> On your generated data, compare your results obtained with the algorithm you developed and the ground truth (in terms of the chosen number of mixture components; and in terms of error rate).</p>
<p><strong>2.5</strong> Apply the algorithm you developed on <a target="_blank" href="datasets/iris.data"> Iris <i class="fa fa-table" aria-hidden="true"></i></a> dataset.</p>
<div class="rmdtip">
<p>
To visualize your results on Iris dataset, you can use PCA projection
and coloring with respect to clustering results.
</p>
<p>
The package <code>mclust</code> provides also a dimensionality
reduction technique in function <code>MclustDR()</code>.
</p>
</div>
<p align="right">
◼
</p>

</div>
</div>



  <div class="chapter-nav">
<div class="prev"><a href="gaussian-mixture-models-em.html"><span class="header-section-number">8</span> Gaussian Mixture Models &amp; EM</a></div>
<div class="next"><a href="hackathon.html">Hackathon</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#pw-8">PW 8</a></li>
<li><a class="nav-link" href="#report-template">Report template</a></li>
<li>
<a class="nav-link" href="#em-using-mclust"><span class="header-section-number">8.4</span> EM using mclust</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#gmm-vs-k-means">GMM vs \(k\)-means</a></li>
<li><a class="nav-link" href="#em-on-1d">EM on 1D</a></li>
</ul>
</li>
<li><a class="nav-link" href="#em-from-scratch"><span class="header-section-number">8.5</span> EM from scratch</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Machine Learning</strong>" was written by Mohamad Ghassany. It was last built on 2022-04-26.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
