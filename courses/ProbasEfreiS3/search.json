[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Probabilit√©s",
    "section": "",
    "text": "Bienvenu\nCe cours de Probabilit√©s est destin√© aux √©tudiants en Licence 2 √† l‚ÄôEFREI Paris. Ce cours est r√©dig√© par Mohamad Ghassany, enseignant chercheur et responsable de la majeure Data & Artificial Intelligence.\nLes probabilit√©s permettent de mod√©liser des ph√©nom√®nes al√©atoires et d‚Äôy effectuer des calculs th√©oriques. La th√©orie des probabilit√©s concerne la mod√©lisation du hasard et le calcul des probabilit√©s, son √©valuation. Ces techniques doivent faire partie des connaissances de base de tous les ing√©nieurs, tr√®s souvent confront√©s √† des syst√®mes ou des situations de plus en plus complexes et parfois gouvern√©es en partie par le hasard. Elles jouent par exemple un r√¥le essentiel dans l‚Äô√©valuation de la s√ªret√© de fonctionnement des syst√®mes d‚Äôinformation et de communication. Le cours est divis√© en deux parties: une premi√®re portant sur les probabilit√©s et les variables al√©atoires discr√®tes; une deuxi√®me sur les variables al√©atoires continues. Les deux parties introduisent des lois usuelles de probabilit√©s."
  },
  {
    "objectID": "index.html#plan-du-cours-et-supports",
    "href": "index.html#plan-du-cours-et-supports",
    "title": "Probabilit√©s",
    "section": "Plan du cours et Supports",
    "text": "Plan du cours et Supports\n\n\n\nChapitre\nTitre\nDocument\nSlides [Fr]\nSlides [En]\nAnnotated Slides\nExercices [Fr]\nExercices [En]\n\n\n\n\n1\nVariables al√©atoires discr√®tes\nüìñ\nüìã\nüìã\nüìã‚úçÔ∏è\n\n\n\n\n2\nLois ususelles discr√®tes\nüìñ\nüìã\nüìã\n\n\n\n\n\n\nFeuille d‚Äôexercices 1\n\n\n\n\n‚úçÔ∏è\n‚úçÔ∏è\n\n\n3\nVariables al√©atoires continues\nüìñ\nüìã\nüìã\nüìã‚úçÔ∏è\n\n\n\n\n4\nLois usuelles continues\nüìñ\nüìã\nüìã\n\n\n\n\n\n\nFeuille d‚Äôexercices 2\n\n\n\n\n‚úçÔ∏è\n‚úçÔ∏è"
  },
  {
    "objectID": "variables-aleatoires-discretes.html#rappel-probabilit√©s",
    "href": "variables-aleatoires-discretes.html#rappel-probabilit√©s",
    "title": "\n1¬† Variables Al√©atoires Discr√®tes\n",
    "section": "Rappel probabilit√©s",
    "text": "Rappel probabilit√©s\nEspace Probabilisable\nExemple fondamental: Consid√©rons le jeu du lanc√© d‚Äôun d√©.\n\nExp√©rience al√©atoire \\(\\varepsilon\\) : ‚Äúlancer un d√© √©quilibr√©‚Äù.\nUnivers: l‚Äôensemble de tous les r√©sultats possibles de cette exp√©rience al√©atoire \\(\\Omega= \\{1,2,3,4,5,6\\}\\)\nEv√©nements: Dans cette exp√©rience al√©atoire, on peut s‚Äôint√©resser √† des √©v√©nements plus complexes qu‚Äôun simple r√©sultat √©l√©mentaire.\nL‚Äôensemble de parties de \\(\\Omega\\), appel√© \\(\\mathcal{P}(\\Omega)\\), est l‚Äôensemble des sous-ensembles de \\(\\Omega\\).\nUne famille \\(\\mathcal{A}\\) de parties (i.e.¬†de sous ensembles) de \\(\\Omega\\). Ces parties sont appel√©es des √©v√©nements. On dit que l‚Äô√©v√©nement \\(A\\) s‚Äôest r√©alis√© si et seulement si le r√©sultat \\(\\omega\\) de \\(\\Omega\\) qui s‚Äôest produit appartient √† \\(A\\).\n\nTribu: On appelle tribu sur \\(\\Omega\\), toute famille \\(\\mathcal{A}\\) de parties de \\(\\Omega\\) v√©rifiant:\n\n\\(\\Omega \\in \\mathcal{A}\\).\nsi \\(A \\in \\mathcal{A}\\), alors \\(\\bar{A} \\in \\mathcal{A}\\).\nsi \\((A_n)_{n\\in\\mathbb{N}}\\) est une suite d‚Äô√©l√©ments de \\(\\mathcal{A}\\), alors \\(\\bigcup\\limits_{n\\in\\mathbb{N}} A_n \\in \\mathcal{A}\\).\n\n\n\\((\\Omega,\\mathcal{A})\\) est un espace probibilisable.\nNotions sur les Ev√©nements\n\n\nSoit \\((\\Omega,\\mathcal{A})\\) un espace probibilisable:\n\nL‚Äôensemble \\(\\mathcal{A}\\) est appel√© tribu des √©v√©nements. Les √©l√©ments de \\(\\mathcal{A}\\) s‚Äôappellent les √©v√©nements.\nL‚Äô√©v√©nement \\(\\Omega\\) est appel√© √©v√©nement certain. L‚Äô√©v√©nement \\(\\emptyset\\) est appel√© √©v√©nement impossible.\n\n\n\nOp√©rations sur les √©v√©nements: Soient \\(A\\) et \\(B\\) deux √©v√©nements:\n\n\n\\(\\bar{A}\\) est l‚Äô√©v√©nement contraire de \\(A\\) (on note aussi \\(A^c\\)). \\(\\bar{A}=\\Omega\\setminus A\\).\n\\(\\bar{A}\\) se r√©alise si et seulement si \\(A\\) ne se r√©alise pas.\n\n\n\\(A\\, {\\color{blue}\\cap} \\,B\\) est l‚Äô√©v√©nement \\(A\\) et \\(B\\).\n\\(A\\, {\\color{blue}\\cap} \\,B\\) se r√©alise lorsque les deux √©v√©nements se r√©alisent.\n\n\n\\(A\\, {\\color{blue}\\cup} \\,B\\) est l‚Äô√©v√©nement \\(A\\) ou \\(B\\).\n\\(A\\, {\\color{blue}\\cup} \\,B\\) se r√©alise lorsque au moins un des deux √©v√©nements se r√©alise.\n\n\n\nIncompatibilit√©: \\(A\\) et \\(B\\) sont incompatibles si leur r√©alisation simultan√©e est impossible: \\(A \\cap B = \\emptyset\\).\nImplication: \\(A\\) implique \\(B\\) signifie que si \\(A\\) se r√©alise, alors \\(B\\) se r√©alise aussi: \\(A \\subset B\\).\nEspace Probabilis√©\n\n\nSoit \\((\\Omega,\\mathcal{A})\\) un espace probabilisable. On appelle probabilit√© sur \\((\\Omega,\\mathcal{A})\\), toute application \\[P : \\mathcal{A} \\rightarrow \\mathbb{R}\\] v√©rifiant:\n\n\\(\\forall A \\in \\mathcal{A}, P(A) \\geq 0\\).\n\\(P(\\Omega)=1\\).\n\\(\\forall (A_n)_{n\\in\\mathbb{N}^*} \\in \\mathcal{A}^{\\mathbb{N}^*}\\), une suite d‚Äô√©l√©ments de \\(\\mathcal{A}\\) deux √† deux incompatibles, on a: \\[P(\\bigcup\\limits_{n\\in\\mathbb{N}^*} A_n) = \\sum_{n=1}^{+\\infty} P(A_n)\\]\n\n\nLe triplet \\((\\Omega,\\mathcal{A},P)\\) est appel√© espace probabilis√©.\nProbabilit√©: Propri√©t√©s\n\n\\(P(\\emptyset) = 0\\).\n\\(P(A_1 \\cup A_2 ) = P(A_1 ) + P(A_2 )-P(A_1 \\cap A_2 )\\).\nSi \\(A_1\\) et \\(A_2\\) sont incompatibles, \\(A_1 \\cap A_2 = \\emptyset\\), \\(P(A_1 \\cup A_2 ) = P(A_1 ) + P(A_2 )\\).\n\\(\\begin{align} P(A_1 \\cup A_2 \\cup A_3 ) &= P(A_1 ) + P(A_2 ) + P(A_3 ) - P(A_1 \\cap A_2 ) \\\\  &- P(A_1 \\cap A_3 ) - P(A_2 \\cap A_3 )+P(A_1 \\cap A_2 \\cap A_3 )\\end{align}\\).\n\\(P(\\bar{A}) = 1-P(A)\\).\n\\(P(B\\setminus A)=P(B)-P(B\\cap A)\\).\n\\(A \\subset B \\Rightarrow P(A) \\leq P(B)\\).\n\n\n\n\n\n\n\nNote\n\n\n\nProbabilit√© uniforme sur \\(\\Omega\\) fini\n\nSoit \\(\\Omega\\) un univers fini. On dit que \\(P\\) est la probabilit√© uniforme sur l‚Äôespace probabilisable \\((\\Omega,P(\\Omega))\\) si: \\[\\forall \\omega,\\omega' \\in \\Omega, \\quad \\quad P(\\{\\omega\\})=P(\\{\\omega'\\})\\] On dit aussi qu‚Äôil y a √©quiprobabilit√© des √©v√©nements √©l√©mentaires.\nSoit \\((\\Omega, \\mathcal{P}(\\Omega), P)\\) un espace probabilis√© fini. Si \\(P\\) est la probabilit√© uniforme, alors \\[\\forall A \\in \\mathcal{A}, \\quad \\quad P(A)=\\frac{Card(A)}{Card(\\Omega)}\\]\n\n\n\nProbabilit√© conditionnelle\n\nSoit \\((\\Omega,\\mathcal{A},P)\\) une espace probabilis√© et \\(B \\in \\mathcal{A}\\) tel que \\(P(B) > 0\\). L‚Äôapplication \\(P_B\\) d√©finie sur \\(\\mathcal{A}\\) par: \\[P_B(A) = P(A|B) =\\frac{P(A\\cap B)}{P(B)}, \\quad \\quad \\forall A \\in \\mathcal{A}\\] est une probabilit√© sur \\((\\Omega, \\mathcal{A})\\); elle est appel√©e la probabilit√© conditionnelle sachant \\(B\\). C‚Äôest la probabilit√© pour que l‚Äô√©v√©nement \\(A\\) se produise sachant que l‚Äô√©v√©nement \\(B\\) s‚Äôest produit.\nRemarque: \\((A|B)\\) n‚Äôest pas un √©v√©nement! On utilise la notation \\(P(A|B)\\) par simplicit√©, mais c‚Äôest \\(P_B (A)\\) qui est correcte.\nFormule des probabilit√©s compos√©es: \\[P(A\\cap B) = P(A|B)P(B) = P(B|A)P(A)\\]\n\nFormule des probabilit√©s totales:\n\n\\(\\forall A \\in \\mathcal{A}, \\quad P(A) = P(A \\cap B) + P(A \\cap \\bar{B} )\\)\nOn appelle syst√®me complet d‚Äô√©v√©nements (SCE), toute partition d√©nombrable de \\(\\Omega\\) form√©e d‚Äô√©l√©ments de \\(A\\); c-√†-d tout ensemble d√©nombrable d‚Äô√©v√©nements, deux √† deux incompatibles et dont l‚Äôunion d√©nombrable est l‚Äô√©v√©nement certain.\nSoit \\((B_n)_{n\\geq 0}\\) un SCE de \\(\\Omega\\). On a: \\[\\forall A \\in \\mathcal{A},\\quad \\quad P(A)=\\sum_{n\\geq 0} P(A \\cap B_n)\\]\n\n\nInd√©pendance: Les √©v√©nements \\(A\\) et \\(B\\) sont ind√©pendants ssi \\(P(A\\cap B)=P(A)P(B)\\).\nFormule de Bayes\nPremi√®re formule de Bayes:\nSoit \\((\\Omega,\\mathcal{A},P)\\) une espace probabilis√©. Pour tous √©v√©nements \\(A\\) et \\(B\\) tels que \\(P(A) \\neq 0\\) et \\(P(B) \\neq 0\\), on a: \\[P(B|A) = \\frac{P(A|B)P(B)}{P(A)}\\]\nDeuxi√®me formule de Bayes:\nSoit \\((\\Omega,\\mathcal{A},P)\\) une espace probabilis√© et \\((B_n)_{n\\geq 0}\\) un SCE de \\(\\Omega\\) t.q. pour tout \\(n\\geq 0 \\,\\, P(B_n)\\neq 0\\). On a pour tout \\(A \\in \\mathcal{A}\\) t.q. \\(P(A)\\neq 0\\) \\[P(B_i|A) = \\frac{P(A|B_i) P(B_i)}{\\sum_{n\\geq 0} P(A|B_n) P(B_n)} \\quad \\quad \\forall i \\geq 0\\]"
  },
  {
    "objectID": "variables-aleatoires-discretes.html#notion-de-variable-al√©atoire-r√©elle-v.a.r.",
    "href": "variables-aleatoires-discretes.html#notion-de-variable-al√©atoire-r√©elle-v.a.r.",
    "title": "\n1¬† Variables Al√©atoires Discr√®tes\n",
    "section": "Notion de variable al√©atoire r√©elle (v.a.r.)",
    "text": "Notion de variable al√©atoire r√©elle (v.a.r.)\nApr√®s avoir r√©alis√© une exp√©rience al√©atoire, il arrive bien souvent qu‚Äôon s‚Äôint√©resse plus √† une fonction du r√©sultat qu‚Äôau r√©sultat lui-m√™me. Expliquons ceci au moyen des exemples suivants: lorsqu‚Äôon joue au d√©s, certains jeux accordent de l‚Äôimportance √† la somme obtenue sur deux d√©s, 7 par exemple, plut√¥t qu‚Äô√† la question de savoir si c‚Äôest la paire (1,6) qui est apparue, ou (2,5), (3,4), (4,3), (5,2) ou plut√¥t (6,1). Dans le cas du jet d‚Äôune pi√®ce, il peut √™tre plus int√©ressant de conna√Ætre le nombre de fois o√π le c√¥t√© pile est apparue plut√¥t que la s√©quence d√©taill√©e des jets pile et face. Ces grandeurs auxquelles on s‚Äôint√©resse sont en fait des fonctions r√©elles d√©finies sur l‚Äôensemble fondamental et sont appel√©es variables al√©atoires.\nDu fait que la valeur d‚Äôune variable al√©atoire est d√©termin√©e par le r√©sultat de l‚Äôexp√©rience, il est possible d‚Äôattribuer une probabilit√© aux diff√©rentes valeurs que la variable al√©atoire peut prendre.\nSoient \\(\\varepsilon\\) une exp√©rience al√©atoire et \\((\\Omega,\\mathcal{A},P)\\) un espace probabilis√© li√© √† cette exp√©rience. Dans de nombreuses situations, on associe √† chaque r√©sultat \\(\\omega \\in \\Omega\\) un nombre r√©el not√© \\(X(\\omega)\\); on construit ainsi une application \\(X : \\Omega \\rightarrow \\mathbb{R}\\). Historiquement, \\(\\varepsilon\\) √©tait un jeu et \\(X\\) repr√©sentait le gain du joueur.\n\n\n\n\n\n\n\n\n\nExemple: Un joueur lance un d√© √©quilibr√© √† 6 faces num√©rot√©es de 1 √† 6, et on observe le num√©ro obtenu.\n\nSi le joueur obtient 1, 3 ou 5, il gagne 1 euro.\nS‚Äôil obtient 2 ou 4, il gagne 5 euros.\nS‚Äôil obtient 6, il perd 10 euros.\n\nSelon l‚Äôexp√©rience al√©atoire (lancer d‚Äôun d√© √©quilibr√©) l‚Äôensemble fondamental est \\(\\Omega = \\{1,2,3,4,5,6\\}\\), \\(\\mathcal{A} = \\mathcal{P}(\\Omega)\\) et \\(P\\) l‚Äô√©quiprobabilit√© sur \\((\\Omega,\\mathcal{A})\\). Soit \\(X\\) l‚Äôapplication de \\(\\Omega\\) dans \\(\\mathbb{R}\\) qui √† tout \\(\\omega \\in \\Omega\\) associe le gain correspondant. On a donc\n\n\\(X(1) = X(3) = X(5) = 1\\)\n\\(X(2) = X(4) = 5\\)\n\\(X(6) = -10\\)\n\nOn dit que \\(X\\) est une variable al√©atoire sur \\(\\Omega\\).\nOn peut s‚Äôint√©resser √† la probabilit√© de gagner 1 euro, c‚Äôest-√†-dire d‚Äôavoir \\(X(\\omega) = 1\\), ce qui se r√©alise si et seulement si \\(\\omega \\in \\{1,3,5\\}\\). La probabilit√© cherch√©e est donc √©gale √† \\(P(\\{1,3,5\\}) = 1/2\\). On √©crira aussi \\(P(X=1) = 1/2\\).\nOn pourra donc consid√©rer l‚Äô√©v√©nement : \\[\n\\begin{align}\n\\{X=1\\} &= \\{\\omega \\in \\Omega / X(\\omega) = 1\\}  \\\\\n&= \\{\\omega \\in \\Omega / X(\\omega) \\in \\{1\\}\\} \\\\\n&= X^{-1} (\\{1\\}) \\\\\n&= \\{1,3,5\\}\n\\end{align}\\]\nOn aura du m√™me \\(P(X=5) = 1/3\\) et \\(P(X=-10) = 1/6\\). Ce que l‚Äôon peut pr√©senter dans un tableau\n\n\n\\(x_i\\)\n-10\n1\n5\n\n\n\\(p_i=P(X = x_i)\\)\n\\(1/6\\)\n\\(1/2\\)\n\\(1/3\\)\n\n\nCela revient √† consid√©rer un nouvel ensemble d‚Äô√©v√©nements √©l√©mentaires: \\[\\Omega_X = X(\\Omega)= \\{-10,1,5\\}\\] et √† munir cet ensemble de la probabilit√© \\(P_X\\) d√©finie par le tableau des \\(P(X=x_i)\\) ci dessus. Cette nouvelle probabilit√© s‚Äôappelle loi de la variable al√©atoire X.\nRemarquer que \\[P(\\bigcup_{x_i \\in \\Omega_X} \\{X=x_i\\}) = \\sum_{x_i \\in \\Omega_X} P(X=x_i) = 1\\]\nDans ce chapitre, nous traitons le cas o√π \\(X(\\Omega)\\) est d√©nombrable. La variable al√©atoire est alors dite discr√®te. Sa loi de probabilit√©, qui peut √™tre toujours d√©finie par sa fonction de r√©partition, le sera plut√¥t par les probabilit√©s individuelles. Nous d√©finirons les deux caract√©ristiques num√©riques principales d‚Äôune variable al√©atoire discr√®te, l‚Äôesp√©rance caract√©ristique de valeur centrale, et la variance, caract√©ristique de dispersion. Nous d√©finirons aussi les couples de variables al√©atoires."
  },
  {
    "objectID": "variables-aleatoires-discretes.html#d√©finition-loi-de-probabilit√©",
    "href": "variables-aleatoires-discretes.html#d√©finition-loi-de-probabilit√©",
    "title": "\n1¬† Variables Al√©atoires Discr√®tes\n",
    "section": "D√©finition, loi de probabilit√©",
    "text": "D√©finition, loi de probabilit√©\n\nD√©finition 1.1 On dit qu‚Äôune variable al√©atoire r√©elle (v.a.r.) \\(X\\) est discr√®te (v.a.r.d.) si l‚Äôensemble des valeurs que prend \\(X\\) est fini ou infini d√©nombrable.\nSi on suppose \\(X(\\Omega)\\) l‚Äôensemble des valeurs de \\(X\\) qui admet un plus petit √©l√©ment \\(x_1\\). Alors la v.a.r.d. \\(X\\) est enti√®rement d√©finie par:\n\nL‚Äôensemble \\(X(\\Omega)\\) des valeurs prises par \\(X\\), rang√©es par ordre croissant: \\(X(\\Omega) = \\{x_1, x_2,\\ldots,x_i,\\ldots\\}\\) avec \\(x_1 \\leq x_2 \\leq \\ldots \\leq x_i \\leq \\ldots\\).\nLa loi de probabilit√© d√©finie sur \\(X(\\Omega)\\) par \\[p_i = P(X=x_i) \\,\\,\\,\\,\\, \\forall \\,\\, i=1,2,\\ldots\\]\n\n\nRemarques:\n\nSoit \\(B\\) un ensemble de \\(\\mathbb{R}\\), \\[P(X \\in B) = \\sum_{i / x_i \\in B} p(x_i)\\]\nEn particulier \\[P( a < X \\leq b) =  \\sum_{i / a < x_i \\leq b} p(x_i)\\]\nBien s√ªr tous les \\(p(x_i)\\) sont positives et \\(\\sum_{i=1}^{\\infty} p(x_i) =1\\).\nSi \\(X\\) ne prend qu‚Äôun petit nombre de valeurs, cette loi est g√©n√©ralement pr√©sent√©e dans un tableau."
  },
  {
    "objectID": "variables-aleatoires-discretes.html#fonction-de-r√©partition-dune-variable-al√©atoire-discr√®te",
    "href": "variables-aleatoires-discretes.html#fonction-de-r√©partition-dune-variable-al√©atoire-discr√®te",
    "title": "\n1¬† Variables Al√©atoires Discr√®tes\n",
    "section": "Fonction de r√©partition d‚Äôune variable al√©atoire discr√®te",
    "text": "Fonction de r√©partition d‚Äôune variable al√©atoire discr√®te\n\nD√©finition 1.2 On appelle fonction de r√©partition de la v.a. \\(X\\), qu‚Äôon note \\(F(a)\\) de la v.a.r.d. \\(X\\), ou \\(F_X(a)\\), la fonction d√©finie pour tout r√©el \\(a\\), \\(-\\infty < a < \\infty\\), par\n\\[F(a)=P(X \\leq a)=\\sum_{i / x_{i}\\leq a} P(X=x_{i})\\]\n\nCette valeur repr√©sente la probabilit√© de toutes les r√©alisations inf√©rieures ou √©gales au r√©el \\(a\\).\nPropri√©t√©s: Voici quelques propri√©t√©s de cette fonction:\n\nC‚Äôest une fonction en escalier (constante par morceaux).\n\\(F(a) \\leq 1\\) car c‚Äôest une probabilit√©.\n\\(F(a)\\) est continue √† droite.\n\\(\\lim\\limits_{a\\to - \\infty} F(a) = 0\\) et \\(\\lim\\limits_{a\\to\\infty} F(a) = 1\\)\n\nLa fonction de r√©partition caract√©rise la loi de \\(X\\), autrement dit: \\(F_{X} = F_{Y}\\) si et seulement si les variables al√©atoires \\(X\\) et \\(Y\\) ont la m√™me loi de probabilit√©."
  },
  {
    "objectID": "variables-aleatoires-discretes.html#fonction-de-r√©partition-et-probabilit√©s-sur-x",
    "href": "variables-aleatoires-discretes.html#fonction-de-r√©partition-et-probabilit√©s-sur-x",
    "title": "\n1¬† Variables Al√©atoires Discr√®tes\n",
    "section": "Fonction de r√©partition et probabilit√©s sur \\(X\\)\n",
    "text": "Fonction de r√©partition et probabilit√©s sur \\(X\\)\n\nTous les calculs de probabilit√© concernant \\(X\\) peuvent √™tre trait√©s en termes de fonction de r√©partition. Par exemple,\n\\[P(a < X \\leq b) = F(b) - F(a) \\quad \\quad \\text{pour tout } a < b\\]\nOn peut mieux s‚Äôen rendre compte en √©crivant \\(\\{X \\leq b\\}\\) comme union des deux √©v√©nements incompatibles \\(\\{X \\leq a\\}\\) et \\(\\{ a < X \\leq b\\}\\), soit\n\\[\\{X \\leq b\\} = \\{X \\leq a\\} \\cup   \\{ a < X \\leq b\\}\\]\net ainsi\n\\[P(X \\leq b) = P(X \\leq a) + P(a < X \\leq b)\\] ce qui √©tablit l‚Äô√©galit√© ci dessus.\n\n\n\n\n\n\nAstuce\n\n\n\nOn peut d√©duire de \\(F\\) les probabilit√©s individuelles par:\n\\[ p_{i}=F(x_{i})-F(x_{i-1})\\quad \\quad \\text{pour  } 1 \\leq i \\leq n \\]\n\n\nExemple: On joue trois fois √† pile ou face. Soit \\(X\\) la variable al√©atoire ‚Äúnombre de pile obtenus‚Äù. Ici \\(\\Omega=\\{P, F\\}^3\\), et donc \\[X(\\Omega)=\\{0, 1, 2, 3\\}\\]\nOn a \\(card(\\Omega)=2^3=8\\). Calculons par exemple \\(P(X=1)\\), c‚Äôest √† dire la probabilit√© d‚Äôavoir exactement une pile. \\[X^{-1}(1)=\\{(P, F, F), (F, P, F), (F, F, P) \\}\\] D‚Äôo√π \\(P(X=1)=\\displaystyle \\frac{3}{8}\\).\nEn proc√©dant de la m√™me fa√ßon, on obtient la loi de probabilit√© de \\(X\\):\n\n\n\n\n\n\n\n\n\n\\(k\\)\n0\n1\n2\n3\n\n\n\\(P(X = k)\\)\n\\(\\displaystyle \\frac{1}{8}\\)\n\\(\\displaystyle \\frac{3}{8}\\)\n\\(\\displaystyle \\frac{3}{8}\\)\n\\(\\displaystyle \\frac{1}{8}\\)\n\n\nLa fonction de r√©partition de \\(X\\) est donc donn√©e par:\n\\[F(x) = \\left\\{\n\\begin{array}{l l}\n0 & \\quad \\text{si $x<0$}\\\\\n  1/8 & \\quad \\text{si $0 \\leq x < 1$}\\\\\n   1/2 & \\quad \\text{si $1 \\leq x < 2$}\\\\\n    7/8 & \\quad \\text{si $2 \\leq x < 3$}\\\\\n     1 & \\quad \\text{si $x \\geq 3$}\\\\\n\\end{array} \\right.\\]\nLe graphe de cette derni√®re est repr√©sent√©e dans la figure suivante:\n\n\n\n\n\nExemple: Soit \\(A\\) un √©v√©nement quelconque. On appelle variable al√©atoire indicatrice de cet √©v√©nement \\(A\\), la variable al√©atoire d√©finie par: \\[X(\\omega) = \\left\\{\n\\begin{array}{l l}\n1 & \\quad \\text{si $\\omega \\in A$}\\\\\n0 & \\quad \\text{si $\\omega \\in \\bar{A}$}\\\\   \n  \\end{array} \\right.\\]\net not√©e \\(X=1_A\\). Ainsi: \\[P(X=1)=P(A)=p\\] \\[P(X=0)=P(\\bar{A})=1-p\\]\nLa fonction de r√©partition de \\(X\\) est donc donn√©e par:\n\\[ \\begin{equation}\nF(x) = \\left\\{\n\\begin{array}{l l}\n0 & \\quad \\text{si $x<0$}\\\\\n  1-p & \\quad \\text{si $0 \\leq x < 1$}\\\\\n   1 & \\quad \\text{si $x \\geq 1$}\\\\\n\\end{array} \\right.\n\\end{equation} \\]\nOn peut prendre par exemple le cas d‚Äôun tirage d‚Äôune boule dans une urne contenant 2 boules blanches et 3 boules noires. Soit \\(A\\):‚Äúobtenir une boule blanche‚Äù et \\(X\\) la variable indicatrice de \\(A\\). La loi de probabilit√© de \\(X\\) est alors\n\n\n\\(k\\)\n0\n1\n\n\n\\(P(X = k)\\)\n\\(\\frac{3}{5}\\)\n\\(\\frac{2}{5}\\)\n\n\net sa fonction de r√©partition est:\n\\[ \\begin{equation}\nF(x) = \\left\\{\n\\begin{array}{l l}\n0 & \\quad \\text{si $x<0$}\\\\\n  3/5 & \\quad \\text{si $0 \\leq x < 1$}\\\\\n   1 & \\quad \\text{si $x \\geq 1$}\\\\\n\\end{array} \\right.\n\\end{equation} \\]"
  },
  {
    "objectID": "variables-aleatoires-discretes.html#moments-dune-variable-al√©atoire-discr√®te",
    "href": "variables-aleatoires-discretes.html#moments-dune-variable-al√©atoire-discr√®te",
    "title": "\n1¬† Variables Al√©atoires Discr√®tes\n",
    "section": "Moments d‚Äôune variable al√©atoire discr√®te",
    "text": "Moments d‚Äôune variable al√©atoire discr√®te\nEsp√©rance math√©matique\n\nD√©finition 1.3 Pour une variable al√©atoire discr√®te \\(X\\) de loi de probabilit√© \\(p(.)\\), on d√©finit l‚Äôesp√©rance de \\(X\\), not√©e \\(E(X)\\), par l‚Äôexpression\n\\[E(X)=\\sum_{i \\in \\mathbb{N}} x_{i} p(x_i)\\]\nEn termes concrets, l‚Äôesp√©rance de \\(X\\) est la moyenne pond√©r√©e des valeurs que \\(X\\) peut prendre, les poids √©tant les probabilit√©s que ces valeurs soient prises.\n\nReprenons l‚Äôexemple o√π on joue 3 fois √† pile ou face. L‚Äôesp√©rance de \\(X=\\)‚Äúnombre de pile obtenus‚Äù est √©gal √†: \\[E(X)=0 \\times \\frac{1}{8}+1 \\times \\frac{3}{8}+2 \\times \\frac{3}{8}+3 \\times \\frac{1}{8}=1.5\\]\nDans le cas de la loi uniforme sur \\(X(\\Omega)=\\{x_{1},\\ldots, x_{k}\\}\\), c‚Äôest √† dire avec √©quiprobabilit√© de toutes les valeurs \\(p_{i}=1/k\\), on obtient: \\[E(X)=\\frac{1}{k} \\sum_{i=1}^k x_{i}\\] et dans ce cas \\(E(X)\\) se confond avec la moyenne arithm√©tique simple \\(\\bar{x}\\) des valeurs possibles de \\(X\\).\nPour le jet d‚Äôun d√© √©quilibr√© par exemple: \\[E(X)=\\frac{1}{6} \\sum_{i=1}^6 i=\\frac{7}{2}=3.5\\]\nEsp√©rance d‚Äôune fonction d‚Äôune variable al√©atoire\n\nTh√©or√®me 1.1 (Th√©or√®me du transfert) Si X est une variable al√©atoire discr√®te pouvant prendre ses valeurs parmi les valeurs \\(x_i\\), \\(i \\geq 1\\), avec des probabilit√©s respectives \\(p(x_i)\\), alors pour toute fonction r√©elle \\(g\\) on a\n\\[E(g(X)) = \\sum_i g(x_i)p(x_i)\\]\n\n\nExemple 1.1 Soit \\(X\\) une variable al√©atoire qui prend une des trois valeurs \\(\\{-1,0,1\\}\\) avec les probabilit√©s respectives\n\\[P(X=-1) = 0.2 \\quad \\quad P(X=0)=0.5 \\quad \\quad P(X=1) = 0.3\\]\nCalculer \\(E(X^2)\\).\n\nPremi√®re approche: Soit \\(Y=X^2\\). La distribution de \\(Y\\) est donn√©e par\n\\[\n\\begin{align}\nP(Y=1) &= P(X=-1) + P(X=1) = 0.5 \\\\\nP(Y=0) &= P(X=0) = 0.5\n\\end{align}\\]\nDonc \\(E(X^2)=E(Y) = 1(0.5) + 0(0.5) = 0.5\\)\nDeuxi√®me approche: En utilisant le th√©or√®me\n\\[\n\\begin{align}\nE(X^2)  &= (-1)^2(0.2) + 0^2(0.5) + 1^2 (0.3) \\\\\n        &= 1(0.2+0.3)+0(0.5)=0.5\n\\end{align}\\]\nRemarquer que \\(0.5=E(X^2) \\neq (E(X))^2 = 0.01\\)\nLin√©arit√© de l‚Äôesp√©rance Propri√©t√©s de l‚Äôesp√©rance\n\n\n\\(E(X+a)=E(X)+a, \\quad a \\in \\mathbb{R}\\). Un r√©sultat qui se d√©duit de:\n\\[\n\\begin{align}\n\\sum_{i}p_{i}(x_{i}+a) &= \\sum_{i}p_{i}x_{i}+\\sum_{i}ap_{i} \\\\\n& =\\sum_{i}p_{i}x_{i}+a \\sum_{i}p_{i}\\\\\n&=\\sum_{i}p_{i}x_{i}+a\n\\end{align}\\]\n\n\\(E(aX)=aE(X), \\quad a\\in \\mathbb{R}\\)\nil suffit d‚Äô√©crire: \\[\\sum_{i}p_{i}a x_{i}=a\\sum_{i}p_{i}x_{i}\\]\n\\(E(X+Y)=E(X)+E(Y)\\), \\(X\\) et \\(Y\\) √©tant deux variables al√©atoire.\n\nOn peut r√©sumer ces trois propri√©t√©s en disant que l‚Äôesp√©rance math√©matique est lin√©aire: \\[E(\\lambda X + \\mu Y)= \\lambda E(X)+\\mu E(Y), \\quad \\forall \\lambda \\in \\mathbb{R}, \\, \\forall \\mu \\in \\mathbb{R}\\]\nVariance\n\nD√©finition 1.4 La variance est un indicateur mesurant la dispersion des valeurs \\(x_{i}\\) que peut prendre la v.a. \\(X\\) et son esp√©rance \\(E(X)\\). On appelle variance de X, que l‚Äôon note \\(V(X)\\), la quantit√©\n\\[V(X)=E\\big[ (X-E(X))^2 \\big]\\] lorsque cette quantit√© existe.\nC‚Äôest l‚Äôesp√©rance math√©matique du carr√© de la v.a. centr√©e \\(X-E(X)\\).\n\nOn peut √©tablir une autre formule pour le calcul de \\(V(X)\\):\n\\[V(X)=E(X^2)-E^2(X)\\]\nOr: \\[\\begin{aligned}\n      V(X)&= E\\left[X^2-2XE(X)+E^2(X)\\right] \\\\\n           &=E(X^2)-E[2XE(X)]+ E[E^2(X)]\\\\\n           &=E(X^2)-2E^2(X)+E^2(X) \\\\\n           &=E(X^2)-E^2(X)\n    \\end{aligned}\\]\nOn cherche \\(V(X)\\) o√π \\(X\\) est le nombre obtenu lors du jet d‚Äôun d√© √©quilibr√©. On a vu dans l‚Äôexemple que \\(E(X) = \\frac{7}{2}\\). De plus,\n\\[\\begin{aligned}\n  E(X^2) &= 1^2 \\bigg(\\frac{1}{6}\\bigg) + 2^2 \\bigg(\\frac{1}{6}\\bigg) + 3^2 \\bigg(\\frac{1}{6}\\bigg) + 4^2 \\bigg(\\frac{1}{6}\\bigg) + 5^2 \\bigg(\\frac{1}{6}\\bigg) + 6^2 \\bigg(\\frac{1}{6}\\bigg) \\\\\n        &=\\bigg(\\frac{1}{6}\\bigg) (91) = \\frac{91}{6}.\\end{aligned}\\] Et donc\n\\[V(X) = \\frac{91}{6} - \\bigg(\\frac{7}{2}\\bigg)^2 = \\frac{35}{12}\\]\nPropri√©t√©s de la variance\n\n\\(V(X) \\geq 0\\)\n\\(V(X+a)=V(X)\\)\nen effet: \\[\\begin{aligned}\n   V(X+a)   &= E\\big[\\left[X+a-E(X+a)\\right]^2\\big] \\\\\n        &=E\\big[\\left[X+a-E(X)-a\\right]^2\\big] \\\\\n        &=E\\big[\\left[X-E(X)\\right]^2\\big] \\\\\n        &=V(X).\n   \\end{aligned}\\]\n\\(V(aX)=a^2V(X)\\)\nen effet: \\[\\begin{aligned}\n   V(aX)  &= E\\big[\\left[aX-E(aX)\\right]^2\\big] \\\\\n      &=E\\big[\\left[aX-aE(X)\\right]^2\\big] \\\\\n      &=E\\big[a^2\\left[X-E(X)\\right]^2\\big] \\\\\n      &=a^2\\big[E\\left[X-E(X)\\right]^2\\big] \\\\\n      &= a^2V(X).\n   \\end{aligned}\\]\nEcart-type\n\nD√©finition 1.5 La racine carr√©e de \\(V(X)\\) est appel√©e l‚Äô√©cart-type de \\(X\\), qui se note \\(\\sigma_{X}\\). On a\n\\[\\sigma_{X} = \\sqrt{V(X)}\\]\n\\(\\sigma_{X}\\) s‚Äôexprime dans les m√™mes unit√©s de mesure que la variable al√©atoire \\(X\\).\n\nA noter:\n\nL‚Äô√©cart type sert √† mesurer la dispersion d‚Äôun ensemble de donn√©es.\nPlus il est faible, plus les valeurs sont regroup√©es autour de la moyenne.\nExemple: La r√©partition des notes d‚Äôune classe. Plus l‚Äô√©cart type est faible, plus la classe est homog√®ne.\nL‚Äôesp√©rance et l‚Äô√©cart-type sont reli√©s par l‚Äôin√©galit√© de Bienaym√©-Tchebychev.\nIn√©galit√© de Bienaym√©-Tchebychev\n\nTh√©or√®me 1.2 Soit \\(X\\) une variable al√©atoire d‚Äôesp√©rance \\(\\mu\\) et de variance \\(\\sigma^2\\). Pour tout \\(\\varepsilon > 0\\), on a l‚Äôin√©galit√© suivante: \\[P\\left(|X-E(X)| \\geq \\varepsilon \\right) \\leq \\frac{\\sigma^2}{\\varepsilon^2}\\]\nOn peut l‚Äô√©crire autrement. Soit \\(k=\\varepsilon/\\sigma\\). \\[P\\left(|X-E(X)| \\geq k\\sigma \\right) \\leq \\frac{1}{k^2}\\]\n\n\n\n\n\n\n\nNote\n\n\n\nImportance: Cette in√©galit√© relie la probabilit√© pour \\(X\\) de s‚Äô√©carter de sa moyenne \\(E(X)\\), √† sa variance qui est justement un indicateur de dispersion autour de la moyenne de la loi. Elle montre quantitativement que ‚Äúplus l‚Äô√©cart type est faible, plus la probabilit√© de s‚Äô√©carter de la moyenne est faible‚Äù.\n\n\n\nTh√©or√®me 1.3 (In√©galit√© de Markov) Soit \\(X\\) une variable al√©atoire √† valeur non n√©gatives. Pour tout r√©el \\(a > 0\\) \\[P(X>a) \\leq \\frac{E(X)}{a}\\]\n\nMoments non centr√©s et centr√©s\nOn appelle moment non centr√© d‚Äôordre \\(r \\in \\mathbb{N^*}\\) de \\(X\\) la quantit√©, lorsqu‚Äôelle existe: \\[m_{r}(X)=\\sum_{i \\in \\mathbb{N} } x_{i}^r p(x_{i})=E(X^r)\\] Le moment centr√© d‚Äôordre \\(r \\in \\mathbb{N^*}\\) est la quantit√©, lorsqu‚Äôelle existe: \\[\\mu_{r}(X)=\\sum_{i \\in \\mathbb{N} } p_{i}\\left[x_{i}-E(X)\\right]^r=E\\left[X-E(X)\\right]^r\\]\nLes premiers moments sont: \\[m_{1}(X)=E(X), \\quad \\mu_{1}(X)=0\\] \\[\\mu_{2}(X)=V(X)=m_{2}(X)-m_{1}^2(X)\\]"
  },
  {
    "objectID": "variables-aleatoires-discretes.html#couple-de-variables-al√©atoires-discr√®tes",
    "href": "variables-aleatoires-discretes.html#couple-de-variables-al√©atoires-discr√®tes",
    "title": "\n1¬† Variables Al√©atoires Discr√®tes\n",
    "section": "Couple de variables al√©atoires discr√®tes",
    "text": "Couple de variables al√©atoires discr√®tes\nConsid√©rons deux variables al√©atoires discr√®tes \\(X\\) et \\(Y\\). Il nous faut pour mod√©liser le probl√®me une fonction qui nous donne la probabilit√© que \\((X = x_i )\\) en m√™me temps que \\((Y = y_j )\\). C‚Äôest la loi de probabilit√© conjointe.\nSoit \\(X\\) et \\(Y\\) deux variables al√©atoires r√©elles discr√®tes, d√©finies sur un espace probabilis√© \\((\\Omega,\\mathcal{A},P)\\) et que\n\\[\\begin{aligned}\n  X(\\Omega) &= \\{x_1,x_2,\\ldots,x_{\\ell}\\} \\\\\n  Y(\\Omega) &= \\{y_1,y_2,\\ldots,y_k\\} \\\\\n            & \\quad (\\ell \\text{ et } k \\in \\mathbb{N})\\end{aligned}\\]\nLa loi du couple \\((X,Y)\\), dite loi de probabilit√© conjointe ou simultan√©e, est enti√®rement d√©finie par les probabilit√©s:\n\\[p_{ij} = P(X=x_i;Y=y_j) = P(\\{X=x_i\\}\\cap\\{Y=y_j\\})\\]\nOn a\n\\[p_{ij} \\geq 0 \\quad \\text{et} \\quad \\sum_{i=1}^{\\ell} \\sum_{j=1}^{k} p_{ij} = 1\\]\nLe couple \\((X,Y)\\) s‚Äôappelle variable al√©atoire √† deux dimensions et peut prendre \\(\\ell \\times k\\) valeurs.\nTable de probabilit√© conjointe\nLes probabilit√©s \\(p_{ij}\\) peuvent √™tre pr√©sent√©es dans un tableau √† deux dimensions qu‚Äôon appelle table de probabilit√© conjointe:\n\nTable de probabilit√© conjointe\n\n\n\n\n\n\n\n\n\n\n\n\\(X\\)\\\\(Y\\)\n\n\\(y_1\\)\n\\(y_2\\)\n\\(\\ldots\\)\n\\(y_j\\)\n\\(\\ldots\\)\n\\(y_k\\)\n\n\n\n\\(x_1\\)\n\\(p_{11}\\)\n\\(p_{12}\\)\n\n\\(p_{1j}\\)\n\n\\(p_{1k}\\)\n\n\n\\(\\vdots\\)\n\n\n\n\n\n\n\n\n\\(x_i\\)\n\\(p_{i1}\\)\n\\(p_{i2}\\)\n\n\\(p_{ij}\\)\n\n\\(p_{ik}\\)\n\n\n\\(\\vdots\\)\n\n\n\n\n\n\n\n\n\\(x_{\\ell}\\)\n\\(p_{\\ell 1}\\)\n\\(p_{\\ell 2}\\)\n\n\\(p_{\\ell j}\\)\n\n\\(p_{\\ell k}\\)\n\n\n\nA la premi√®re ligne figure l‚Äôensemble des valeurs de \\(Y\\) et √† la premi√®re colonne figure l‚Äôensemble des valeurs de \\(X\\). La probabilit√© \\(p_{ij} = P(X=x_i;Y=y_j)\\) est √† l‚Äôintersection de la \\(i^{e}\\) et de la \\(j^{e}\\) colonne.\nLois marginales\nLorsqu‚Äôon conna√Æt la loi conjointe des variables al√©atoires \\(X\\) et \\(Y\\), on peut aussi s‚Äôint√©resser √† la loi de probabilit√© de \\(X\\) seule et de \\(Y\\) seule. Ce sont les lois de probabilit√© marginales.\n\nLoi marginale de \\(X\\): \\[p_{i.} = P(X=x_i) = P[\\{X=x_i\\}\\cap \\Omega] = \\sum_{j=1}^k p_{ij} \\quad \\quad \\forall \\, i=1,2,\\ldots,\\ell\\]\nLoi marginale de \\(Y\\): \\[p_{.j} = P(Y=y_j) = P[ \\Omega \\cap \\{Y=y_j\\}] = \\sum_{i=1}^{\\ell} p_{ij} \\quad \\quad \\forall \\, j=1,2,\\ldots,k\\]\n\nOn peut calculer les lois marginales directement depuis la table de la loi conjointe. La loi marginale de \\(X\\) est calcul√©e en faisant les totaux par ligne, tandis que celle de \\(Y\\) l‚Äôest en faisant les totaux par colonne.\nC‚Äôest le fait que les lois de \\(X\\) et \\(Y\\) individuellement puissent √™tre lues dans les marges du tableau qui leur vaut leur nom de lois marginales.\n\nTable de probabilit√© conjointe avec les lois marginales\n\n\n\n\n\n\n\n\n\n\n\n\n\\(X\\)\\\\(Y\\)\n\n\\(y_1\\)\n\\(y_2\\)\n\\(\\ldots\\)\n\\(y_j\\)\n\\(\\ldots\\)\n\\(y_k\\)\nMarginale de \\(X\\)\n\n\n\n\n\\(x_1\\)\n\\(p_{11}\\)\n\\(p_{12}\\)\n\n\\(p_{1j}\\)\n\n\\(p_{1k}\\)\n\\(p_{1.}\\)\n\n\n\\(\\vdots\\)\n\n\n\n\n\n\n\n\n\n\\(x_i\\)\n\\(p_{i1}\\)\n\\(p_{i2}\\)\n\n\\(p_{ij}\\)\n\n\\(p_{ik}\\)\n\\(p_{i.}\\)\n\n\n\\(\\vdots\\)\n\n\n\n\n\n\n\n\n\n\\(x_{\\ell}\\)\n\\(p_{\\ell 1}\\)\n\\(p_{\\ell 2}\\)\n\n\\(p_{\\ell j}\\)\n\n\\(p_{\\ell k}\\)\n\\(p_{\\ell .}\\)\n\n\n\n\n\n\n\n\n\n\n\n\nMarginale de \\(Y\\)\n\n\\(p_{.1}\\)\n\\(p_{.2}\\)\n\n\\(p_{. j}\\)\n\n\\(p_{.k}\\)\n1\n\n\n\n\n\n\n\n\n\nExercice\n\n\n\nOn tire au hasard 3 boules d‚Äôune urne contenant 3 boules rouges, 4 blanches et 5 noires. \\(X\\) et \\(Y\\) d√©signent respectivement le nombre de boules rouges et celui de boules blanches tir√©es. D√©terminer la loi de probabilit√© conjointe du couple \\((X,Y)\\) ainsi que les lois marginales de \\(X\\) et de \\(Y\\).\n\n\nLois conditionnelles\nPour chaque valeur \\(y_j\\) de \\(Y\\) telle que \\(p_{.j} = P(Y=y_j) \\neq 0\\) on peut d√©finir la loi conditionnelle de \\(X\\) sachant \\(Y=y_j\\) par\n\\[p_{i/j} = P(X=x_i / Y=y_j) = \\frac{P(X=x_i;Y=y_j)}{P(Y=y_j)} = \\frac{p_{ij}}{p_{.j}} \\quad \\quad \\forall i = 1,2,\\ldots,\\ell\\]\nDe m√™me on d√©finit la loi de \\(Y\\) sachant \\(X=x_i\\) par\n\\[p_{j/i} = P(Y=y_j / X=x_i) = \\frac{P(X=x_i;Y=y_j)}{P(X=x_i)} = \\frac{p_{ij}}{p_{i.}} \\quad \\quad \\forall j = 1,2,\\ldots,k\\]\nInd√©pendance de variables al√©atoires\n\nTh√©or√®me 1.4 On dit que deux v.a.r.d sont ind√©pendantes si et seulement si\n\\[P(X=x_i;Y=y_j) = P(X=x_i) P(Y=y_j)\\] \\[\\forall \\, i = 1,2,\\ldots,\\ell \\text{ et }  j = 1,2,\\ldots,k\\]\n\nOn montre que\n\\[P(\\{X\\in A\\} \\cap \\{Y \\in B\\}) = P(\\{X\\in A\\}) P(\\{Y \\in B\\}) \\quad \\quad \\forall \\,\\, A \\text{ et } B \\in \\mathcal{A}\\]\nPropri√©t√©s\nSoit deux v.a.r.d. \\(X\\) et \\(Y\\),\n\n\\(E(X+Y)=E(X)+E(Y)\\)\nSi \\(X\\) et \\(Y\\) sont ind√©pendantes alors \\(E(XY)=E(X)E(Y)\\). Mais la r√©ciproque n‚Äôest pas toujours vraie.\nCovariance\nSoit \\(X\\) et \\(Y\\) deux v.a.r.d. On appelle covariance de \\(X\\) et de \\(Y\\) la valeur si elle existe de\n\\[\n\\begin{align}\nCov(X,Y) &= E[(X-E(X))(Y-E(Y))] \\\\ &= \\sum_i \\sum_j (x_i-E(X))(y_j-E(Y)) p_{ij}\n\\end{align}\\]\nqu‚Äôon peut calculer en utilisant la formule suivante\n\\[Cov(X,Y) = E(XY) - E(X)E(Y)\\]\nPropri√©t√©s\n\n\\(Cov(X,Y)=Cov(Y,X)\\)\n\\(Cov(aX_1+bX_2,Y) = a Cov(X_1,Y) + b Cov(X_2,Y)\\)\n\\(V(X+Y)= V(X) + V(Y) + 2 Cov(X,Y)\\)\n\nSi \\(X\\) et \\(Y\\) sont ind√©pendantes alors\n\n\\(Cov(X,Y) = 0\\) (la r√©ciproque n‚Äôest pas vraie)\n\\(V(X+Y) = V(X) + V(Y)\\) (la r√©ciproque n‚Äôest pas vraie)\n\n\nCoefficient de corr√©lation lin√©aire\nOn appelle coefficient de corr√©lation lin√©aire de \\(X\\) et de \\(Y\\) la valeur d√©finie par\n\\[\\rho = \\rho(X,Y) = \\frac{Cov(X,Y)}{\\sqrt{V(X)V(Y)}} = \\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y}\\]\nOn peut montrer que \\[-1 \\leq \\rho(X,Y) \\leq 1\\]\nPour le montrer on peut partir du fait que la variance est toujours positive ou nulle. Donc \\(V(\\frac{X}{\\sigma_X} + \\frac{Y}{\\sigma_Y}) \\geq 0\\) et \\(V(\\frac{X}{\\sigma_X} - \\frac{Y}{\\sigma_Y}) \\geq 0\\).\nInterpr√©tation de \\(\\rho\\)\n\nLe coefficient de corr√©lation est une mesure du degr√© de lin√©arit√© entre \\(X\\) et \\(Y\\).\nLes valeurs de \\(\\rho\\) proches de \\(1\\) ou \\(-1\\) indiquent une lin√©arit√© quasiment rigoureuse entre \\(X\\) et \\(Y\\).\nLes valeurs de \\(\\rho\\) proche de 0 indiquent une absence de toute relation lin√©aire.\nLorsque \\(\\rho(X,Y)\\) est positif, \\(Y\\) a tendance √† augmenter si \\(X\\) en fait autant.\nLorsque \\(\\rho(X,Y) < 0\\), \\(Y\\) a tendance √† diminuer si \\(X\\) augmente.\nSi \\(\\rho(X,Y) =0\\), on dit que ces deux statistiques sont non corr√©l√©es."
  },
  {
    "objectID": "lois-usuelles-discretes.html#loi-uniforme-discr√®te-mathcalun",
    "href": "lois-usuelles-discretes.html#loi-uniforme-discr√®te-mathcalun",
    "title": "2¬† Lois usuelles discr√®tes",
    "section": "Loi uniforme discr√®te \\(\\mathcal{U}(n)\\)",
    "text": "Loi uniforme discr√®te \\(\\mathcal{U}(n)\\)\n\nD√©finition 2.1 Une distribution de probabilit√© suit une loi uniforme lorsque toutes les valeurs prises par la variable al√©atoire sont √©quiprobables. Si \\(n\\) est le nombre de valeurs diff√©rentes prises par la variable al√©atoire alors on a:\n\\[\nP(X=x_i)=\\frac{1}{n} \\qquad \\forall \\, i \\in \\{1,\\ldots, n\\}\n\\tag{2.1}\\]\n\nExemple: La distribution des chiffres obtenus au lancer de d√© (si ce dernier est non pip√©) suit une loi uniforme dont la loi de probabilit√© est la suivante :\n\n\n\n\\(x_i\\)\n1\n2\n3\n4\n5\n6\n\n\n\n\n\\(P(X = x_i)\\)\n\\(\\frac{1}{6}\\)\n\\(\\frac{1}{6}\\)\n\\(\\frac{1}{6}\\)\n\\(\\frac{1}{6}\\)\n\\(\\frac{1}{6}\\)\n\\(\\frac{1}{6}\\)\n\n\n\nMoments de loi uniforme discr√®te\nDans le cas particulier d‚Äôune loi uniforme discr√®te o√π chaque valeur de la variable al√©atoire \\(X\\) correspond √† son rang, i.e. \\(x_i=i \\, \\, \\forall i \\in \\{1,\\ldots, n\\}\\), on a: \\[E(X)=\\frac{n+1}{2} \\quad \\text{et} \\quad V(X)=\\frac{n^2-1}{12}\\] La d√©monstration de ces r√©sultats est √©tablie en utilisant les √©galit√©s (cf.¬†Annexe) \\[\\sum_{i=1}^n i=\\frac{n(n+1)}{2} \\quad \\text{et} \\quad \\sum_{i=1}^n i^2=\\frac{n(n+1)(2n+1)}{6}\\]\nEn revenant √† l‚Äôexemple du lancer du d√© de cette section, on peut calculer directement les moments de \\(X\\): \\[E(X)=\\frac{6+1}{2}=3.5\\] et \\[V(X)=\\frac{6^2-1}{12}=\\frac{35}{12}\\simeq 2.92\\]"
  },
  {
    "objectID": "lois-usuelles-discretes.html#loi-de-bernoulli-mathcalbp",
    "href": "lois-usuelles-discretes.html#loi-de-bernoulli-mathcalbp",
    "title": "2¬† Lois usuelles discr√®tes",
    "section": "Loi de Bernoulli \\(\\mathcal{B}(p)\\)",
    "text": "Loi de Bernoulli \\(\\mathcal{B}(p)\\)\n\nD√©finition 2.2 On r√©alise une exp√©rience dont le r√©sultat sera interpr√©t√© soit comme un succ√®s soit comme un √©chec. On d√©finit alors la variable al√©atoire \\(X\\) en lui donnant la valeur 1 lors d‚Äôun succ√®s et 0 lors d‚Äôun √©chec (variable indicatrice). La loi de probabilit√© de \\(X\\) est alors\n\\[\n\\begin{align}\n    &p(1)=P(X=1)=p  \\\\\n    &p(0)=P(X=0)= 1-p=q \\notag\n\\end{align}\n\\tag{2.2}\\]\no√π \\(p\\) est la probabilit√© d‚Äôun succ√®s, \\(0 \\leq p \\leq 1\\).\nUne variable al√©atoire \\(X\\) est dite de Bernoulli \\(X \\sim \\mathcal{B} \\left({p}\\right)\\) s‚Äôil existe un nombre \\(p \\, \\in \\, ]0,1[\\) tel que la loi de probabilit√© de \\(X\\) soit donn√©e par (2.2).\n\nLa fonction de r√©partition est d√©finie par: \\[F(x) =\n       \\left\\{\n       \\begin{array}{ll}\n         0 & \\quad \\text{si $x < 0$} \\\\\n         1 - p & \\quad \\text{si $0 \\leq x < 1$} \\\\\n         1 & \\quad \\text{si $x \\geq 1$}.\n       \\end{array}\n       \\right.\\]\nL‚Äôesp√©rance la loi de Bernoulli est \\(p\\), en effet\n\\[E(X) =1 \\times P(X=1)+0 \\times P(X=0)=P(X=1)=p\\]\nLa variance la loi de Bernoulli est \\(np\\), en effet\n\\[V(X) =E(X^2)-E^2(X)=p-p^2=p(1-p)=pq\\] car \\[E(X^2) =1^2\\times P(X=1)+0^2 \\times P(X=0)=P(X=1)=p\\]"
  },
  {
    "objectID": "lois-usuelles-discretes.html#loi-binomiale-mathcalbnp",
    "href": "lois-usuelles-discretes.html#loi-binomiale-mathcalbnp",
    "title": "2¬† Lois usuelles discr√®tes",
    "section": "Loi Binomiale \\(\\mathcal{B}(n,p)\\)",
    "text": "Loi Binomiale \\(\\mathcal{B}(n,p)\\)\n\n\n\n\n\n\nNote\n\n\n\nD√©crite pour la premi√®re fois par Isaac Newton en 1676 et d√©montr√©e pour la premi√®re fois par le math√©maticien suisse Jacob Bernoulli en 1713, la loi binomiale est l‚Äôune des distributions de probabilit√© les plus fr√©quemment rencontr√©es en statistique appliqu√©e.\n\n\nSupposons qu‚Äôon ex√©cute maintenant \\(n\\) √©preuves ind√©pendantes, chacune ayant \\(p\\) pour probabilit√© de succ√®s et \\(1-p\\) pour probabilit√© d‚Äô√©chec. La variable al√©atoire \\(X\\) qui compte le nombre de succ√®s sur l‚Äôensemble des \\(n\\) √©preuves est dite variable al√©atoire binomiale de param√®tres \\(n\\) et \\(p\\).\n\n\n\n\n\n\nAstuce\n\n\n\nUne variable de Bernoulli n‚Äôest donc qu‚Äôune variable binomiale de param√®tres \\((1,p)\\).\n\n\n\nD√©finition 2.3 Si on effectue \\(n\\) √©preuves successives ind√©pendantes o√π on note √† chaque fois la r√©alisation ou non d‚Äôun certain √©v√©nement \\(A\\), on obtient une suite de la forme \\(AA\\bar{A}A\\bar{A}\\ldots \\bar{A}AA\\). Soit \\(X\\) le nombre de r√©alisations de \\(A\\). On d√©finit ainsi une v.a. \\(X\\) qui suit une loi binomiale de param√®tres \\(n\\) et \\(p=P(A)\\), caract√©ris√©e par \\(X(\\Omega)=\\{0, 1,\\ldots, n\\}\\) :\n\\[\nP(X=k)=\\binom{n}{k}p^k (1-p)^{n-k} \\qquad 0\\leq k \\leq n   \n\\tag{2.3}\\]\nOn √©crit \\(X \\sim \\mathcal{B} \\left({n, p}\\right)\\). Donc la loi binomiale mod√©lise le nombre de r√©alisations de \\(A\\) (succ√®s) obtenues lors de la r√©p√©tition ind√©pendante et identique de \\(n\\) √©preuves de Bernoulli.\n\n\n\n\n\n\n\nAstuce\n\n\n\nPour √©tablir (2.3) il faut remarquer que \\(\\binom{n}{k}\\) est le nombre d‚Äô√©chantillons de taille \\(n\\) comportant exactement \\(k\\) √©v√©nements \\(A\\), de probabilit√© \\(p^k\\), ind√©pendamment de l‚Äôordre, et donc \\(n-k\\) √©v√©nements \\(\\bar{A}\\), de probabilit√© \\((1-p)^{n-k}\\).\n\n\nRemarque: Il est possible d‚Äôobtenir ais√©ment les valeurs des combinaisons de la loi binomiale en utilisant le triangle de Pascal.\nEn utilisant la formule du bin√¥me de Newton, on v√©rifie bien que c‚Äôest une loi de probabilit√©:\n\\[{\\sum_{k=0}^nP(X=k)=\\sum_{k=0}^n\\binom{n}{k} p^{k}(1-p)^{n-k}=[p+(1-p)]^n=1}\\]\nExemple: On jette cinq pi√®ces √©quilibr√©es. Les r√©sultats sont suppos√©s ind√©pendants. Donner la loi de probabilit√© de la variable \\(X\\) qui compte le nombre de piles obtenus.\nMoments de la loi Binomiale\nPour calculer facilement les moments de cette loi, nous allons associer √† chaque √©preuve \\(i\\), \\(1\\leq i \\leq n\\), une v.a. de Bernoulli (variable indicatrice sur \\(A\\)): \\[{1}_A=X_i = \\left\\{\n\\begin{array}{l l}\n1 & \\quad \\text{si $A$ est r√©alis√©}\\\\\n0 & \\quad \\text{si $\\bar{A}$ est r√©alis√©}\\\\\n  \\end{array} \\right.\\] On peut √©crire alors: \\(X=\\sum_{i=1}^nX_i=X_1+X_2+\\ldots+X_n\\), ce qui nous permet de d√©duire ais√©ment:\n\\[\\begin{aligned}\n    E(X)    &=E\\left(\\sum_{i=1}^nX_i\\right)=\\sum_{i=1}^nE(X_i)=np \\\\\n    \\text{et} \\nonumber \\\\\n    V(X)    &=V\\left(\\sum_{i=1}^nX_i\\right) \\\\\n            &=\\sum_{i=1}^nV(X_i)=np(1-p) \\quad \\text{car les v.a. $X_i$ sont ind√©pendantes.}\n  \\end{aligned}\\]\nLe calcul direct des moments de \\(X\\) peut s‚Äôeffectuer √† partir de la d√©finition g√©n√©rale, mais de fa√ßon beaucoup plus laborieuse:\n\\[\\begin{aligned}\nE(X)   &= \\sum_{k=0}^nk \\binom{n}{k} p^{k}(1-p)^{n-k} \\\\\n        &=\\sum_{k=1}^nk \\frac{n!}{k!(n-k)!} p^{k}(1-p)^{n-k} \\\\\n        &= \\sum_{k=1}^n\\frac{n!}{(k-1)!(n-k)!} p^{k}(1-p)^{n-k} \\\\\n        &= np \\sum_{k=1}^n\\frac{(n-1)!}{(k-1)!(n-k)!} p^{k-1}(1-p)^{n-k} \\\\\n        &= np \\sum_{j=0}^{n-1}\\frac{(n-1)!}{j!(n-1-j)!}p^j (1-p)^{n-1-j} \\\\\n        &=np \\sum_{j=0}^{n-1}\\binom{n-1}{j} p^{j}(1-p)^{n-1-j} \\\\\n        &= np [p+(1-p)]^{n-1}=np\n\\end{aligned}\\]\nPour obtenir \\(E(X^2)\\) par un proc√©d√© de calcul identique, on passe par l‚Äôinterm√©diaire du moment factoriel \\(E[X(X-1)]=E(X^2)-E(X)\\): \\[\\begin{aligned}\n  E[X(X-1)]&= \\sum_{k=0}^nk(k-1) \\frac{n!}{k!(n-k)!} p^{k}(1-p)^{n-k} \\\\\n  &= n(n-1)p^2 \\sum_{k=2}^{n}\\frac{(n-2)!}{(k-2)!(n-k)!} p^{k-2}(1-p)^{n-k} \\\\ &= n(n-1)p^2 \\sum_{j=0}^{n-2}\\binom{n-2}{j} p^{j}(1-p)^{n-2-j} \\\\\n  &= n(n-1)p^2[p+(1-p)]^{n-2}= n(n-1)p^2\n   \\end{aligned}\\]\nOn en d√©duit alors:\n\\[E(X^2)=E[X(X-1)]+E(X)= n(n-1)p^2+np,\\]\npuis:\n\\[\\begin{aligned}\n   V(X) &=n(n-1)p^2+np-(np)^2 \\\\\n        &=n^2p^2+np(1-p)-n^2p^2 \\\\\n        &=np(1-p)\n  \\end{aligned}\\]\nLe nombre de r√©sultats pile apparus au cours de \\(n\\) jets d‚Äôune pi√®ce de monnaie suit une loi binomiale \\(\\mathcal{B} \\left({n, 1/2}\\right)\\): \\[P(X=k)=\\binom{n}{k}\\left(\\frac{1}{2}\\right)^k \\left(\\frac{1}{2}\\right)^{n-k}=\\frac{\\binom{n}{k}}{2^n}, \\quad 0\\leq k \\leq n\\] avec \\(E(X)=n/2\\) et \\(V(X)=n/4\\).\nLe nombre \\(N\\) de boules rouges apparues au cours de \\(n\\) tirages avec remise dans une urne contenant deux rouges, trois vertes et une noire suit une loi binomiale \\(\\mathcal{B} \\left({n, 1/3}\\right)\\): \\[P(N=k)=\\binom{n}{k}\\left(\\frac{1}{3}\\right)^k \\left(\\frac{2}{3}\\right)^{n-k}=\\binom{n}{k} \\frac{2^{n-k}}{3^n}, \\quad 0\\leq k \\leq n\\] avec \\(E(X)=n/3\\) et \\(V(X)=2n/9\\).\n\nTh√©or√®me 2.1 Si \\(X_1 \\sim \\mathcal{B} \\left({n_1, p}\\right)\\) et \\(X_2 \\sim \\mathcal{B} \\left({n_2, p}\\right)\\), les v.a. \\(X_1\\) et \\(X_2\\) √©tant ind√©pendantes, alors \\(X_1+X_2 \\sim \\mathcal{B} \\left({n_1+n_2, p}\\right)\\). Ceci r√©sulte de la d√©finition d‚Äôune loi binomiale puisqu‚Äôon totalise ici le r√©sultat de \\(n_1+n_2\\) √©preuves ind√©pendantes."
  },
  {
    "objectID": "lois-usuelles-discretes.html#loi-de-poisson-mathcalplambda",
    "href": "lois-usuelles-discretes.html#loi-de-poisson-mathcalplambda",
    "title": "2¬† Lois usuelles discr√®tes",
    "section": "Loi de Poisson \\(\\mathcal{P}(\\lambda)\\)",
    "text": "Loi de Poisson \\(\\mathcal{P}(\\lambda)\\)\n\n\n\n\n\n\nNote\n\n\n\nLa loi de Poisson est d√©couverte au d√©but du XIX\\(^e\\) si√®cle par le magistrat fran√ßais Sim√©on-Denis Poisson. Les variables al√©atoires de Poisson ont un champ d‚Äôapplication fort vaste, en particulier du fait qu‚Äôon peut les utiliser pour approximer des variables al√©atoires binomiales de param√®tres \\((n,p)\\) pour autant que \\(n\\) soit grand et \\(p\\) assez petit pour que \\(np\\) soit d‚Äôordre de grandeur moyen.\n\n\n\nD√©finition 2.4 Une v.a. \\(X\\) suit une loi de Poisson de param√®tre \\(\\lambda>0\\) si c‚Äôest une variable √† valeurs enti√®res, \\(X(\\Omega)=\\mathbb{N}\\), donc avec une infinit√© de valeurs possibles, de probabilit√©:\n\\[\nP(X=k)=e^{-\\lambda} \\frac{\\lambda^k}{k!}, \\quad k \\in \\mathbb{N}\n\\tag{2.4}\\]\nCette loi ne d√©pend qu‚Äôun seul param√®tre r√©el positif \\(\\lambda\\), avec l‚Äô√©criture symbolique \\(X \\sim \\mathcal{P}(\\lambda)\\).\n\nLe d√©veloppement en s√©rie enti√®re de l‚Äôexponentielle \\(e^\\lambda=\\displaystyle \\sum_{k=0}^{+\\infty} \\frac{\\lambda^k}{k!}\\) permet de v√©rifier qu‚Äôil s‚Äôagit bien d‚Äôune loi de probabilit√©:\n\\[\\sum_{k=0}^{\\infty} P(X=k)=\\sum_{k=0}^{\\infty} e^{-\\lambda} \\frac{\\lambda^k}{k!}=e^{-\\lambda}\\sum_{k=0}^{\\infty} \\frac{\\lambda^k}{k!}=e^{-\\lambda}e^{\\lambda}=1\\]\nMoments de loi de Poisson\nLe calcul de l‚Äôesp√©rance math√©matique se d√©duit du d√©veloppement en s√©rie enti√®re de l‚Äôexponentielle: \\[\\begin{aligned}\n    E(X)&=\\sum_{k=0}^{\\infty} k P(X=k)=\\sum_{k=1}^{\\infty} k e^{-\\lambda} \\frac{\\lambda^k}{k!} \\\\\n        &=e^{-\\lambda} \\sum_{k=1}^{\\infty}  \\frac{\\lambda^k}{(k-1)!}=\\lambda e^{-\\lambda} \\sum_{k=1}^{\\infty}  \\frac{\\lambda^{k-1}}{(k-1)!} \\\\\n        &= \\lambda e^{-\\lambda} \\sum_{j=0}^{\\infty}  \\frac{\\lambda^{j}}{j!}= \\lambda e^{-\\lambda}  e^{\\lambda} \\\\\n        &= \\lambda.\\end{aligned}\\] Pour calculer la variance nous n‚Äôallons pas calculer \\(E(X^2)\\) mais le moment factoriel \\(E[X(X-1)]\\) qui s‚Äôobtient plus facilement, selon la m√©thode pr√©c√©dente:\n\\[\\begin{aligned}\n    E[X(X-1)] &=\\sum_{k=0}^{\\infty} k(k-1)P(X=k)=\\sum_{k=2}^{\\infty} k(k-1)  \\,e^{-\\lambda} \\frac{\\lambda^k}{k!} \\\\\n        &=e^{-\\lambda} \\sum_{k=2}^{\\infty}  \\frac{\\lambda^k}{(k-2)!}=\\lambda^2 e^{-\\lambda} \\sum_{k=2}^{\\infty}  \\frac{\\lambda^{k-2}}{(k-2)!} \\\\\n        &= \\lambda^2 e^{-\\lambda} \\sum_{j=0}^{\\infty}  \\frac{\\lambda^{j}}{j!}= \\lambda^2 e^{-\\lambda}  e^{\\lambda} = \\lambda^2.\\end{aligned}\\] On en d√©duit: \\[\\begin{aligned}\n    V(X)&=E(X^2)-E^2(X)=E[X(X-1)]+E(X)-E^2(X) \\\\\n        &=\\lambda^2+\\lambda-\\lambda^2=\\lambda.\\end{aligned}\\]\n\nTh√©or√®me 2.2 Si \\(X\\) et \\(Y\\) sont deux variables ind√©pendantes suivant des lois de Poisson \\[X \\sim \\mathcal{P}(\\lambda) \\quad \\text{et} \\quad Y \\sim \\mathcal{P}(\\mu)\\] alors leur somme suit aussi une loi de Poisson: \\[X+Y \\sim \\mathcal{P}(\\lambda+\\mu)\\]\n\nExemple: Soit \\(X\\) la variable al√©atoire associ√©e au nombre de micro-ordinateurs vendus chaque jour dans le magasin. On suppose que \\(X\\) suit une loi de Poisson de param√®tre \\(\\lambda=5\\). On √©crit alors \\(X \\sim \\mathcal{P}(5).\\)\nLa probabilit√© associ√©e √† la vente de 5 micro-ordinateurs se d√©termine par : \\[P(X=5)=e^{-5} \\frac{5^5}{5!}=e^{-5}\\simeq 0.1755\\] La probabilit√© de vendre au moins 2 micro-ordinateurs est √©gal √†:\n\\[\\begin{aligned}\nP(X \\geq 2)&=1-\\left(e^{-5} \\frac{5^0}{0!}+e^{-5} \\frac{5^1}{1!}\\right)\\simeq 0.9596\\end{aligned}\\]\nLe nombre moyen de micro-ordinateurs vendus chaque jour dans le magasin est √©gal √† 5 puisque \\(E(X)=\\lambda=5\\)."
  },
  {
    "objectID": "lois-usuelles-discretes.html#approximation-dune-loi-binomiale",
    "href": "lois-usuelles-discretes.html#approximation-dune-loi-binomiale",
    "title": "2¬† Lois usuelles discr√®tes",
    "section": "Approximation d‚Äôune loi binomiale",
    "text": "Approximation d‚Äôune loi binomiale\nLe th√©or√®me de Poisson nous montre que si \\(n\\) est suffisamment grand et \\(p\\) assez petit, alors on peut approcher la distribution d‚Äôune loi binomiale de param√®tres \\(n\\) et \\(p\\) par celle d‚Äôune loi de Poisson de param√®tre \\(\\lambda=np\\), en effet \\[\\text{si} \\; n \\rightarrow \\infty \\; \\text{et}\\; p \\rightarrow 0 \\; \\text{alors} \\; X: \\mathcal{B}(n, p) \\rightarrow \\mathcal{P}(\\lambda)\\]\nUne bonne approximation est obtenue si \\(n \\geq 50\\) et \\(np \\leq 5\\).\nDans ce contexte, la loi de Poisson est souvent utilis√©e pour mod√©liser le nombre de succ√®s lorsqu‚Äôon r√©p√®te un tr√®s grand nombre de fois une exp√©rience ayant une chance tr√®s faible de r√©ussir par une loi de Poisson (nombre de personnes dans la population fran√ßaise atteints d‚Äôune maladie rare, par exemple).\nOn cherche la probabilit√© de trouver au moins un centenaire parmi 200 personnes dans une population o√π une personne sur cent est un centenaire.\nLa probabilit√© \\(p=1/100=0.01\\) √©tant faible et \\(n=200\\) √©tant suffisamment grand, on peut mod√©liser le nombre \\(X\\) de centenaires pris parmi 200 personnes par la loi de Poisson de param√®tre \\(\\lambda=200 \\times 0.01=2\\). Donc on a: \\[P(X\\geq 1)=1-P(X=0)=1-e^{-2}\\simeq 0.86\\]\nSoit une v.a. \\(X\\) telle que \\(X \\sim \\mathcal{B}(100, 0.01)\\), les valeurs des probabilit√©s pour \\(k\\) de 0 √† 5 ainsi que leur approximation √† \\(10^{-3}\\) avec une loi de Poisson de param√®tre \\(\\lambda= np =1\\) sont donn√©es dans le tableau ci-dessous :\n\n\n\n\\(k\\)\n0\n1\n2\n3\n4\n5\n\n\n\n\n\\(P(X = k)\\)\n0.366\n0.370\n0.185\n0.061\n0.015\n0.000\n\n\nApproximation\n0.368\n0.368\n0.184\n0.061\n0.015\n0.003\n\n\n\nDans le cas de cet exemple o√π \\(n =100\\) et \\(np =1\\), l‚Äôapproximation de la loi binomiale par une loi de poisson donne des valeurs de probabilit√©s identiques √† \\(10^{-3}\\) pr√®s."
  },
  {
    "objectID": "lois-usuelles-discretes.html#loi-g√©om√©trique-ou-de-pascal-mathcalgp",
    "href": "lois-usuelles-discretes.html#loi-g√©om√©trique-ou-de-pascal-mathcalgp",
    "title": "2¬† Lois usuelles discr√®tes",
    "section": "Loi G√©om√©trique ou de Pascal \\(\\mathcal{G}(p)\\)",
    "text": "Loi G√©om√©trique ou de Pascal \\(\\mathcal{G}(p)\\)\nOn effectue des √©preuves successives ind√©pendantes jusqu‚Äô√† la r√©alisation d‚Äôun √©v√©nement particulier \\(A\\) de probabilit√© \\(p=P(A)\\) et on note \\(X\\) le nombre al√©atoire d‚Äô√©preuves effectu√©es. On d√©finit ainsi une v.a. √† valeurs enti√®res de loi g√©om√©trique, ou de Pascal. A chaque √©preuve est associ√© l‚Äôensemble fondamental \\(\\Omega=\\{A, \\bar{A}\\}\\) et l‚Äô√©v√©nement \\(\\{X=k\\}\\) pour \\(k\\in \\mathbb{N^*}\\) est repr√©sent√© par une suite de \\(k-1\\) √©v√©nements \\(\\bar{A}\\), termin√©e par l‚Äô√©v√©nement \\(A\\): \\[\\underbrace{\\bar{A}\\bar{A}\\ldots \\bar{A}}_{k-1}A\\] D‚Äôo√π:\n\\[\nP(X=k)=(1-p)^{k-1}p \\quad \\forall \\, k \\in \\mathbb{N^*}\n\\tag{2.5}\\]\nCette loi peut servir √† mod√©liser des temps de vie, ou des temps d‚Äôattente, lorsque le temps est mesur√© de mani√®re discr√®te (nombre de jours par exemple).\nEn utilisant la s√©rie enti√®re\n\\[\n\\sum_{k=0}^\\infty x^k = 1/(1-x) \\quad \\text{pour} \\quad |x|<1\n\\tag{2.6}\\]\non v√©rifie bien que c‚Äôest une loi de probabilit√©:\n\\[\\begin{aligned}\n\\sum_{k=1}^\\infty P(X=k)&= \\sum_{k=1}^\\infty (1-p)^{k-1}p = p \\sum_{j=0}^\\infty (1-p)^{j} \\\\\n&= p \\frac{1}{1-(1-p)}=1\\end{aligned}\\]\nMoments de loi G√©om√©trique\nEn d√©rivant la s√©rie enti√®re (2.5) ci-dessus, on obtient \\(\\sum_{k=1}^\\infty k x^{k-1}=1/(1-x)^2\\). Ceci permet d‚Äôobtenir l‚Äôesp√©rance:\n\\[E(X)=\\sum_{k=1}^\\infty kp(1-p)^{k-1}=\\frac{p}{[1-(1-p)]^2}=\\frac{1}{p}\\]\nEn d‚Äôautres termes, si des √©preuves ind√©pendantes ayant une probabilit√© \\(p\\) d‚Äôobtenir un succ√®s sont r√©alis√©s jusqu‚Äô√† ce que le premier succ√®s se produise, le nombre esp√©r√© d‚Äôessais n√©cessaires est √©gal √† \\(1/p\\). Par exemple, le nombre esp√©r√© de jets d‚Äôun d√© √©quilibr√© qu‚Äôil faut pour obtenir la valeur 1 est 6.\nLe calcul de la variance se fait √† partir du moment factoriel et en utilisant la d√©riv√©e seconde de la s√©rie enti√®re (2.5): \\(\\sum_{k=2}^\\infty k(k-1) x^{k-2} = 2/(1-x)^3\\), Donc\n\\[\\begin{aligned}\nE[X(X-1)]   &=\\sum_{k=2}^\\infty k(k-1)p(1-p)^{k-1} \\\\\n            &= p(1-p)\\sum_{k=2}^\\infty k(k-1)(1-p)^{k-2} \\\\\n            &= \\frac{2p(1-p)}{[1-(1-p)]^3}=\\frac{2(1-p)}{p^2}\n            \\end{aligned}\\]\nd‚Äôo√π on d√©duit: \\[V(X)=E[X(X-1)]+E(X)-E^2(X)=\\frac{1-p}{p^2}\\]\nSi l‚Äôon consid√®re la variable al√©atoire \\(X\\) ‚Äúnombre de naissances observ√©es jusqu‚Äô√† l‚Äôobtention d‚Äôune fille‚Äù avec p = 1/2 (m√™me probabilit√© de naissance d‚Äôune fille ou d‚Äôun gar√ßon), alors X suit une loi g√©om√©trique et on a pour tout \\(k\\in \\mathbb{N^*}\\):\n\\[P(X=k)=(1-1/2)^{k-1}(1/2)=1/2^k\\]\navec \\(E(X)=2\\) et \\(V(X)=2\\)."
  },
  {
    "objectID": "lois-usuelles-discretes.html#loi-binomiale-n√©gative-mathcalbnrp",
    "href": "lois-usuelles-discretes.html#loi-binomiale-n√©gative-mathcalbnrp",
    "title": "2¬† Lois usuelles discr√®tes",
    "section": "Loi Binomiale N√©gative \\(\\mathcal{BN}(r,p)\\)",
    "text": "Loi Binomiale N√©gative \\(\\mathcal{BN}(r,p)\\)\n\n\\(\\varepsilon\\): ‚ÄúOn r√©p√©te l‚Äô√©preuve de Bernoulli jusqu‚Äô√† obtenir un total de \\(r\\) succ√®s‚Äù.\nExemple avec : \\[\\bar{A} \\quad  {A} \\quad  \\bar{A} \\quad  \\bar{A} \\quad  \\bar{A} \\quad  {A} \\quad \\bar{A} \\quad  \\bar{A} \\quad  {A}\\] \\[{E} \\quad  {S} \\quad  {E} \\quad  {E} \\quad  {E} \\quad  {S} \\quad {E} \\quad  {E} \\quad  {S}\\]\nMais on peut obtenir d‚Äôautres fa√ßons: \\[{S} \\quad  {E} \\quad  {E} \\quad  {E} \\quad  {E} \\quad  {E} \\quad {S} \\quad  {E} \\quad  {S}\\] \\[{E} \\quad  {E} \\quad  {E} \\quad  {E} \\quad  {S} \\quad  {E} \\quad {S} \\quad  {E} \\quad  {S}\\]\nChaque √©preuve a \\({p}\\) pour probabilit√© de succ√®s et \\({1-p}\\) pour probabilit√© d‚Äô√©chec.\nD√©signons \\(X=\\)‚Äúle nombre d‚Äô√©preuves n√©cessaires pour atteindre ce r√©sultat‚Äù. \\[\\underbrace{\\overbrace{{E} \\quad  {S} \\quad  {E} \\quad  {E} \\quad  {E} \\quad  {S} \\quad {E} \\quad  {E}}^{ {r-1 \\, succ√®s}\\, et \\, {k-r \\, √©checs}} \\quad  {S}}_{X=k}\\]\n\\(X(\\Omega)=\\{r,r+1,r+2,\\ldots\\}\\). On dit \\(X \\sim \\mathcal{BN}(r,p)\\).\n\\(\\forall \\, k \\in X(\\Omega),\\) \\[P(X=k) = \\binom{{k-1}}{{r-1}} {p^r} {(1-p)^{k-r}}\\]\n\n\n\\(\\mathcal{G}(p)=\\mathcal{BN}(1,p)\\)\n\n\\(\\varepsilon\\): ‚ÄúOn r√©p√©te l‚Äô√©preuve de Bernoulli jusqu‚Äô√† obtenir un total de \\(r\\) succ√®s‚Äù.\nSoit, \\[{E} \\quad \\ldots \\quad {E} \\quad {S} \\quad  {E} \\quad  \\ldots \\quad  {E} \\quad  {S} \\ldots \\quad {E} \\ldots \\quad  {E} \\quad  {S}\\]\nSoit, \\(Y_1\\) le nombre d‚Äô√©preuves n√©cessaires jusqu‚Äôau premier succ√®s, \\(Y_2\\) le nombre d‚Äô√©preuves suppl√©mentaires n√©cessaires pour obtenir un deuxi√®me succ√®s, \\(Y_3\\) celui menant au 3√®me et ainsi de suite.\nC√†d, \\[\\underbrace{{E} \\quad \\ldots \\quad {E} \\quad {S}}_{Y_1} \\quad  \\underbrace{{E} \\quad  \\ldots \\quad  {E} \\quad  {S}}_{Y_2} \\quad \\underbrace{\\ldots}_{\\ldots} \\quad \\underbrace{{E} \\quad \\ldots \\quad  {E} \\quad  {S}}_{Y_r}\\]\nLes tirages √©tants ind√©pendantes et ayant toujours la m√™me probabilit√© de succ√®s, chacune des variables \\(Y_1,Y_2,\\ldots,Y_r\\) est g√©om√©trique \\(\\mathcal{G}(p)\\).\n\\(X=\\)‚Äúle nombre d‚Äô√©preuves n√©cessaires √† l‚Äôobtention de \\(r\\) succ√®s‚Äù\\(=Y_1 + Y_2 + \\ldots + Y_r\\).\nDonc, \\[E(X)= E(Y_1) + E(Y_2) + \\ldots + E(Y_r) = \\sum_{i=1}^r \\frac{1}{p} = \\frac{r}{p}\\] et \\[V(X)= \\sum_{i=1}^r V(Y_i) = \\frac{r(1-p)}{p^2}\\] car les \\(Y_i\\) sont ind√©pendantes."
  },
  {
    "objectID": "exos1.html#combinatoire",
    "href": "exos1.html#combinatoire",
    "title": "Feuille d‚Äôexercices 1",
    "section": "Combinatoire",
    "text": "Combinatoire\n\nExercice 1 Questions diverses:\n\nEn informatique, on utilise le syst√®me binaire pour coder les caract√®res. Un bit (binary digit : chiffre binaire) est un √©l√©ment qui prend la valeur 0 ou la valeur 1. Avec 8 chiffres binaires (un octet), combien de caract√®res peut-on coder?\n\n\n\n\n\nA l‚Äôoccasion d‚Äôune comp√©tition sportive groupant 18 athl√®tes, on attribue une m√©daille d‚Äôor, une d‚Äôargent, une de bronze. Combien y-a-t-il de distributions possibles (avant la comp√©tition, bien s√ªr‚Ä¶) ?\n\n\n\n\n\nUn tournoi sportif compte 8 √©quipes engag√©es. Chaque √©quipe doit rencontrer toutes les autres une seule fois. Combien doit-on organiser de matchs ?\n\n\n\n\n\n\nExercice 2 On jette un d√© üé≤ √©quilibr√© 3 fois de suite, et on s‚Äôint√©resse au total des points obtenus. De combien de fa√ßons peut-on obtenir:\n\nun total √©gale √† 16.\nun total √©gale √† 15.\nun total au moins √©gale √† 15.\n\n\n\nExercice 3 Dans une entreprise, on compte 12 c√©libataires parmi les 30 employ√©s. On d√©sire faire un sondage : pour cela on choisit un √©chantillon de quatre personnes dans ce service.\n\nQuel est le nombre d‚Äô√©chantillons diff√©rents possibles ?\nQuel est le nombre d‚Äô√©chantillons ne contenant aucun c√©libataire ?\nQuel est le nombre d‚Äô√©chantillons contenant au moins un c√©libataire ?\n\n\n\nExercice 4 On tire simultan√©ment 5 cartes d‚Äôun jeu de 52 cartes.\n\nCombien de tirages diff√©rents peut-on obtenir?\n\nCombien de tirages peut-on obtenir ? contenant:\n\n5 carreaux ou 5 coeurs;\n2 coeurs et 3 piques;\nau moins 1 roi;\nau plus 1 roi."
  },
  {
    "objectID": "exos1.html#√©v√©nements",
    "href": "exos1.html#√©v√©nements",
    "title": "Feuille d‚Äôexercices 1",
    "section": "√âv√©nements",
    "text": "√âv√©nements\n\nExercice 5 Soit \\(A\\),\\(B\\) et \\(C\\) trois √©v√©nements d‚Äôun espace probabilisable \\((\\Omega,\\mathcal{A})\\). Exprimer en fonction de \\(A\\),\\(B\\) et \\(C\\) et des op√©rations ensemblistes (r√©union, intersection et compl√©mentaire) les √©v√©nements ci-apr√®s:\n\n\\(A\\) seul (parmi les 3 √©v√©nements) se produit.\n\\(A\\) et \\(C\\) se produisent, mais non \\(B\\).\nLes trois √©v√©nements se produisent.\nL‚Äôun au moins des 3 √©v√©nements se produit.\nAucun des trois √©v√©nements ne se produit."
  },
  {
    "objectID": "exos1.html#probabilit√©",
    "href": "exos1.html#probabilit√©",
    "title": "Feuille d‚Äôexercices 1",
    "section": "Probabilit√©",
    "text": "Probabilit√©\n\nExercice 6 Soit les √©v√©nements \\(A\\) et \\(B\\) t.q. \\(P(A) = x, P(B) = y\\) et \\(P(A \\cup B) = z\\). Trouver les probabilit√©s suivantes: \\(P(A \\cap B), P(\\bar{A} \\cup \\bar{B})\\), et \\(P(A \\cap \\bar{B})\\).\n\n\nExercice 7 Soit \\((\\Omega,\\mathcal{A},P)\\) un espace probabilis√©. Soient \\(A\\),\\(B\\) et \\(C\\) trois √©v√©nements quelconques. On pose : \\(E = A \\cap \\bar{B} \\cap \\bar{C}\\) et \\(F = A \\cap (B \\cup C)\\).\n\nMontrer que \\(E\\) et \\(F\\) sont incompatibles.\nMontrer que \\(E \\cup F = A\\).\nSachant que \\(P(A)=0.6; \\, P(A\\cap B)=0.2; \\, P(A\\cap C)=0.1\\) et \\(P(A\\cap B \\cap C)=0.05\\): Calculer \\(P(F)\\) et \\(P(E)\\).\n\n\n\n\nExercice 8 \n\nUne urne contient 13 boules dont 6 noires, 3 blanches et 4 rouges. On pioche 4 boules. On pose\n\nE: ¬´ obtenir exactement 2 blanches ¬ª\nF: ¬´ obtenir exactement 2 rouges ¬ª\n\n\nOn suppose qu‚Äô il n‚Äô y a pas remise. Calculer les probabilit√©s suivantes : \\(P(E \\cap F), P_F (E), P_E(F)\\). Les √©v√®nements E et F sont-ils ind√©pendants?\nRecommencer l‚Äô exercice en supposant que l‚Äôon pioche avec remise.\n\n\n\nExercice 9 Une urne contient dix boules (6 blanches et 4 rouges). On tire au hasard et successivement deux boules de cette urne. Calculer, dans le cas o√π le tirage est effectu√© sans remise, puis dans le cas o√π le tirage est effectu√© avec remise, les probabilit√©s suivantes:\n\nprobabilit√© pour que les deux boules soient blanches.\nprobabilit√© pour que les deux boules soient de m√™me couleur.\nprobabilit√© pour que l‚Äôune au moins des boules tir√©es soit blanche."
  },
  {
    "objectID": "exos1.html#bayes",
    "href": "exos1.html#bayes",
    "title": "Feuille d‚Äôexercices 1",
    "section": "Bayes",
    "text": "Bayes\n\n\nExercice 10 \n\nUn nouveau vaccin a √©t√© test√© sur 12500 personnes. 75 d‚Äôentre elles, dont 35 femmes enceintes, ont eu des r√©actions secondaires n√©cessitant une hospitalisation.\n\nSachant que ce vaccin a √©t√© administr√© √† 680 femmes enceintes, quelle est la probabilit√© qu‚Äôune femme enceinte ait eu une r√©action secondaire si elle re√ßoit le vaccin?\nQuelle est la probabilit√© qu‚Äôune personne non enceinte ait une r√©action secondaire?\nUne personne a pr√©sent√© des r√©actions secondaires, quelle est la probabilit√© qu‚Äôil s‚Äôagit d‚Äôune femme enceinte?\n\n\n\n\nExercice 11 \n\nPour se rendre au lyc√©e, un √©l√®ve a le choix entre 4 itin√©raires: A, B, C et D. La probabilit√© qu‚Äôil a de choisir A (respectivement B, C) est \\(\\frac{1}{3}\\) (respectivement \\(\\frac{1}{4}\\), \\(\\frac{1}{12}\\)). La probabilit√© d‚Äôarriver en retard en empruntant A (respectivement B, C) est \\(\\frac{1}{20}\\) (respectivement \\(\\frac{1}{10}\\), \\(\\frac{1}{5}\\)). En empruntant D, il n‚Äôest jamais en retard.\n\nQuelle est la probabilit√© que l‚Äô√©l√®ve choisisse l‚Äôitin√©raire D?\nL‚Äô√©l√®ve arrive en retard. Quelle est la probabilit√© qu‚Äôil ait emprunt√© l‚Äôitin√©raire C?"
  },
  {
    "objectID": "exos1.html#variables-al√©atoires-discr√®tes",
    "href": "exos1.html#variables-al√©atoires-discr√®tes",
    "title": "Feuille d‚Äôexercices 1",
    "section": "Variables al√©atoires discr√®tes",
    "text": "Variables al√©atoires discr√®tes\n\nExercice 12 Soit \\(X\\) une variable al√©atoire qui prend ses valeurs dans l‚Äôensemble \\(\\{- 4, 2, 3, 4, 6, 7, 10\\}\\) et dont la distribution est donn√©e par:\n\n\n\\(x_i\\)\n-4\n2\n3\n4\n6\n7\n10\n\n\n\\(P(X=x_i)\\)\n0.1\n0.2\n0.2\n0.1\n0.05\n0.2\n\\(k\\)\n\n\n\nCalculer \\(k\\).\nRepr√©senter graphiquement la loi de \\(X\\).\nCalculer les probabilit√©s suivantes:\n\n\n\\(P(X >3)\\)\n\\(P(X \\geq 3)\\)\n\\(P(3 \\leq X \\leq 7)\\)\n\\(P(3 < X < 9)\\)\n\\(P(X+2 > 3)\\)\n\\(P(X^2 > 4)\\)\n\n\n\nExercice 13 On choisit deux boules au hasard dans une urne contenant 8 boules blanches, 4 boules noires et 2 boules oranges. Supposons que l‚Äôon re√ßoive 2 euros pour chaque boule noire tir√©e et que l‚Äôon perde 1 euro pour chaque boule blanche tir√©e. D√©signons les gains nets par \\(X\\).\n\nQuelles sont les valeurs possibles pour \\(X\\) et les probabilit√©s associ√©es √† ces valeurs ?\nQuelle est l‚Äôesp√©rance de \\(X\\) ?\n\n\n\nExercice 14 Une urne contient une boule qui porte le num√©ro 0, deux qui portent le num√©ro 1 et quatre qui portent le num√©ro 3. On extrait simultan√©ment deux boules dans cette urne.\n\nD√©terminer la loi de probabilit√© de la variable al√©atoire \\(X\\) qui repr√©sente la somme des nombres obtenus.\nD√©terminer la fonction de r√©partition de \\(X\\).\nCalculer \\(E(X)\\), \\(V(X)\\) et \\(\\sigma(X)\\).\n\n\n\nExercice 15 Soit \\(X\\) une v.a. qui suit la loi uniforme (e.g.¬†√©quiprobabilit√© de valeurs de \\(X\\)) sur l‚Äôensemble \\(X(\\Omega) = \\{-3, -2, 1, 4\\}\\).\n\nDonner la loi de \\(X\\).\n\nCalculer \\(E(X)\\) et \\(V(X)\\).\nOn d√©finit la variable al√©atoire \\(Y=(X+1)^2\\).\n\nDonner \\(Y(\\Omega)\\) et la loi de \\(Y\\).\nCalculer \\(E(Y)\\) de deux fa√ßons diff√©rents.\n\n\n\nExercice 16 Soit la fonction de r√©partition \\(F\\) de la variable al√©atoire \\(X\\) d√©finie par:\n\\[F(x) = \\left\\{\n\\begin{array}{l l}\n0 & \\quad \\text{si $x<0$}\\\\\n  1/4 & \\quad \\text{si $0 \\leq x < 1$}\\\\\n   1/2 & \\quad \\text{si $1 \\leq x < 4$}\\\\\n    c & \\quad \\text{si $x \\geq 4$}\\\\\n  \\end{array} \\right.\\]\n\nD√©terminer en justifiant la constante \\(c\\).\nCalculer \\(P (1 \\leq X < 5)\\).\nCalculer \\(P(X=1)+P(X=2)\\).\nD√©teminer la loi de probabilit√© de \\(X\\).\n\n\n\nExercice 17 Soient X et Y des variables al√©atoires discr√®tes dont la loi jointe est donn√©e par le tableau suivant:\n\n\n\n\\(X\\)\\\\(Y\\)\n\n-1\n0\n2\n5\n\n\n\n0\n0.10\n0.05\n0.15\n0.05\n\n\n1\n0.15\n0.20\n0.25\n0.05\n\n\n\n\nQuelle est la loi marginale de X ?\nQuelle est la loi marginale de Y ?\nCalculer \\(P(Y \\geq 0 / X = 1)\\).\nCalculer \\(E(X)\\), \\(E(Y)\\), et \\(cov(X,Y)\\).\nLes variables \\(X\\) et \\(Y\\) sont elles ind√©pendantes ?\n\n\n\nExercice 18 Soit \\((X,Y)\\) un couple de variables al√©atoires √† valeurs dans \\(\\mathbb{N}^2\\) tel que\n\\[\\forall (p,q) \\in \\mathbb{N}^2, \\quad P(X=p,Y=q) = \\lambda \\frac{p+q}{p! q! 2^{p+q}}\\]\n\nD√©terminer \\(\\lambda\\).\nCalculer les lois marginales.\nLes variables \\(X\\) et \\(Y\\) sont elles ind√©pendantes ?"
  },
  {
    "objectID": "exos1.html#lois-usuelles-discr√®tes",
    "href": "exos1.html#lois-usuelles-discr√®tes",
    "title": "Feuille d‚Äôexercices 1",
    "section": "Lois usuelles discr√®tes",
    "text": "Lois usuelles discr√®tes\n\nExercice 19 Une urne contient 2 boules de num√©ro 20, 4 boules de num√©ro 10 et 4 boules de num√©ro 5.\n\nUne √©preuve consiste √† tirer simultan√©ment 3 boules de l‚Äôurne. Calculer la probabilit√© \\(p\\) que la somme des num√©ros tir√©s soit √©gale √† 30.\n\nOn r√©p√©te cette √©preuve 4 fois en remettant √† chaque fois les trois boules tir√©s dans l‚Äôurne. Soit \\(X\\) la v.a. indiquant le nombre de tirages donnant une somme de num√©ros √©gale √† 30.\n\nQuelle la loi de \\(X\\). Donner son esp√©rance et son √©cart-type.\nD√©terminer la probabilit√© d‚Äôavoir au moins une fois la somme 30 dans les 4 tirages.\n\n\n\n\n\nExercice 20 Vous avez besoin d‚Äôune personne pour vous aider √† d√©m√©nager. Quand vous t√©l√©phonez √† un ami, il y a une chance sur quatre qu‚Äôil accepte. Soit \\(X\\) la variable al√©atoire qui repr√©sente le nombre d‚Äôamis que vous devrez contacter pour obtenir cette aide.\n\nD√©terminer la loi de probabilit√© de \\(X\\).\nCalculer \\(P(X\\leq 3)\\).\nCalculer \\(E(X)\\).\n\n\n\nExercice 21 Pour √™tre s√©lectionn√© aux jeux olympiques, un athl√®te doit r√©ussir deux fois √† d√©passer les minima fix√©s par sa f√©d√©ration. Il a une chance sur trois de r√©ussir √† chaque √©preuve √† laquelle il participe. On note \\(X\\) la variable al√©atoire qui repr√©sente le nombre d‚Äô√©preuves auxquelles il devra participer pour √™tre s√©lectionn√©.\n\nD√©terminer la loi de probabilit√© de \\(X\\).\nSi cet athl√®te ne peut participer qu‚Äô√† quatre √©preuves maximum, quelle est la probabilit√© qu‚Äôil soit s√©lectionn√© ?\n\n\n\nExercice 22 Un sac contient cinq jetons : deux sont num√©rot√©s 1 et les trois autres sont num√©rot√©s 2. On effectue une s√©rie illimit√©e de tirages avec remise d‚Äôun jeton dans le sac S. On d√©signe par \\(Y\\) la variable al√©atoire √©gale au nombre de tirages effectu√©s avant le tirage amenant un jeton num√©rot√© 1 pour la premi√®re fois.\n\n\nJustifier que la variable al√©atoire \\(Z=Y+1\\) suit une loi usuelle que l‚Äôon pr√©cisera.\nEn d√©duire la loi de probabilit√© de \\(Y\\).\n\n\nPr√©ciser l‚Äôesp√©rance math√©matique et la variance de \\(Z\\).\nEn d√©duire l‚Äôesp√©rance math√©matique et la variance de \\(Y\\).\n\n\n\n\nExercice 23 Le nombre de pannes d‚Äô√©lectricit√© qui se produisent dans une certaine r√©gion au cours d‚Äôune p√©riode d‚Äôun an suit une loi de Poisson de param√®tre \\(\\lambda=3\\).\n\nCalculer la probabilit√© qu‚Äôau cours d‚Äôune p√©riode d‚Äôun an il y a exactement une panne qui se produit.\nEn supposant l‚Äôind√©pendance des pannes d‚Äôune ann√©e √† l‚Äôautre, calculer la probabilit√© qu‚Äôau cours des dix prochaines ann√©es il y ait au moins une ann√©e pendant laquelle il se produira exactement une panne.\n\n\n\nExercice 24 Un poste de radio a 2 types de pannes: transistor ou condensateur. Durant la premi√®re ann√©e d‚Äôutilisation, on d√©signe par:\n\\(X=\\) nombre de pannes dues √† une d√©faillance de transistor.\\(Y=\\) nombre de pannes dues √† une d√©faillance de condensateur.\nOn suppose que \\(X\\) et \\(Y\\) sont des v.a. ind√©pendantes suivant des lois de Poisson de param√®tres respectives \\(\\lambda=2\\) et \\(\\mu=1\\).\n\nCalculer la probabilit√© qu‚Äôil y ait 2 pannes dues √† une d√©faillance de transistor.\nCalculer la probabilit√© qu‚Äôil y ait au moins une panne due √† une d√©faillance de condensateur.\n\nQuelle est la loi du nombre \\(Z=X+Y\\) de pannes durant la premi√®re ann√©e ?\nD√©terminer la probabilit√© qu‚Äôil y ait 2 pannes de type quelconque.\nCalculer \\(P(Z=3)\\). Que peut-on remarquer ?\nD√©crire les variations de \\(P(Z=k)\\) en fonction de \\(k\\).\nDonner le nombre moyen de pannes et la probabilit√© qu‚Äôil y ait au plus une panne durant cette p√©riode.\n\n\n\n\n\n\n\n\n\nCorrections exercices 23 et 24"
  },
  {
    "objectID": "exos1.html#exercices-suppl√©mentaires",
    "href": "exos1.html#exercices-suppl√©mentaires",
    "title": "Feuille d‚Äôexercices 1",
    "section": "Exercices suppl√©mentaires",
    "text": "Exercices suppl√©mentaires\n\nExercice 25 Soient \\(X\\) et \\(Y\\) deux v.a.r. ind√©pendantes v√©rifiant:\n\\[P(X=n) = P(Y=n) = \\frac{1}{4} (\\frac{1+a^n}{n!}) \\quad \\forall n \\in \\mathbb{N}\\]\n\nD√©terminer \\(a\\).\nCalculer \\(E(X)\\) et \\(E(Y)\\).\nD√©terminer la loi de \\(Z=X+Y\\).\n\n\n\nExercice 26 Soit \\((X,Z)\\) un couple de variables al√©atoires √† valeurs dans \\(\\mathbb{N}\\). On pose\n\\[p_{k,n} = P(X=k,Z=n)= \\frac{\\lambda^n e^{-\\lambda} \\alpha^k (1-\\alpha)^{n-k}}{k! (n-k)!}\n\\times {1}_{\\{0\\leq k\\leq n\\}}\\]\n\nCalculer et reconnaitre la loi de \\(Z\\). Donner l‚Äôesp√©rance de \\(Z\\).\nCalculer et reconnaitre la loi de \\(X\\).\nCalculer \\(P(X=k | Z=n)\\) et reconnaitre la loi.\nOn fait l‚Äôhypoth√®se que \\(Z\\) est le nombre d‚Äôenfants d‚Äôune famille, \\(X\\) le nombre de gar√ßons et la probabilit√© qu‚Äôun enfant est un gar√ßon est \\(0.53\\). Soit \\(Y\\) le nombre de filles dans la famille. Calculer et reconnaitre la loi de \\(Y\\).\nInterpr√©ter les r√©sultats trouv√©s. Est ce par hasard qu‚Äôon reconnait ces lois?\n\n\n\nExercice 27 Un joueur dispose d‚Äôun d√© et d‚Äôune pi√®ce. Le d√© est √©quilibr√© et la pi√®ce a une probabilit√© \\(p\\) (\\(0 < p < 1\\)) de tomber sur pile. On note \\(q=1-p\\). Le joueur lance d‚Äôabord le d√©, puis lance la pi√®ce autant de fois que le r√©sultat du d√©. Il compte enfin le nombre de piles obtenu au cours des lancers. Les r√©sultats de chaque lancer sont ind√©pendants.\nOn note \\(X\\) la variable al√©atoire correspondant √† la valeur du d√© et \\(Y\\) celle correspondant au nombre de piles obtenus √† la fin du jeu.\n\nSoit \\((i,j) \\, \\in \\, \\{1,\\ldots,6\\} \\times \\{0,\\ldots,6\\}\\). Que vaut \\(P(Y=j|X=i)\\).\nD√©terminer la loi du couple \\((X,Y)\\).\nCalculer \\(P(Y=6)\\).\nMontrer que \\[P(Y=0)= \\frac{q}{6} \\big(\\frac{1-q^6}{1-q}\\big)\\]\n\n\n\n\n\n\n\n\nRappel: \\(\\displaystyle \\sum_{i=0}^{n} x^i = \\frac{1-x^{n+1}}{1-x}\\) si \\(x\\neq 1\\).\n\n\n\n\nSachant que l‚Äôon a obtenu aucun pile au cours du jeu, quelle √©tait la probabilit√© que le r√©sultat du d√© √©tait \\(1\\)? √âvaluer cette quantit√© quand \\(p=q=\\frac{1}{2}\\)."
  },
  {
    "objectID": "exos1_en.html#counting-combinatorics",
    "href": "exos1_en.html#counting-combinatorics",
    "title": "Exercises sheet 1",
    "section": "Counting (Combinatorics)",
    "text": "Counting (Combinatorics)\n\nExercice 1 Separated questions:\n\nIn computing, the binary system is used to code characters. A bit (binary digit) is an element that takes the value 0 or the value 1. With 8 binary digits (one byte), how many characters can we code?\n\n\n\n\n\nIn a sports competition grouping 18 athletes, one gold, one silver and one bronze medal are awarded. How many distributions are there possible (before the competition, of course‚Ä¶)?\n\n\n\n\n\nA sports tournament has 8 participating teams. Each team must meet all the others once. How many games must we organize?\n\n\n\n\n\n\nExercice 2 A die üé≤ is rolled 3 times in a row, and we are interested in the total of the points obtained. In how many ways can we get:\n\na total of 16.\na total of 15.\nat least 15.\n\n\n\nExercice 3 In a company, there are 12 singles among the 30 employees. We wish to make a survey: for that we choose a sample of four people in this department.\n\nHow many different samples can we obtain?\nWhat is the number of samples containing no single person?\nWhat is the number of samples containing at least one single?\n\n\n\nExercice 4 Five cards are drawn simultaneously from a deck of 52 cards.\n\nHow many different draws can we get?\n\nHow many different draws can we get? containing:\n\n5 diamonds or 5 hearts;\n2 hearts and 3 spades;\nat least 1 king\nat most 1 king."
  },
  {
    "objectID": "exos1_en.html#events",
    "href": "exos1_en.html#events",
    "title": "Exercises sheet 1",
    "section": "Events",
    "text": "Events\n\nExercice 5 Let \\(A\\),\\(B\\) and \\(C\\) be three events of a measurable space \\((\\Omega,\\mathcal{A})\\). Express in terms of \\(A\\),\\(B\\) and \\(C\\) and the operations (Union, intersection and complementary) the following events below:\n\n\\(A\\) alone (among the 3 events) occurs.\n\\(A\\) and \\(C\\) occur, but not \\(B\\).\nAll three events occur.\nAt least one of the 3 events occurs.\nNone of the three events occur."
  },
  {
    "objectID": "exos1_en.html#probability",
    "href": "exos1_en.html#probability",
    "title": "Exercises sheet 1",
    "section": "Probability",
    "text": "Probability\n\nExercice 6 Let the events \\(A\\) et \\(B\\) such that \\(P(A) = x, P(B) = y\\) and \\(P(A \\cup B) = z\\). Find the following probabilities: \\(P(A \\cap B), P(\\bar{A} \\cup \\bar{B})\\), and \\(P(A \\cap \\bar{B})\\).\n\n\nExercice 7 Let \\((\\Omega,\\mathcal{A},P)\\) a probability space. Let \\(A\\),\\(B\\) et \\(C\\) three events. Let: \\(E = A \\cap \\bar{B} \\cap \\bar{C}\\) and \\(F = A \\cap (B \\cup C)\\).\n\nShow that \\(E\\) and \\(F\\) are mutually exclusive.\nShow that \\(E \\cup F = A\\).\nKnowing that \\(P(A)=0.6; P(A\\cap B)=0.2; P(A\\cap C)=0.1\\) and \\(P(A\\cap B \\cap C)=0.05\\): Compute \\(P(F)\\) and \\(P(E)\\).\n\n\n\n\nExercice 8 \n\nAn urn contains 13 balls, 6 black, 3 white and 4 red. We draw 4 balls. Let\n\nE: ‚Äù get exactly 2 white ‚Äú.\nF: ‚Äù get exactly 2 red ‚Äú.\n\n\nWe suppose that we draw without replacement. Calculate the following probabilities: \\(P(E \\cap F), P_F (E), P_E(F)\\). Are the events E and F independent?\nRepeat the exercise, assuming that the draw is made with replacement.\n\n\n\nExercice 9 An urn contains ten balls (6 white and 4 red). Two balls are drawn at random and successively from this urn. Calculate, in the case where the draw is made without replacement, then in the case where the draw is made with replacement, the following probabilities:\n\nprobability that the two balls are white.\nprobability that the two balls are of the same color.\nprobability that at least one of the balls drawn is white."
  },
  {
    "objectID": "exos1_en.html#bayes",
    "href": "exos1_en.html#bayes",
    "title": "Exercises sheet 1",
    "section": "Bayes",
    "text": "Bayes\n\n\nExercice 10 \n\nA new vaccine was tested on 12,500 people. 75 of them, including 35 pregnant women, had adverse reactions requiring hospitalization.\n\nKnowing that this vaccine was administered to 680 pregnant women, what is the probability that a pregnant woman will have a secondary reaction if she receives the vaccine?\nWhat is the probability that a non-pregnant person would have a secondary reaction?\nA person had adverse reactions. What is the probability that this person is a pregnant woman?\n\n\n\n\nExercice 11 \n\nTo go to high school, a student has the choice between 4 routes: A, B, C and D. The probability that he chooses A (respectively B, C) is \\(\\frac{1}{3}\\) (respectively \\(\\frac{1}{4}\\), \\(\\frac{1}{12}\\)). The probability to arrive late by taking A (respectively B, C) is \\(\\frac{1}{20}\\) (respectively \\(\\frac{1}{10}\\), \\(\\frac{1}{5}\\)). By borrowing D, he is never late.\n\nWhat is the probability that the student chooses route D?\nThe student arrives late. What is the probability that he took route C?"
  },
  {
    "objectID": "exos1_en.html#discrete-random-variables",
    "href": "exos1_en.html#discrete-random-variables",
    "title": "Exercises sheet 1",
    "section": "Discrete random variables",
    "text": "Discrete random variables\n\nExercice 12 Let \\(X\\) be a random variable which takes its values in the set \\(\\{- 4, 2, 3, 4, 6, 7, 10\\}\\) and whose distribution is given by:\n\n\n\\(x_i\\)\n-4\n2\n3\n4\n6\n7\n10\n\n\n\\(P(X=x_i)\\)\n0.1\n0.2\n0.2\n0.1\n0.05\n0.2\n\\(k\\)\n\n\n\nCalculate \\(k\\).\nPlot the law of \\(X\\).\nCalculate the following probabilities:\n\n\n\\(P(X >3)\\)\n\\(P(X \\geq 3)\\)\n\\(P(3 \\leq X \\leq 7)\\)\n\\(P(3 < X < 9)\\)\n\\(P(X+2 > 3)\\)\n\\(P(X^2 > 4)\\)\n\n\n\nExercice 13 Two balls are chosen randomly from an urn containing 8 white balls, 4 black balls and 2 orange balls. Assume we receive 2 euros for each black ball drawn and we lose 1 euro for each white ball drawn. Let‚Äôs refer to the net gains by \\(X\\).\n\nWhat are the possible values for \\(X\\) and the probabilities associated with these values ?\nWhat is the expected value of \\(X\\)?\n\n\n\nExercice 14 An urn contains a ball with number 0, two with number 1 and four with number 3. Two balls are simultaneously extracted from this urn.\n\nDetermine the probability law of the random variable \\(X\\) which represents the sum of the numbers obtained.\nDetermine the distribution function of \\(X\\).\nCalculate \\(E(X)\\), \\(V(X)\\) and \\(\\sigma(X)\\).\n\n\n\nExercice 15 et \\(X\\) be a random variable that follows the uniform distribution (e.g.¬†equiprobability of values of \\(X\\)) on the set \\(X(\\Omega) = \\{-3, -2, 1, 4\\}\\).\n\nGive the law of \\(X\\).\n\nCalculate \\(E(X)\\) and \\(V(X)\\).\nWe define the random variable \\(Y=(X+1)^2\\).\n\nGive \\(Y(\\Omega)\\) and the law of \\(Y\\).\nCalculate \\(E(Y)\\) using two different methods.\n\n\n\nExercice 16 Let \\(F\\) be the distribution function of the random variable \\(X\\) defined by:\n\\[F(x) = \\left\\{\n\\begin{array}{l l}\n0 & \\quad \\text{si $x<0$}\\\\\n  1/4 & \\quad \\text{si $0 \\leq x < 1$}\\\\\n   1/2 & \\quad \\text{si $1 \\leq x < 4$}\\\\\n    c & \\quad \\text{si $x \\geq 4$}\\\\\n  \\end{array} \\right.\\]\n\nDetermine with justification the constant value \\(c\\).\nCalculate \\(P (1 \\leq X < 5)\\).\nCalculate \\(P(X=1)+P(X=2)\\).\nDetermine the probability distribution of \\(X\\).\n\n\n\nExercice 17 Let X and Y be discrete random variables whose joint distribution is given by the following table:\n\n\n\n\\(X\\)\\\\(Y\\)\n\n-1\n0\n2\n5\n\n\n\n0\n0.10\n0.05\n0.15\n0.05\n\n\n1\n0.15\n0.20\n0.25\n0.05\n\n\n\n\nWhat is the marginal distribution of X?\nWhat is the marginal distribution of Y?\nCalculate \\(P(Y \\geq 0 / X = 1)\\).\nCompute \\(E(X)\\), \\(E(Y)\\), and \\(cov(X,Y)\\).\nAre the variables \\(X\\) and \\(Y\\) independent?\n\n\n\nExercice 18 Let \\((X,Y)\\) be a pair of random variables with values in \\(\\mathbb{N}^2\\) such that\n\\[\\forall (p,q) \\in \\mathbb{N}^2, \\quad P(X=p,Y=q) = \\lambda \\frac{p+q}{p! q! 2^{p+q}}\\]\n\nDetermine \\(\\lambda\\).\nCalculate the marginal distributions.\nAre the variables \\(X\\) and \\(Y\\) independent?"
  },
  {
    "objectID": "exos1_en.html#common-discrete-distributions",
    "href": "exos1_en.html#common-discrete-distributions",
    "title": "Exercises sheet 1",
    "section": "Common discrete distributions",
    "text": "Common discrete distributions\n\nExercice 19 An urn contains 2 balls with number 20, 4 balls with number 10 and 4 balls with number 5.\n\nA random expreminet consists in drawing simultaneously 3 balls from the urn. Calculate the probability \\(p\\) that the sum of the drawn numbers is equal to 30.\n\nThis experiment is repeated 4 times, putting the three balls drawn back into the urn after each trial. Let \\(X\\) be the random variable corresponding to the number of draws giving a sum of 30.\n\nWhat is the distribution of \\(X\\). Give its expected value and standard deviation.\nCalculate the probability of having at least once the sum 30 out of 4 draws.\n\n\n\n\n\nExercice 20 You need someone to help you move. When you call a friend, there‚Äôs a one out of four chances he will accept. Let \\(X\\) be the random variable that represents the number of friends you will need to contact for getting help.\n\nWhat is the probability distribution of \\(X\\).\nCalculate \\(P (X \\leq 3)\\).\nCalculate \\(E(X)\\).\n\n\n\nExercice 21 In order to be selected for the Olympic Games, an athlete must succeed twice in exceeding the minima set by his federation. He has a one in three chance of succeeding in every event he participates in. We consider \\(X\\) the random variable that represents the number of his trials until he is selected.\n\nDetermine the probability distribution of \\(X\\).\nIf this athlete can only participate in four events maximum, what is the probability that he will be selected ?\n\n\n\nExercice 22 A bag contains five tokens: two are numbered 1 and the other three are numbered 2. An unlimited series of draws is made with a token being handed over in the bag. The random variable representing the number of draws before getting a token numbered 1 for the first time is denoted by \\(Y\\).\n\n\nJustify that the random variable \\(Z = Y + 1\\) follows a common distribtion.\nDeduce the probability distribution of \\(Y\\).\n\n\nCalculate the expected value and variance of \\(Z\\).\nCalculate the expected value and variance of \\(Y\\).\n\n\n\n\nExercice 23 The number of power outages that occur in a certain region over a one-year timeframe follows a Poisson distribution of parameter \\(\\lambda=3\\).\n\nCalculate the probability that within a period of one year only one failure occurs.\nAssuming there is an independence of number of outages from one year to another, calculate the probability that in the upcoming ten years there will be at least one year in which exactly one failure occurs.\n\n\n\nExercice 24 A radio has 2 types of failures: transistor or capacitor. During the first year of use, we note:\n\n\\(X\\) = number of failures due to transistor failure.\n\\(Y\\) = number of failures due to capacitor failure.\n\nIt is assumed that \\(X\\) and \\(Y\\) are independent random variables following Poisson distribution of respective parameters \\(\\lambda=2\\) and \\(\\mu=1\\).\n\nCalculate the probability that there are 2 failures due to a transistor failure.\nCalculate the probability that there will be at least one failure due to a capacitor failure .\n\nWhat is the distribution of \\(Z = X + Y\\), the number of failures during the first year ?\nDetermine the probability that there will be 2 failures of any type.\nCalculate \\(P (Z = 3)\\). What do you notice ?\nDescribe the variations of \\(P (Z = k)\\) in function of \\(k\\).\nGive the average number of failures and the probability that there will be at most one failure during this period.\n\n\n\n\n\n\n\n\n\nSolution exercises 23 and 24"
  },
  {
    "objectID": "exos1_en.html#exercices-suppl√©mentaires",
    "href": "exos1_en.html#exercices-suppl√©mentaires",
    "title": "Exercises sheet 1",
    "section": "Exercices suppl√©mentaires",
    "text": "Exercices suppl√©mentaires\n\nExercice 25 Let \\(X\\) and \\(Y\\) two independent random variables such that:\n\\[P(X=n) = P(Y=n) = \\frac{1}{4} (\\frac{1+a^n}{n!}) \\quad \\forall n \\in \\mathbb{N}\\]\n\nCalculate \\(a\\).\nCalculate \\(E(X)\\) and \\(E(Y)\\).\nDetermine the distribution of \\(Z=X+Y\\).\n\n\n\nExercice 26 Let \\((X,Z)\\) a pair of random variables defined on \\(\\mathbb{N}\\). Let\n\\[p_{k,n} = P(X=k,Z=n)= \\frac{\\lambda^n e^{-\\lambda} \\alpha^k (1-\\alpha)^{n-k}}{k! (n-k)!} \\times {1}_{\\{0\\leq k\\leq n\\}}\\]\n\nCalculate and recognize the distribution of \\(Z\\). What is the expected value of \\(Z\\).\nCalculate and recognize the distribution of \\(X\\).\nCalculate \\(P(X=k | Z=n)\\) and recognize its distribution.\nSuppose that \\(Z\\) is the number of kids in a famile, \\(X\\) the number of boys while the probability that a kid is a boy is \\(0.53\\). Let \\(Y\\) the number of girls in the family. What is the distribution of \\(Y\\).\nInterpret the results.\n\n\n\nExercice 27 A player has a die and a coin. The die is fait while the coin has probability \\(p\\) (\\(0 < p < 1\\)) of having a head. We note \\(q=1-p\\). This player throws first the die, then flips the coin as much as the obtained number with the die. At the end, he counts the number of obtained heads. The results of every trial are independant.\nLet \\(X\\) the number obtained from the die and \\(Y\\) the number of obtained heads at the end of the game.\n\nLet \\((i,j) \\, \\in \\, \\{1,\\ldots,6\\} \\times \\{0,\\ldots,6\\}\\). Calculate \\(P(Y=j|X=i)\\).\nDetermine the distribution of \\((X,Y)\\).\nCalculate \\(P(Y=6)\\).\nShow that \\[P(Y=0)= \\frac{q}{6} \\big(\\frac{1-q^6}{1-q}\\big)\\]\n\n\n\n\n\n\n\n\nRemark: \\(\\displaystyle \\sum_{i=0}^{n} x^i = \\frac{1-x^{n+1}}{1-x}\\) if \\(x\\neq 1\\).\n\n\n\n\nGiven that the player didn‚Äôt obtain any head during the game, what is that probability that he obtained 1 by throwing the die ? Calculate this probability if \\(p=q=\\frac{1}{2}\\)."
  },
  {
    "objectID": "variables-aleatoires-continues.html#densit√©-dune-variable-al√©atoire-continue",
    "href": "variables-aleatoires-continues.html#densit√©-dune-variable-al√©atoire-continue",
    "title": "\n3¬† Variables Al√©atoires Continues\n",
    "section": "Densit√© d‚Äôune variable al√©atoire continue",
    "text": "Densit√© d‚Äôune variable al√©atoire continue\nDans les chapitres pr√©c√©dents nous avons trait√© des variables al√©atoires discr√®tes, c‚Äôest-√†-dire de variables dont l‚Äôunivers est fini ou infini d√©nombrable. Il existe cependant des variables dont l‚Äôunivers est infini non d√©nombrable. On peut citer par exemple, l‚Äôheure d‚Äôarriv√©e d‚Äôun train √† une gare donn√©e ou encore la dur√©e de vie d‚Äôun transistor. D√©signons par \\(X\\) une telle variable.\n\nD√©finition 3.1 \\(X\\) est une variable al√©atoire continue s‚Äôil existe une fonction \\(f\\) non n√©gative d√©finie pour tout \\(x \\in \\mathbb{R}\\) et v√©rifiant pour tout ensemble \\(B\\) de nombres r√©els la propri√©t√©\n\\[\nP(X \\in B) = \\int_B f(x)dx\n\\tag{3.1}\\]\nLa fonction \\(f\\) est appel√©e densit√© de probabilit√© de la variable al√©atoire \\(X\\).\n\nTous les probl√®mes de probabilit√© relatifs √† \\(X\\) peuvent √™tre trait√©s gr√¢ce √† \\(f\\). Par exemple pour \\(B=[a,b]\\), on obtient gr√¢ce √† l‚Äô√©quation (3.1)\n\\[\nP(a\\le X \\le b) = \\int_a^bf(x)dx\n\\tag{3.2}\\]\nGraphiquement, \\(P(a\\le X \\le b)\\) est l‚Äôaire de la surface entre l‚Äôaxe de \\(x\\), la courbe correspondante √† \\(f(x)\\) et les droites \\(x=a\\) et \\(x=b\\). Voire Figure Figure¬†3.1) et Figure Figure¬†3.2).\n\n\n\n\nFigure¬†3.1: \\(P(a \\leq X \\leq B)=\\) surface gris√©e\n\n\n\n\n\n\n\n\nFigure¬†3.2: L‚Äôaire hachur√©e correspond √† des probabilit√©s. \\(f(x)\\) √©tant une fonction densit√© de probabilit√©\n\n\n\n\n\nD√©finition 3.2 Pour toute variable al√©atoire continue \\(X\\) de densit√© \\(f\\):\n\n\\(f(x) \\ge 0 \\quad \\forall \\, x \\in \\mathbb{R}\\)\n\\(\\int_{-\\infty}^{+\\infty}f(x)dx = 1\\)\nSi l‚Äôon pose \\(a=b\\) dans (3.2), il r√©sulte \\[P(X=a)=\\int_a^a f(x)dx = 0\\]\n\nCeci siginifie que la probabilit√© qu‚Äôune variable al√©atoire continue prenne une valeur isol√©e fixe est toujours nulle. Aussi on peut √©crire\n\\[P(X < a) = P( X \\le a)  = \\int_{-\\infty}^a f(x)dx\\]\n\n\n\n\n\n\n\n\n\nExercice\n\n\n\nSoit \\(X\\) la variable al√©atoire r√©elle de densit√© de probabilit√©\n\\[f(x)= \\left\\lbrace\n      \\begin{array}{ll}\n      kx  & \\mbox{si} \\quad 0\\le x \\le 5\\\\\n      0 & \\mbox{sinon}\n      \\end{array}\n  \\right.\\]\n\nCalculer \\(k\\).\nCalculer: \\(P(1 \\le X \\le 3), P(2 \\le X \\le 4)\\) et \\(P(X < 3)\\).\n\n\n\n\n\n\n\n\n\nExercice\n\n\n\nSoit \\(X\\) une variable al√©atoire r√©elle continue ayant pour densit√© de probabilit√© \\[f(x)= \\left\\lbrace\n      \\begin{array}{ll}\n      \\frac{1}{6} x + k  & \\mbox{si} \\quad 0\\le x \\le 3\\\\\n      0 & \\mbox{sinon}\n      \\end{array}\n  \\right.\\]\n\nCalculer \\(k\\).\nCalculer \\(P(1 \\le X \\le 2)\\)"
  },
  {
    "objectID": "variables-aleatoires-continues.html#fonction-de-r√©partition-dune-v.a.c",
    "href": "variables-aleatoires-continues.html#fonction-de-r√©partition-dune-v.a.c",
    "title": "\n3¬† Variables Al√©atoires Continues\n",
    "section": "Fonction de r√©partition d‚Äôune v.a.c",
    "text": "Fonction de r√©partition d‚Äôune v.a.c\n\nD√©finition 3.3 Si comme pour les variables al√©atoires discr√®tes, on d√©finit la fonction de r√©partition de \\(X\\) par:\n\\[\\begin{aligned}\n    F_X \\colon  \\mathbb{R} &\\longrightarrow \\mathbb{R} \\\\\n                x &\\longmapsto F_X(a) = P(X \\le a)\\end{aligned}\\]\nalors la relation entre la fonction de r√©partition \\(F_X\\) et la fonction densit√© de probabilit√© \\(f(x)\\) est la suivante:\n\\[\\forall \\quad a \\in \\mathbb{R} \\quad F_X(a)= P(X \\le a) = \\int_{-\\infty}^a f(x)dx\\]\n\nLa fonction de r√©partition \\(F_X(a)\\) est la primitive de la fonction densit√© de probabilit√© \\(f(x)\\) (donc la densit√© d‚Äôune v.a.c est la d√©riv√©e de la fonction de r√©partition), et permet d‚Äôobtenir les probabilit√©s associ√©es √† la variable al√©atoire \\(X\\), en effet:\nPropri√©t√©s: Pour une variable al√©atoire continue X:\n\n\\(F'_X(x) = \\frac{\\text{d}}{\\text{d} x} F_X(x) = f(x)\\).\nPour tous r√©els \\(a \\le b\\), \\[\\begin{aligned}\n      P(a < X < b)      & = P(a < X \\le b) \\\\\n                        & = P(a \\le X < b) \\\\\n                        & = P( a \\le X \\le b) \\\\\n                        & = F_X(b) - F_X(a) = \\int_a^bf(x)dx\n    \\end{aligned}\\]\n\nLa fonction de r√©partition correspond aux probabilit√©s cumul√©es associ√©es √† la variable al√©atoire continue sur l‚Äôintervalle d‚Äô√©tude (Figure Figure¬†3.3)).\n\n\n\n\nFigure¬†3.3: L‚Äôaire hachur√©e en vert sous la courbe de la fonction densit√© de probabilit√© correspond √† la probabilit√© \\(P ( X < a ) = F_X ( a )\\) et vaut 0.5 car ceci correspond exactement √† la moiti√© de l‚Äôaire totale sous la courbe\n\n\n\n\nPropri√©t√©s: Les propri√©t√©s associ√©es √† la fonction de r√©partition sont les suivantes:\n\n\\(F_X\\) est continue sur \\(\\mathbb{R}\\), d√©rivable en tout point o√π \\(f\\) est continue.\n\\(F_X\\) est croissante sur \\(\\mathbb{R}\\).\n\\(F_X\\) est √† valeurs dans \\([0,1]\\).\n\\(\\lim\\limits_{x\\to - \\infty} F_X(x) = 0\\) et \\(\\lim\\limits_{x\\to +\\infty} F_X(x) = 1\\)."
  },
  {
    "objectID": "variables-aleatoires-continues.html#fonction-dune-variable-al√©atoire-continue",
    "href": "variables-aleatoires-continues.html#fonction-dune-variable-al√©atoire-continue",
    "title": "\n3¬† Variables Al√©atoires Continues\n",
    "section": "Fonction d‚Äôune variable al√©atoire continue",
    "text": "Fonction d‚Äôune variable al√©atoire continue\nSoit \\(X\\) une variable al√©atoire continue de densit√© \\(f_X\\) et de fonction de r√©partition \\(F_X\\). Soit \\(h\\) une fonction continue d√©finie sur \\(X(\\Omega)\\), alors \\(Y=h(X)\\) est une variable al√©atoire.\nPour d√©terminer la densit√© de \\(Y\\), not√©e \\(f_Y\\), on commence par calculer la fonction de r√©partition de \\(Y\\), not√©e \\(F_Y\\), ensuite nous d√©rivons pour d√©terminer \\(f_Y\\).\nCalcul de densit√©s pour \\(h(X)=aX+b\\)\n\n\\(\\forall \\quad y \\in \\mathbb{R}\\),\n\\[F_Y(y) = P(Y\\leq y)=P(h(X) \\le y) = P(aX+b \\le y)\\] si \\(a>0\\), \\[F_Y(y) = P(aX+b \\le y) = P(X\\leq \\frac{y-b}{a})=F_X(\\frac{y-b}{a})\\] si \\(a<0\\), \\[F_Y(y) = P(aX+b \\le y) =P(X\\geq \\frac{y-b}{a})=1-F_X(\\frac{y-b}{a})\\]\nEn d√©rivant on obtient la densit√© de \\(Y\\) \\[f_Y(y)=\\frac{1}{|a|}f_X(\\frac{y-b}{a})\\]\nCalcul de densit√©s pour \\(h(X)=X^2\\)\n\nSi \\(y<0\\), \\(F_Y(y) =P(Y\\leq y)=0\\).\nSi \\(y>0\\),\n\\[\n\\begin{align}\nF_Y(y)  &=P(Y\\leq y)=P(X^2 \\le y)\\\\\n        &=P(-\\sqrt{y}\\leq X \\leq \\sqrt{y}) \\\\\n        &=F_X(\\sqrt{y})-F_X(-\\sqrt{y})\n\\end{align}\\]\nEn d√©rivant on obtient la densit√© de \\(Y\\),\n\\[f_Y(y)= \\left\\lbrace\n      \\begin{array}{ll}\n      \\displaystyle \\frac{1}{2\\sqrt{y}}\\big[f_X(\\sqrt{y})+f_X(-\\sqrt{y})\\big]  & \\mbox{si} \\quad y \\ge 0\\\\\n      0 & \\mbox{sinon}\n      \\end{array}\n  \\right.\\]\nCalcul de densit√©s pour \\(h(X)=e^X\\)\n\nSi \\(y<0\\), \\(F_Y(y) = P(Y\\leq y)=0\\).\nSi \\(y>0\\), \\(F_Y(y) = P(Y\\leq y)=P(e^X \\le y)=P( X \\leq \\ln (y))=F_X(\\ln(y))\\).\nEn d√©rivant on obtient la densit√© de \\(Y\\)\n\\[f_Y(y)= \\left\\lbrace\n      \\begin{array}{ll}\n      \\displaystyle \\frac{1}{y} f\\big(\\ln (y)\\big)  & \\mbox{si} \\quad y \\ge 0\\\\\n      0 & \\mbox{sinon}\n      \\end{array}\n  \\right.\\]\n\n\n\n\n\n\nExercice\n\n\n\nSoit la v.a.c \\(X\\) ayant la fonction de densit√©\n\\[f_X(x)= \\left\\lbrace\n      \\begin{array}{ll}\n      2 x   & \\mbox{si} \\quad 0 \\le x \\le 1\\\\\n      0 & \\mbox{sinon}\n      \\end{array}\n  \\right.\\]\nD√©terminer la densit√© de: \\(Y=3X+1\\), \\(Z=X^2\\) et \\(T=e^X\\)."
  },
  {
    "objectID": "variables-aleatoires-continues.html#esp√©rance-et-variance-de-variables-al√©atoires-continues",
    "href": "variables-aleatoires-continues.html#esp√©rance-et-variance-de-variables-al√©atoires-continues",
    "title": "\n3¬† Variables Al√©atoires Continues\n",
    "section": "Esp√©rance et variance de variables al√©atoires continues",
    "text": "Esp√©rance et variance de variables al√©atoires continues\nEsp√©rance d‚Äôune v.a.c\n\nD√©finition 3.4 Si \\(X\\) est une variable al√©atoire absolument continue de densit√© \\(f\\), on appelle esp√©rance de X, le r√©el \\(E(X)\\), d√©fini par:\n\\[E(X)= \\int_{-\\infty}^{+\\infty}x f(x) dx\\] si cette int√©grale est convergente.\n\nLes propri√©t√©s de l‚Äôesp√©rance d‚Äôune variable al√©atoire continue sont les m√™mes que pour une variable al√©atoire discr√®te.\nPropri√©t√©s: Soit \\(X\\) une variable al√©atoire continue,\n\n\\(E(aX+b)=aE(X)+b \\quad \\quad a \\ge 0 \\,\\, \\text{et} \\,\\, b \\in \\mathbb{R}\\).\nSi \\(X \\ge 0\\) alors \\(E(X) \\ge 0\\).\nSi \\(X\\) et \\(Y\\) sont deux variables al√©atoires d√©finies sur un m√™me univers \\(\\Omega\\) alors \\[E(X+Y)=E(X)+E(Y)\\]\n\n\nTh√©or√®me 3.1 (Th√©or√®me du transfert) Si \\(X\\) est une variable al√©atoire de densit√© \\(f(x)\\), alors pour toute fonction r√©elle \\(g\\) on aura\n\\[E[g(X)] = \\int_{-\\infty}^{+\\infty}g(x) f(x) dx\\]\n\n\n\n\n\n\n\nExercice\n\n\n\nSoit la v.a.c \\(X\\) ayant la fonction de densit√©\n\\[f_X(x)= \\left\\lbrace\n      \\begin{array}{ll}\n      2 x   & \\mbox{si} \\quad 0 \\le x \\le 1\\\\\n      0 & \\mbox{sinon}\n      \\end{array}\n  \\right.\\]\nCalculer l‚Äôesp√©rance des variables al√©atoires \\(Y=3X+1\\), \\(Z=X^2\\) et \\(T=e^X\\).\n\n\nVariance d‚Äôune v.a.c\nLa variance d‚Äôune variable al√©atoire \\(V(X)\\) est l‚Äôesp√©rance math√©matique du carr√© de l‚Äô√©cart √† l‚Äôesp√©rance math√©matique. C‚Äôest un param√®tre de dispersion qui correspond au moment centr√© d‚Äôordre 2 de la variable al√©atoire \\(X\\).\n\nD√©finition 3.5 Si \\(X\\) est une variable al√©atoire ayant une esp√©rance \\(E(X)\\), on appelle variance de \\(X\\) le r√©el\n\\[V(X)=E\\big([X-E(X)]^2\\big) = E(X^2) - [E(X)]^2\\] Si \\(X\\) est une variable al√©atoire continue, on calcule \\(E(X^2)\\) en utilisant le th√©or√®me @ref(thm:transfert),\n\\[E(X^2) = \\int_{-\\infty}^{+\\infty}x^2 f(x)dx\\]\n\nPropri√©t√©s: Si \\(X\\) est une variable al√©atoire admettant une variance alors:\n\n\\(V(X) \\ge 0\\), si elle existe.\n\\(\\forall \\quad a \\in \\mathbb{R}, V(aX) = a^2 V(X)\\)\n\\(\\forall \\quad (a,b) \\in \\mathbb{R}, V(aX+b) = a^2 V(X)\\)\nSi \\(X\\) et \\(Y\\) sont deux variables al√©atoires ind√©pendantes, \\(V(X+Y)=V(X)+V(Y)\\)\n\n\nD√©finition 3.6 (Ecart-type) Si \\(X\\) est une variable al√©atoire ayant une variance \\(V(X)\\), on appelle √©cart-type de \\(X\\), le r√©el:\n\\[\\sigma_X = \\sqrt{V(X)}\\]"
  },
  {
    "objectID": "lois-usuelles-continues.html#loi-uniforme-uab",
    "href": "lois-usuelles-continues.html#loi-uniforme-uab",
    "title": "\n4¬† Lois usuelles continues\n",
    "section": "Loi uniforme \\(U(a,b)\\)\n",
    "text": "Loi uniforme \\(U(a,b)\\)\n\nLa loi uniforme est la loi exacte de ph√©nom√®nes continus uniform√©ment r√©partis sur un intervalle.\n\nD√©finition 4.1 La variable al√©atoire \\(X\\) suit une loi uniforme sur le segment \\([a,b]\\) avec \\(a < b\\) si sa densit√© de probabilit√© est donn√©e par \\[f(x)= \\left\\lbrace\n      \\begin{array}{ll}\n      \\frac{1}{b-a}   & \\mbox{si} \\quad x \\in [a,b]\\\\\n      0 & \\mbox{si} \\quad x \\notin [a,b]\n      \\end{array}\n  \\right. = \\frac{1}{b-a} {1}_{[a,b]}(x)\\]\n\n\n\n\n\nFigure¬†4.1: Fonction de densit√© de \\(U([a,b]\\))\n\n\n\n\nQuelques commentaires:\n\nLa loi uniforme continue √©tant une loi de probabilit√©, l‚Äôaire hachur√©e en bleu sur la Figure Figure¬†4.1) vaut \\(1\\).\nLa fonction de r√©partition associ√©e √† la loi uniforme continue est \\[F_X(x)= \\left\\lbrace\n       \\begin{array}{ll}\n       0 & \\mbox{si} \\quad x < a \\\\\n       \\displaystyle \\frac{x-a}{b-a}   & \\mbox{si} \\quad  a \\le x \\le b \\\\\n       1 & \\mbox{si} \\quad x > b\n       \\end{array}\n   \\right.\\]\n\nPropri√©t√©s: Si \\(X\\) est une v.a.c qui suit la loi uniforme sur \\([a,b]\\):\n\n\\(E(X) = \\displaystyle \\frac{b+a}{2}\\)\n\\(V(X) = \\displaystyle \\frac{(b-a)^2}{12}\\)\n\n\n\n\n\n\n\nExercice\n\n\n\nSoit \\(X \\thicksim U(0,10)\\). Calculer:\n\n\\(P(X <3)\\)\n\\(P(X\\ge 6)\\)\n\\(P(3 < X < 8)\\)"
  },
  {
    "objectID": "lois-usuelles-continues.html#loi-exponentielle-mathcalelambda",
    "href": "lois-usuelles-continues.html#loi-exponentielle-mathcalelambda",
    "title": "\n4¬† Lois usuelles continues\n",
    "section": "Loi exponentielle \\(\\mathcal{E}(\\lambda)\\)\n",
    "text": "Loi exponentielle \\(\\mathcal{E}(\\lambda)\\)\n\n\nD√©finition 4.2 On dit qu‚Äôune variable al√©atoire \\(X\\) est exponentielle (ou suit la loi exponentielle) de param√®tre \\(\\lambda\\) si sa densit√© est donn√©e par\n\\[f(x)= \\left\\lbrace\n      \\begin{array}{ll}\n      \\lambda e^{- \\lambda x}   & \\mbox{si} \\quad x \\ge 0\\\\\n      0 & \\mbox{si} \\quad x < 0\n      \\end{array}\n  \\right. = \\lambda e^{- \\lambda x} {1}_{\\mathbb{R}^{+}}(x)\\]\nOn dit \\(X \\thicksim \\mathcal{E}(\\lambda)\\)\n\nLa fonction de r√©partition \\(F\\) d‚Äôune variable al√©atoire exponentielle est donn√©e par\n\\[\\begin{align}\n\\mbox{Si}\\,\\, x \\ge 0 \\quad F(x) &= P(X \\le x) = \\int_0^x f(t)dt \\\\\n     &= \\int_0^x \\lambda e^{- \\lambda t} dt  \\\\\n     &= \\big[ -e^{- \\lambda t} \\big]_0^x = 1-e^{- \\lambda x}\n\\end{align}\\]\nPropri√©t√©s: Si \\(X \\thicksim \\mathcal{E}(\\lambda)\\)\n\n\\(E(X) = \\displaystyle \\frac{1}{\\lambda}\\)\n\\(V(X)= \\displaystyle \\frac{1}{\\lambda^2}\\)\n\n\n\n\n\n\n\nNote\n\n\n\nCas d‚Äôutilisations de la loi exponentielle : Dans la pratique, on rencontre souvent la distribution exponentielle lorsqu‚Äôil s‚Äôagit de repr√©senter le temps d‚Äôattente avant l‚Äôarriv√©e d‚Äôun √©v√©nement sp√©cifi√©. Une loi exponentielle mod√©lise la dur√©e de vie d‚Äôun ph√©nom√®ne sans m√©moire, ou sans vieillissement, ou sans usure. En d‚Äôautres termes, le fait que le ph√©nom√®ne ait dur√© pendant un temps \\(t\\) ne change rien √† son esp√©rance de vie √† partir du temps \\(t\\). On dit qu‚Äôune variable al√©atoire non n√©gative \\(X\\) est sans m√©moire lorsque\n\\[P(X > t+h | X > t) = P(X > h) \\quad \\quad \\forall \\quad t,h \\ge 0\\]\nPar exemple, la dur√©e de vie de la radioactivit√© ou d‚Äôun composant √©lectronique, le temps qui nous s√©pare d‚Äôun prochain tremblement de terre ou du prochain appel t√©l√©phonique mal aiguill√© sont toutes des variables al√©atoires dont les distributions tendent en pratique √† se rapprocher de distributions exponentielles."
  },
  {
    "objectID": "lois-usuelles-continues.html#loi-normale-ou-de-laplace-gauss-mathcalnmusigma2",
    "href": "lois-usuelles-continues.html#loi-normale-ou-de-laplace-gauss-mathcalnmusigma2",
    "title": "\n4¬† Lois usuelles continues\n",
    "section": "Loi Normale ou de Laplace-Gauss \\(\\mathcal{N}(\\mu,\\sigma^2)\\)\n",
    "text": "Loi Normale ou de Laplace-Gauss \\(\\mathcal{N}(\\mu,\\sigma^2)\\)\n\n\nD√©finition 4.3 Une variable al√©atoire \\(X\\) est dite normale avec param√®tres \\(\\mu\\) et \\(\\sigma^2\\) si la densit√© de \\(X\\) est donn√©e par\n\\[f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{ \\displaystyle -(x - \\mu)^2/2\\sigma^2} \\quad \\quad \\forall \\,\\, x \\in \\mathbb{R}\\]\nAvec \\(\\mu \\in \\mathbb{R}\\) et \\(\\sigma \\in \\mathbb{R}^{+}\\). On dit que \\(X \\thicksim \\mathcal{N}(\\mu,\\sigma^2)\\).\n\nRemarque: On admet que \\(\\int_{-\\infty}^{+\\infty}f(x)dx = 1\\) dans la mesure o√π l‚Äôint√©gration analytique est impossible.\n√âtude de la densit√© de la loi Normale\n\nLa fonction \\(f\\) est paire autour d‚Äôun axe de sym√©trie \\(x = \\mu\\) car \\(f(x + \\mu ) = f(\\mu - x)\\).\n\\(f'(x)=0\\) pour \\(x=\\mu\\), \\(f'(x) < 0\\) pour \\(x < \\mu\\) et \\(f'(x) > 0\\) pour \\(x > \\mu\\)\n\n\n\n\n\nFigure¬†4.2: Repr√©sentation graphique de la densit√© d‚Äôune loi normale. Remarque: Le param√®tre \\(\\mu\\) repr√©sente l‚Äôaxe de sym√©trie et œÉ le degr√© d‚Äôaplatissement de la courbe de la loi normale dont la forme est celle d‚Äôune courbe en cloche\n\n\n\n\nPropri√©t√©s: Soit \\(X \\thicksim \\mathcal{N}(\\mu,\\sigma^2)\\), on a:\n\n\\(E(X)=\\mu\\)\n\\(V(X)=\\sigma^2\\)\n\n\nTh√©or√®me 4.1 (Stabilit√© de la loi normale) Soit \\(X_1\\) et \\(X_2\\) deux variables al√©atoires normales et ind√©pendantes de param√®tres respectifs \\((\\mu_1,\\sigma_1^2)\\) et \\((\\mu_2,\\sigma_2^2)\\), alors leur somme \\(X_1+X_2\\) est une variable al√©atoire normale de param√®tres \\((\\mu_1 + \\mu_2,\\sigma_1^2+\\sigma_2^2)\\)."
  },
  {
    "objectID": "lois-usuelles-continues.html#loi-normale-centr√©e-r√©duite-mathcaln01",
    "href": "lois-usuelles-continues.html#loi-normale-centr√©e-r√©duite-mathcaln01",
    "title": "\n4¬† Lois usuelles continues\n",
    "section": "Loi Normale centr√©e r√©duite \\(\\mathcal{N}(0,1)\\)\n",
    "text": "Loi Normale centr√©e r√©duite \\(\\mathcal{N}(0,1)\\)\n\n\nD√©finition 4.4 Une variable al√©atoire continue \\(X\\) suit une loi normale centr√©e r√©duite si sa densit√© de probabilit√© est donn√©e par\n\\[\nf(x) =  \\frac{1}{{\\sqrt {2\\pi } }}e^{\\displaystyle - \\frac{1}{2} x^2} \\quad \\quad \\forall \\,\\, x \\in \\mathbb{R}\n\\]\nOn dit \\(X \\thicksim \\mathcal{N}(0,1)\\).\n\nRemarque: \\(E(X)=0\\) et \\(V(X)=1\\).\n\n\n\n\nFigure¬†4.3: Densit√© d‚Äôune loi normale centr√©e r√©duite \\(\\mathcal{N}(0,1)\\)\n\n\n\n\n\n\n\n\nFigure¬†4.4: Fonction de r√©partition de \\(\\mathcal{N}(0,1)\\)"
  },
  {
    "objectID": "lois-usuelles-continues.html#relation-entre-loi-normale-et-loi-normale-centr√©e-r√©duite",
    "href": "lois-usuelles-continues.html#relation-entre-loi-normale-et-loi-normale-centr√©e-r√©duite",
    "title": "\n4¬† Lois usuelles continues\n",
    "section": "Relation entre loi normale et loi normale centr√©e r√©duite",
    "text": "Relation entre loi normale et loi normale centr√©e r√©duite\n\nTh√©or√®me 4.2 (Relation avec la loi normale) Si \\(X\\) suit une loi normale \\(\\mathcal{N}(\\mu,\\sigma^2)\\), alors \\(\\displaystyle Z= \\frac{X-\\mu}{\\sigma}\\) est une variable centr√©e r√©duite qui suit la loi normale centr√©e r√©duite \\(\\mathcal{N}(0,1)\\)."
  },
  {
    "objectID": "lois-usuelles-continues.html#calcul-des-probabilit√©s-dune-loi-normale",
    "href": "lois-usuelles-continues.html#calcul-des-probabilit√©s-dune-loi-normale",
    "title": "\n4¬† Lois usuelles continues\n",
    "section": "Calcul des probabilit√©s d‚Äôune loi normale",
    "text": "Calcul des probabilit√©s d‚Äôune loi normale\nLa fonction de r√©partition de la loi normale r√©duite permet d‚Äôobtenir les probabilit√©s associ√©es √† toutes variables al√©atoires normales \\(\\mathcal{N}(\\mu,\\sigma^2)\\) apr√®s transformation en variable centr√©e r√©duite.\n\nD√©finition 4.5 On appelle fonction \\(\\Phi\\), la fonction de r√©partition de la loi normale centr√©e r√©duite \\(\\mathcal{N}(0,1)\\), telle que\n\\[\\forall \\,\\, x \\in \\mathbb{R} \\quad \\Phi(x) = P(X \\le x) =  \\frac{1}{{\\sqrt {2\\pi}}} \\int_{-\\infty}^x f(t)dt\\]\n\nPropri√©t√©s: Les propri√©t√©s associ√©es √† la fonction de r√©partition \\(\\Phi\\) sont:\n\n\n\\(\\Phi\\) est croissante, continue et d√©rivable sur \\(\\mathbb{R}\\) et v√©rifie:\n\\(\\lim\\limits_{x\\to - \\infty} \\Phi(x) = 0\\) et \\(\\lim\\limits_{x\\to\\infty} \\Phi(x) = 1\\)\n\n\\(\\forall \\,\\, x \\in \\mathbb{R} \\quad \\Phi(x) + \\Phi(-x) = 1\\)\n\\(\\forall \\,\\, x \\in \\mathbb{R} \\quad \\Phi(x) - \\Phi(-x) = 2\\Phi(x) -1\\)\n\nUne application directe de la fonction \\(\\Phi\\) est la lecture des probabilit√©s de la loi normale sur la table de la loi normale centr√©e r√©duite.\n\n\n\n\n\n\nExercice\n\n\n\nSoit \\(X\\) une variable al√©atoire normale de param√®tres \\(\\mu =3\\) et \\(\\sigma^2=4\\). Calculer:\n\n\\(P(X > 0)\\)\n\\(P(2 < X < 5)\\)\n\\(P(|X-3| > 4)\\)"
  },
  {
    "objectID": "lois-usuelles-continues.html#approximation-normale-dune-r√©partition-binomiale",
    "href": "lois-usuelles-continues.html#approximation-normale-dune-r√©partition-binomiale",
    "title": "\n4¬† Lois usuelles continues\n",
    "section": "Approximation normale d‚Äôune r√©partition binomiale",
    "text": "Approximation normale d‚Äôune r√©partition binomiale\nUn r√©sultat important de la th√©orie de probabilit√© est connu sous le nom de th√©or√®me limite de Moivre-Laplace. Il dit que pour \\(n\\) grand, une variable binomiale \\(\\mathcal{B}(n,p)\\) suivra approximativement la m√™me loi qu‚Äôune variable al√©atoire normale avec m√™me moyenne et m√™me variance. Ce th√©or√®me √©nonce que si ‚Äúon standardise‚Äù une variable al√©atoire binomiale \\(\\mathcal{B}(n,p)\\) en soustrayant d‚Äôabord sa moyenne \\(np\\) puis en divisant le r√©sultat par son √©cart-type \\(\\sqrt{np(1-p)}\\), alors la variable al√©atoire standardis√©e (de moyenne 0 et variance 1) suivra approximativement, lorsque \\(n\\) est grand, une distribution normale standard. Ce r√©sultat fut ensuite progressivement g√©n√©ralis√© par Laplace, Gauss et d‚Äôautres pour devenir le th√©or√®me actuellement connu comme th√©or√®me centrale limite qui est un des deux r√©sultats les plus importants de la th√©orie de probabilit√©s. Ce th√©or√®me sert de base th√©orique pour expliquer un fait empirique souvent relev√©, √† savoir qu‚Äôen pratique de tr√®s nombreux ph√©nom√®nes al√©atoires suivent approximativement une distribution normale.\nOn remarquera qu‚Äô√† ce stade deux approximations de la r√©partition binomiale ont √©t√© propos√©es: l‚Äôapproximation de Poisson, satisfaisante lorsque \\(n\\) est grand et lorsque \\(np\\) n‚Äôest pas extr√™me; l‚Äôapproximation normale pour laquelle on peut montrer qu‚Äôelle est de bonne qualit√© lorsque \\(np(1-p)\\) est grand (d√®s que \\(np(1-p)\\) d√©passe 10).\n\n\n\n\nFigure¬†4.5: La loi de probabilit√© d‚Äôune variable al√©atoire \\(B( n,p )\\) devient de plus en plus normale √† mesure que \\(n\\) augmente."
  },
  {
    "objectID": "lois-usuelles-continues.html#loi-de-chi2-de-pearson",
    "href": "lois-usuelles-continues.html#loi-de-chi2-de-pearson",
    "title": "\n4¬† Lois usuelles continues\n",
    "section": "Loi de \\(\\chi^{2}\\) de Pearson",
    "text": "Loi de \\(\\chi^{2}\\) de Pearson\n\nD√©finition 4.6 Soit \\(X_1,X_2,\\ldots,X_n\\), \\(n\\) variables normales centr√©es r√©duites, et \\(Y\\) la variable al√©atoire d√©finie par\n\\[Y = X_1^2 + X_2^2 + \\ldots + X_i^2 + \\ldots + X_n^2 = \\sum_{i=1}^n X_i^2\\] On dit que \\(Y\\) suit la loi de \\(\\chi^2\\) (ou loi de Pearson) √† \\(n\\) degr√©s de libert√©, \\(Y \\thicksim \\chi^2 (n)\\)\n\n\n\n\n\n\n\nNote\n\n\n\nLa loi de \\(\\chi^2\\) trouve de nombreuses applications dans le cadre de la comparaison de proportions, des tests de conformit√© d‚Äôune distribution observ√©e √† une distribution th√©orique et le test d‚Äôind√©pendance de deux caract√®res qualitatifs. Ce sont les tests du khi-deux.\n\n\nRemarque: Si \\(n=1\\), la variable du \\(\\chi^2\\) correspond au carr√© d‚Äôune variable normale centr√©e r√©duite \\(\\mathcal{N}(0,1)\\).\nPropri√©t√©s: Si \\(Y \\thicksim \\chi^2 (n)\\), alors:\n\n\\(E(Y)= n\\)\n\\(V(Y) = 2n\\)"
  },
  {
    "objectID": "lois-usuelles-continues.html#loi-de-student-stn",
    "href": "lois-usuelles-continues.html#loi-de-student-stn",
    "title": "\n4¬† Lois usuelles continues\n",
    "section": "Loi de Student \\(St(n)\\)\n",
    "text": "Loi de Student \\(St(n)\\)\n\n\nD√©finition 4.7 Soit \\(U\\) une variable al√©atoire suivant une loi normale centr√©e r√©duite \\(\\mathcal{N}(0,1)\\) et \\(V\\) une variable al√©atoire suivant une loi de \\(\\chi^2(n)\\), \\(U\\) et \\(V\\) √©tant ind√©pendantes, on dit alors que \\(\\displaystyle T_n = \\frac{U}{\\sqrt{\\frac{V}{n}}}\\) suit une loi de Student √† \\(n\\) degr√©s de libert√©. \\(T_n \\thicksim St(n)\\)\n\n\n\n\n\n\n\nNote\n\n\n\nLa loi de Student est utilis√©e lors des tests de comparaison de param√®tres comme la moyenne et dans l‚Äôestimation de param√®tres de la population √† partir de donn√©es sur un √©chantillon (Test de Student)."
  },
  {
    "objectID": "lois-usuelles-continues.html#loi-de-fisher-snedecor-mathcalfnm",
    "href": "lois-usuelles-continues.html#loi-de-fisher-snedecor-mathcalfnm",
    "title": "\n4¬† Lois usuelles continues\n",
    "section": "Loi de Fisher-Snedecor \\(\\mathcal{F}(n,m)\\)\n",
    "text": "Loi de Fisher-Snedecor \\(\\mathcal{F}(n,m)\\)\n\n\nD√©finition 4.8 Soit \\(U\\) et \\(V\\) deux variables al√©atoires ind√©pendantes suivant une loi de \\(\\chi^2\\) respectivement √† \\(n\\) et \\(m\\) degr√©s de libert√©.\nOn dit que \\(\\displaystyle F= \\frac{U/n}{V/m}\\) suit une loi de Fisher-Snedecor √† \\((n,m)\\) degr√©s de libert√©. \\(F \\thicksim \\mathcal{F}(n,m)\\)\n\n\n\n\n\n\n\nNote\n\n\n\nLa loi de Fisher-Snedecor est utilis√©e pour comparer deux variances observ√©es et sert surtout dans les tr√®s nombreux tests d‚Äôanalyse de variance et de covariance."
  },
  {
    "objectID": "exos2.html#variables-al√©toires-continues",
    "href": "exos2.html#variables-al√©toires-continues",
    "title": "Feuille d‚Äôexercices 2",
    "section": "Variables al√©toires continues",
    "text": "Variables al√©toires continues\n\nExercice 1 Soit \\(X\\) une v.a.c. de densit√© \\(f\\) d√©finie par: \\(f(x) = k x \\times {1}_{]0,2[} (x)\\).\n\nD√©terminer la constante \\(k\\).\nCalculer \\(E(X)\\) et \\(E(X^2)\\).\nOn pose \\(Z=X^2\\). D√©terminer la densit√© de \\(Z\\). Calculer \\(E(Z)\\).\n\n\n\nExercice 2 Soit \\(X\\) une variable al√©atoire continue dont la fonction de densit√© est donn√©e par:\n\\[f(x)= \\left\\lbrace\n      \\begin{array}{ll}\n      c(1-x^2)  & \\mbox{si} \\quad -1<x<1\\\\\n      0 & \\mbox{sinon}\n      \\end{array}\n\\right.\n\\]\n\nQuelle est la valeur de \\(c\\)?\nQuelle est la fonction de r√©partition de \\(X\\)?\n\n\n\nExercice 3 Soit \\(X\\) une variable al√©atoire continue dont la fonction de densit√© est donn√©e par:\n\\[\nf(x)= \\left\\lbrace\n      \\begin{array}{ll}\n      0  & \\mbox{si} \\quad |x| > k > 0\\\\\n      x+1 & \\mbox{si} \\quad |x| \\le k\n      \\end{array}\n  \\right.\n\\]\n\nD√©terminer \\(k\\).\nCalculer \\(E(X)\\) et \\(E(X^2)\\).\nD√©terminer la fonction de r√©partition de \\(X\\).\nSoit \\(Y=X^2\\). D√©terminer la fonction de r√©partition ainsi que la fonction de densit√© de \\(Y\\).\nCalculer \\(E(Y)\\).\n\n\n\nExercice 4 (Variable al√©atoire de densit√© paire) Soit \\(X\\) une variable al√©atoire r√©elle admettant une fonction paire \\(f\\) pour densit√©.\n\nCalculer \\(P(X \\le 0)\\) et \\(P(X\\ge 0)\\).\nMontrer que la fonction de r√©partition \\(F\\) de \\(X\\) v√©rifie: \\(\\forall \\, x \\in \\mathbb{R}, F(x)=1-F(-x)\\).\nOn admet que \\(X\\) admet une esp√©rance, calculer \\(E(X)\\).\nDonner un exemple de densit√© paire.\n\n\n\nExercice 5 (Loi de Laplace) Soit \\(c > 0\\), une constante r√©elle. Soit \\(f\\) la fonction d√©finie sur \\(\\mathbb{R}\\) par:\n\\[ \\forall \\, x \\in  \\mathbb{R}, \\quad f(x) = \\frac{c}{2} e^{- c |x|}\\]\n\nMontrer que \\(f\\) est une densit√©.\nD√©terminer la fonction de r√©partition \\(F\\) de \\(X\\).\nOn admet que \\(X\\) admet une esp√©rance. Calculer \\(E(X)\\).\nD√©terminer la loi de \\(Y=|X|\\).\n\n\n\n\nExercice 6 (Lois des v.a.r. min(X,Y) et max(X,Y)) Soit \\(X\\) et \\(Y\\) deux v.a.c de densit√©s respectives \\(f_X\\) et \\(f_Y\\) et de fonctions de r√©partition respectives \\(F_X\\) et \\(F_Y\\). On suppose que \\(X\\) et \\(Y\\) sont ind√©pendantes. On pose:\n\\[Z = max(X,Y) \\quad \\quad \\text{et} \\quad \\quad  T=min(X,Y)\\]\n\nExprimer les fonctions de r√©partition de \\(Z\\) et \\(T\\) √† l‚Äôaide des fonctions de r√©partition \\(F_X\\) et \\(F_Y\\).\nExprimer une densit√© de \\(Z\\) et une densit√© de \\(T\\) √† l‚Äôaide de \\(f_X, f_Y, F_X\\) et \\(F_Y\\).\n\n\n\nExercice 7 (Minimum et Maximum de deux lois exponentielles) Soit \\(X_1\\) et \\(X_2\\) deux variables al√©atoires ind√©pendantes suivant une loi exponentielle de param√®tres \\(\\lambda_1\\) et \\(\\lambda_2\\). On pose \\(X = min(X_1,X_2)\\).\n\nMontrer que \\(X\\) suit une loi exponentielle de param√®tre \\(\\lambda_1 + \\lambda_2\\).\nDeux guichets sont ouverts √† une banque. Le temps de service au premier guichet (resp. au deuxi√®me) suit une loi exponentielle de moyenne 20 min (resp. 30 min). Deux client rentrent simultan√©ment, l‚Äôun choisit le guicher 1 et l‚Äôautre le guichet 2.\n\nEn moyenne, apr√®s combien de temps sort le premier?\nEn moyenne, apr√®s combien de temps sort le dernier?\n\n\nIndication: On pourra utiliser la relation \\(X_1 + X_2 = min(X_1,X_2) + max(X_1,X_2)\\). La somme de deux nombres r√©els est √©gale √† la somme de leur minimum et de leur maximum.\n\n\nExercice 8 (Fonction Gamma (Euler)) La fonction Gamma est d√©finie sur \\(\\mathbb{R}_{+}^*\\) par:\n\\[\\Gamma(x) = \\int_0^{+\\infty} t^{x-1}e^{-t} dt\\]\n\nMontrer que \\(\\Gamma(x+1)=x\\Gamma(x)\\), \\(\\forall \\, x >0\\).\nExprimer \\(\\Gamma(x+n)\\) en fonction de \\(\\Gamma(x)\\) pour \\(n\\in \\mathbb{N}\\).\nCalculer \\(\\Gamma(1)\\). En d√©duire \\(\\Gamma(n+1)\\) pour \\(n\\in \\mathbb{N}\\).\nEn utilisant le changement de variable \\(t=u^2\\), montrer qu‚Äôon a: \\[\\Gamma(x)=2 \\int_0^{+\\infty} u^{2x-1}e^{-u^2} du\\]\nOn suppose que: \\[\\int_0^{+\\infty} e^{-x^2} dx = \\frac{\\sqrt{\\pi}}{2}\\] Calculer alor \\(\\Gamma(\\frac{1}{2})\\).\n\n\n\n\nExercice 9 (Loi Gamma) Pour \\(a>0\\) et \\(\\lambda>0\\), deux constantes r√©elles, on d√©finit la fonction \\(f_{a,\\lambda}\\) sur \\(\\mathbb{R}\\) par: \\[\\forall \\, x \\in \\mathbb{R}, \\quad  f_{a,\\lambda} (x)= \\frac{\\lambda^a}{\\Gamma(a)} x^{a-1} e^{-\\lambda x} \\times {1}_{\\mathbb{R}_{+}}(x)\\]\n\nMontrer que \\(f_{a,\\lambda}\\) est bien une densit√© d‚Äôune v.a. \\(X\\).\nCalculer \\(E(X)\\).\n\n\n\nExercice 10 (Loi uniforme et loi exponentielle) Soit \\(U\\) une v.a.c de loi unifrorme sur \\([0,1]\\). Montrer que la v.a. \\(X= - \\ln U\\) suit une loi exponentielle."
  },
  {
    "objectID": "exos2.html#loi-normale-loi-gaussienne",
    "href": "exos2.html#loi-normale-loi-gaussienne",
    "title": "Feuille d‚Äôexercices 2",
    "section": "Loi Normale (Loi Gaussienne)",
    "text": "Loi Normale (Loi Gaussienne)\n\nExercice 11 On note \\(\\Phi\\) la fonction de r√©partition de la loi normale centr√©e r√©duite.\n\nSoit \\(X\\) une v.a. qui suit une loi normale centr√©e r√©duite, i.e. \\(X \\thicksim \\mathcal{N}(0,1)\\). A l‚Äôaide de la table de la loi normale, calculer: \\(P(X>2), P(-1<X<1.5)\\) et \\(P(X<0.5)\\).\nSoit \\(Y\\) une v.a. qui suite une loi normale: \\(Y \\thicksim \\mathcal{N}(\\mu,\\sigma^2)=\\mathcal{N}(4,16)\\). Calculer: \\(P(Y>2), P(-1<Y<1.5)\\) et \\(P(Y<0.5)\\).\nSoit \\(U \\thicksim \\mathcal{N}(6,4)\\). Calculer: \\(P(|U-4|<3)\\) et \\(P( U>6 | U > 3)\\).\n\n\n\nExercice 12 Une machine produit des pi√®ces dont le diam√®tre \\(X\\) (en cm) est une variable al√©atoire qui suit une loi normale d‚Äôesp√©rance \\(\\mu\\) et de variance \\(\\sigma^2 = (0.01)^2\\). Quelle devrait √™tre la valeur de \\(\\mu\\) de sorte que la probabilit√© qu‚Äôune pi√®ce prise au hasard ait un diam√®tre sup√©rieur √† 3 cm, soit inf√©rieure √† 0.01?\n\n\nExercice 13 On envisage de construire √† l‚Äôentr√©e d‚Äôune caserne une gu√©rite dans laquelle pourra s‚Äôabriter la sentinelle en cas d‚Äôintemp√©ries. Les sentinelles sont des appel√©s dont la taille est approximativement distribu√©e selon une loi normale d‚Äôesp√©rance 175cm et d‚Äô√©cart-type 7cm. A quelle hauteur minimale doit se trouver le toit de la gu√©rite, pour qu‚Äôun sentinelle pris au hasard ait une probabilit√© sup√©rieure √† 0.95 de s‚Äôy tenir debout?\n\n\nExercice 14 Exprimer les int√©grales suivantes √† l‚Äôaide de la fonction de r√©partition \\(\\Phi\\) de la loi normale centr√©e r√©duite, puis en calculer des valeurs approch√©es √† \\(10^{-4}\\) pr√®s:\n\n\\(A=\\int_0^1 e^{-\\frac{x^2}{2}} dx\\)\n\\(B=\\int_0^2 e^{-2x^2+4x-2} dx\\)"
  },
  {
    "objectID": "exos2_en.html#continuous-random-variables",
    "href": "exos2_en.html#continuous-random-variables",
    "title": "Exercises sheet 2",
    "section": "Continuous Random Variables",
    "text": "Continuous Random Variables\n\nExercice 1 Let \\(X\\) be a continuous random variable of density function \\(f\\) defined by: \\(f(x) = k x \\times {1}_{]0,2[} (x)\\).\n\nDetermine the constant \\(k\\).\nCalculate \\(E(X)\\) and \\(E(X^2)\\).\nLet \\(Z=X^2\\). Determine the density function \\(Z\\). Calculate \\(E(Z)\\).\n\n\n\nExercice 2 Let \\(X\\) a continuous random variable of density:\n\\[f(x)= \\left\\lbrace\n      \\begin{array}{ll}\n      c(1-x^2)  & \\mbox{if} \\quad -1<x<1\\\\\n      0 & \\mbox{sinon}\n      \\end{array}\n\\right.\n\\]\n\nCalculate \\(c\\)?\nDetermine the distribution function of \\(X\\)?\n\n\n\nExercice 3 Let \\(X\\) a continuous random variable density:\n\\[\nf(x)= \\left\\lbrace\n      \\begin{array}{ll}\n      0  & \\mbox{if} \\quad |x| > k > 0\\\\\n      x+1 & \\mbox{if} \\quad |x| \\le k\n      \\end{array}\n  \\right.\n\\]\n\nDetermine \\(k\\).\nCalculate \\(E(X)\\) and \\(E(X^2)\\).\nDetermine the distribution function of \\(X\\).\nLet \\(Y=X^2\\). Determine the distribution function and the density function of \\(Y\\).\nCalculate \\(E(Y)\\).\n\n\n\nExercice 4 (Random variable of even density function) Let \\(X\\) a random variable having an even density function \\(f\\).\n\nCalculate \\(P(X \\le 0)\\) and \\(P(X\\ge 0)\\).\nShow that \\(\\forall \\, x \\in \\mathbb{R}, F(x)=1-F(-x)\\), where \\(F\\) is the distribution function of \\(X\\).\nCalculate \\(E(X)\\).\nGive an example of an even density function.\n\n\n\nExercice 5 (Laplace distribution) Let \\(c > 0\\), a real constant. Let \\(f\\) a function defines on \\(\\mathbb{R}\\) by:\n\\[ \\forall \\, x \\in  \\mathbb{R}, \\quad f(x) = \\frac{c}{2} e^{- c |x|}\\]\n\nShow that \\(f\\) is a density function.\nDetermine the distribution function \\(F\\) of \\(X\\).\nCalculate \\(E(X)\\).\nWhat is the distribution of \\(Y=|X|\\).\n\n\n\n\nExercice 6 (Distributions of min(X,Y) and max(X,Y)) Let \\(X\\) and \\(Y\\) two random variables of respective density functions \\(f_X\\) and \\(f_Y\\). Let \\(F_X\\) and \\(F_Y\\) their distribution functions. It is assumed that \\(X\\) and \\(Y\\) are independent. Let:\n\\[Z = max(X,Y) \\quad \\quad \\text{and} \\quad \\quad  T=min(X,Y)\\]\n\nExpress the distribution functions of \\(Z\\) and \\(T\\) with respect to \\(F_X\\) and \\(F_Y\\).\nExpress the density functions of \\(Z\\) and \\(T\\) with respect to \\(f_X, f_Y, F_X\\) and \\(F_Y\\).\n\n\n\nExercice 7 (Minimum and Maximum of two exponential distributions) Let \\(X_1\\) and \\(X_2\\) two independent random variables following the exponential distribution with parameters \\(\\lambda_1\\) and \\(\\lambda_2\\). Let \\(X = min(X_1,X_2)\\).\n\nShow that \\(X\\) follows an exponential distribution of parameter \\(\\lambda_1 + \\lambda_2\\).\nTwo counters are open at a bank. The service time at the first counter (resp. at the second) follows an exponential distribution of average 20 min (resp. 30 min). Two customers enter simultaneously, one chooses the counter number 1 and the other the counter number 2.\n\nOn average, after how long does the first one come out?\nOn average, after how long does the last one come out?\n\n\nHint: Indication: We can use the relation \\(X_1 + X_2 = min(X_1,X_2) + max(X_1,X_2)\\). The sum of two real numbers is equal to the sum of their minimum and maximum.\n\n\nExercice 8 (Gamma function (Euler)) Gamma function is defined on \\(\\mathbb{R}_{+}^*\\) by:\n\\[\\Gamma(x) = \\int_0^{+\\infty} t^{x-1}e^{-t} dt\\]\n\nShow that \\(\\Gamma(x+1)=x\\Gamma(x)\\), \\(\\forall \\, x >0\\).\nExpress \\(\\Gamma(x+n)\\) in function of \\(\\Gamma(x)\\) for \\(n\\in \\mathbb{N}\\).\nCalculate \\(\\Gamma(1)\\). Deduce \\(\\Gamma(n+1)\\) for \\(n\\in \\mathbb{N}\\).\nUsing the variable change \\(t=u^2\\), show that: \\[\\Gamma(x)=2 \\int_0^{+\\infty} u^{2x-1}e^{-u^2} du\\]\nAssuming that: \\[\\int_0^{+\\infty} e^{-x^2} dx = \\frac{\\sqrt{\\pi}}{2}\\] Calculate \\(\\Gamma(\\frac{1}{2})\\).\n\n\n\n\nExercice 9 (Loi Gamma) For \\(a>0\\) and \\(\\lambda>0\\), two real constants, we define the function \\(f_{a,\\lambda}\\) on \\(\\mathbb{R}\\) by: \\[\\forall \\, x \\in \\mathbb{R}, \\quad  f_{a,\\lambda} (x)= \\frac{\\lambda^a}{\\Gamma(a)} x^{a-1} e^{-\\lambda x} \\times {1}_{\\mathbb{R}_{+}}(x)\\]\n\nShow that \\(f_{a,\\lambda}\\) is a density function of \\(X\\).\nCalculate \\(E(X)\\).\n\n\n\nExercice 10 (Uniform and Exponential distributions) Let \\(U\\) a random variable of Uniform distribution on \\([0,1]\\). Show that \\(X= - \\ln U\\) follows an exponential distribution."
  },
  {
    "objectID": "exos2_en.html#normal-gaussian-distribution",
    "href": "exos2_en.html#normal-gaussian-distribution",
    "title": "Exercises sheet 2",
    "section": "Normal (Gaussian) distribution",
    "text": "Normal (Gaussian) distribution\n\nExercice 11 Note \\(\\Phi\\) the distribution function of the Standard Normal distribution.\n\nLet \\(X\\) a random variable following the Standard Normal distribution, i.e. \\(X \\thicksim \\mathcal{N}(0,1)\\). Using the table of Standard Normal distribution function, Calculate: \\(P(X>2), P(-1<X<1.5)\\) and \\(P(X<0.5)\\).\nLet \\(Y\\) a random variable following a Normal distribution: \\(Y \\thicksim \\mathcal{N}(\\mu,\\sigma^2)=\\mathcal{N}(4,16)\\). Calculate: \\(P(Y>2), P(-1<Y<1.5)\\) and \\(P(Y<0.5)\\).\nLet \\(U \\thicksim \\mathcal{N}(6,4)\\). Calculate: \\(P(|U-4|<3)\\) et \\(P( U>6 | U > 3)\\).\n\n\n\nExercice 12 A machine produces coins of diameter \\(X\\) (in cm) that is a random variable following a Normal distribution of expected value \\(\\mu\\) and variance \\(\\sigma^2 = (0.01)^2\\). What should be the value of \\(\\mu\\) so that the probability that a random coin has a diameter greater than 3 cm, is less than 0.01?\n\n\nExercice 13 It is planned to build a gatehouse at the entrance of a barrack in which the sentry can shelter in case of bad weather. Sentinels are conscripts whose size is approximately distributed according to a Normal distribution of expected value 175cm and standard deviation of 7cm. At what minimum height must the roof of the gatehouse be located, so that a randomly chosen sentinel has a probability greater than 0.95 of standing there?\n\n\nExercice 14 Express the following integrals w.r.t. the distribution function of the standard normal distribution \\(\\Phi\\), then calculate:\n\n\\(A=\\int_0^1 e^{-\\frac{x^2}{2}} dx\\)\n\\(B=\\int_0^2 e^{-2x^2+4x-2} dx\\)"
  },
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "Appendix A ‚Äî Table de la loi Normale centr√©e r√©duite",
    "section": "",
    "text": "0 \n    0.01 \n    0.02 \n    0.03 \n    0.04 \n    0.05 \n    0.06 \n    0.07 \n    0.08 \n    0.09 \n  \n\n\n 0 \n    0.5000 \n    0.5040 \n    0.5080 \n    0.5120 \n    0.5160 \n    0.5199 \n    0.5239 \n    0.5279 \n    0.5319 \n    0.5359 \n  \n\n 0.1 \n    0.5398 \n    0.5438 \n    0.5478 \n    0.5517 \n    0.5557 \n    0.5596 \n    0.5636 \n    0.5675 \n    0.5714 \n    0.5753 \n  \n\n 0.2 \n    0.5793 \n    0.5832 \n    0.5871 \n    0.5910 \n    0.5948 \n    0.5987 \n    0.6026 \n    0.6064 \n    0.6103 \n    0.6141 \n  \n\n 0.3 \n    0.6179 \n    0.6217 \n    0.6255 \n    0.6293 \n    0.6331 \n    0.6368 \n    0.6406 \n    0.6443 \n    0.6480 \n    0.6517 \n  \n\n 0.4 \n    0.6554 \n    0.6591 \n    0.6628 \n    0.6664 \n    0.6700 \n    0.6736 \n    0.6772 \n    0.6808 \n    0.6844 \n    0.6879 \n  \n\n 0.5 \n    0.6915 \n    0.6950 \n    0.6985 \n    0.7019 \n    0.7054 \n    0.7088 \n    0.7123 \n    0.7157 \n    0.7190 \n    0.7224 \n  \n\n 0.6 \n    0.7257 \n    0.7291 \n    0.7324 \n    0.7357 \n    0.7389 \n    0.7422 \n    0.7454 \n    0.7486 \n    0.7517 \n    0.7549 \n  \n\n 0.7 \n    0.7580 \n    0.7611 \n    0.7642 \n    0.7673 \n    0.7704 \n    0.7734 \n    0.7764 \n    0.7794 \n    0.7823 \n    0.7852 \n  \n\n 0.8 \n    0.7881 \n    0.7910 \n    0.7939 \n    0.7967 \n    0.7995 \n    0.8023 \n    0.8051 \n    0.8078 \n    0.8106 \n    0.8133 \n  \n\n 0.9 \n    0.8159 \n    0.8186 \n    0.8212 \n    0.8238 \n    0.8264 \n    0.8289 \n    0.8315 \n    0.8340 \n    0.8365 \n    0.8389 \n  \n\n 1 \n    0.8413 \n    0.8438 \n    0.8461 \n    0.8485 \n    0.8508 \n    0.8531 \n    0.8554 \n    0.8577 \n    0.8599 \n    0.8621 \n  \n\n 1.1 \n    0.8643 \n    0.8665 \n    0.8686 \n    0.8708 \n    0.8729 \n    0.8749 \n    0.8770 \n    0.8790 \n    0.8810 \n    0.8830 \n  \n\n 1.2 \n    0.8849 \n    0.8869 \n    0.8888 \n    0.8907 \n    0.8925 \n    0.8944 \n    0.8962 \n    0.8980 \n    0.8997 \n    0.9015 \n  \n\n 1.3 \n    0.9032 \n    0.9049 \n    0.9066 \n    0.9082 \n    0.9099 \n    0.9115 \n    0.9131 \n    0.9147 \n    0.9162 \n    0.9177 \n  \n\n 1.4 \n    0.9192 \n    0.9207 \n    0.9222 \n    0.9236 \n    0.9251 \n    0.9265 \n    0.9279 \n    0.9292 \n    0.9306 \n    0.9319 \n  \n\n 1.5 \n    0.9332 \n    0.9345 \n    0.9357 \n    0.9370 \n    0.9382 \n    0.9394 \n    0.9406 \n    0.9418 \n    0.9429 \n    0.9441 \n  \n\n 1.6 \n    0.9452 \n    0.9463 \n    0.9474 \n    0.9484 \n    0.9495 \n    0.9505 \n    0.9515 \n    0.9525 \n    0.9535 \n    0.9545 \n  \n\n 1.7 \n    0.9554 \n    0.9564 \n    0.9573 \n    0.9582 \n    0.9591 \n    0.9599 \n    0.9608 \n    0.9616 \n    0.9625 \n    0.9633 \n  \n\n 1.8 \n    0.9641 \n    0.9649 \n    0.9656 \n    0.9664 \n    0.9671 \n    0.9678 \n    0.9686 \n    0.9693 \n    0.9699 \n    0.9706 \n  \n\n 1.9 \n    0.9713 \n    0.9719 \n    0.9726 \n    0.9732 \n    0.9738 \n    0.9744 \n    0.9750 \n    0.9756 \n    0.9761 \n    0.9767 \n  \n\n 2 \n    0.9772 \n    0.9778 \n    0.9783 \n    0.9788 \n    0.9793 \n    0.9798 \n    0.9803 \n    0.9808 \n    0.9812 \n    0.9817 \n  \n\n 2.1 \n    0.9821 \n    0.9826 \n    0.9830 \n    0.9834 \n    0.9838 \n    0.9842 \n    0.9846 \n    0.9850 \n    0.9854 \n    0.9857 \n  \n\n 2.2 \n    0.9861 \n    0.9864 \n    0.9868 \n    0.9871 \n    0.9875 \n    0.9878 \n    0.9881 \n    0.9884 \n    0.9887 \n    0.9890 \n  \n\n 2.3 \n    0.9893 \n    0.9896 \n    0.9898 \n    0.9901 \n    0.9904 \n    0.9906 \n    0.9909 \n    0.9911 \n    0.9913 \n    0.9916 \n  \n\n 2.4 \n    0.9918 \n    0.9920 \n    0.9922 \n    0.9925 \n    0.9927 \n    0.9929 \n    0.9931 \n    0.9932 \n    0.9934 \n    0.9936 \n  \n\n 2.5 \n    0.9938 \n    0.9940 \n    0.9941 \n    0.9943 \n    0.9945 \n    0.9946 \n    0.9948 \n    0.9949 \n    0.9951 \n    0.9952 \n  \n\n 2.6 \n    0.9953 \n    0.9955 \n    0.9956 \n    0.9957 \n    0.9959 \n    0.9960 \n    0.9961 \n    0.9962 \n    0.9963 \n    0.9964 \n  \n\n 2.7 \n    0.9965 \n    0.9966 \n    0.9967 \n    0.9968 \n    0.9969 \n    0.9970 \n    0.9971 \n    0.9972 \n    0.9973 \n    0.9974 \n  \n\n 2.8 \n    0.9974 \n    0.9975 \n    0.9976 \n    0.9977 \n    0.9977 \n    0.9978 \n    0.9979 \n    0.9979 \n    0.9980 \n    0.9981 \n  \n\n 2.9 \n    0.9981 \n    0.9982 \n    0.9982 \n    0.9983 \n    0.9984 \n    0.9984 \n    0.9985 \n    0.9985 \n    0.9986 \n    0.9986 \n  \n\n 3 \n    0.9987 \n    0.9987 \n    0.9987 \n    0.9988 \n    0.9988 \n    0.9989 \n    0.9989 \n    0.9989 \n    0.9990 \n    0.9990 \n  \n\n\n\n\n\nPar exemple, pour \\(x = 1.23\\) (intersection de la ligne 1.2 et de la colonne 0.03), on obtient : \\(\\Phi(1.23) \\approx 0.8907\\).\n\n\n\n\n\n\n\n\nInteractive Normal distribution application"
  },
  {
    "objectID": "appendix-Exams.html#ce-2022-2023",
    "href": "appendix-Exams.html#ce-2022-2023",
    "title": "Appendix B ‚Äî Examens",
    "section": "B.1 CE 2022-2023",
    "text": "B.1 CE 2022-2023\n\n\n\n\n\n\nExercice 1\n\n\n\nSoit \\(A\\) et \\(B\\) deux √©v√©nements tels que \\(P(A) = 0.2\\) et \\(P(B) = 0.3\\). Calculer la probabilit√© que l‚Äôun des deux √©v√©nements \\(A\\) et \\(B\\) se produit lorsque\n\n\\(A\\) et \\(B\\) sont incompatibles.\n\\(A\\) et \\(B\\) sont ind√©pendants.\n\n\n\n\n\n\n\n\n\nSolution (Click to expand)\n\n\n\n\n\n\nOn a \\(P(A \\cup B)= P(A)+P(B)-P(A\\cap B)\\) or, puisque \\(A\\) et \\(B\\) sont incompatibles,\\(A\\cap B = \\emptyset\\) et \\(P(A\\cap B)=0\\) donc \\(\\boxed{P(A \\cup B)= P(A)+P(B)=0.5}.\\)\nDe m√™me, puisque \\(A\\) et \\(B\\) sont ind√©pendants, \\(P(A\\cap B)=P(A)P(B)\\) donc \\(\\boxed{P(A \\cup B)= P(A)+P(B)-P(A)P(B)=0.44}.\\)\n\n\n\n\n\n\n\n\n\n\nExercice 2\n\n\n\nUne urne contient une boule qui porte le num√©ro -1, deux qui portent le num√©ro 0 et deux qui portent le num√©ro 4. On extrait simultan√©ment deux boules dans cette urne.\n\nD√©terminer la loi de probabilit√© de la variable al√©atoire \\(X\\) qui repr√©sente la somme des nombres obtenus.\nCalculer \\(E(X)\\).\n\n\n\n\n\n\n\n\n\nSolution (Click to expand)\n\n\n\n\n\n\nDans cette exp√©rience, l‚Äôensemble des r√©sultats possibles (l‚Äôunivers) est \\(X(\\Omega)=\\{-1,0,3,4,8\\}\\).\nOn a \\[\\begin{aligned}\n        P(X=-1)&=P(-1 \\text{ et } 0)=\\dfrac{C^1_1 \\times C^1_2}{C^2_5}=\\dfrac{2}{10}\\\\[0.3em]\n        P(X=0)&=P(0 \\text{ et } 0)=\\dfrac{C^2_2}{C^2_5}=\\dfrac{1}{10}\\\\[0.3em]\n        P(X=3)&=P(-1 \\text{ et } 4)=\\dfrac{C^1_1\\times C^1_2}{C^2_5}=\\dfrac{2}{10}\\\\[0.3em]\n        P(X=4)&=P(0 \\text{ et } 4)=\\dfrac{C^1_2\\times C^1_2}{C^2_5}=\\dfrac{4}{10}\\\\[0.3em]\n        P(X=8)&=P(4 \\text{ et } 4)=\\dfrac{C^2_2}{C^2_5}=\\dfrac{1}{10}\\\\[0.3em]\n    \\end{aligned}\\] Et on v√©rifie bien que la somme des probabilit√©s est √©gale √† 1.\nPar d√©finition, on a \\[\\boxed{E(X)=\\sum_{x_i} x_i P(X=x_i)=-P(X=-1)+3P(X=3)+4P(X=4)+8P(X=8)=2.8}\\]\n\n\n\n\n\n\n\n\n\n\nExercice 3\n\n\n\nSoit \\((X,Y)\\) un couple de variables al√©atoires dont la probabilit√© conjointe est donn√©e par le tableau suivant:\n\n\n\n\\(X\\)\\\\(Y\\)\n-1\n0\n1\n\n\n\n\n0\n1/9\n2/9\n1/9\n\n\n1\n1/9\n2/9\n1/9\n\n\n2\n0\n1/9\n0\n\n\n\n\nCalculer les lois marginales de \\(X\\) et \\(Y\\).\nCalculer la probabilit√© que \\(Y\\) soit poisitive ou nulle sachant que \\(X\\) vaut 0.\nCalculer la covariance de \\(X\\) et \\(Y\\).\n\\(X\\) et \\(Y\\) sont elles ind√©pendantes ?\n\n\n\n\n\n\n\n\n\nSolution (Click to expand)\n\n\n\n\n\n\nPour calculer la loi marginale de \\(X\\), on utilise le fait que \\[\\begin{aligned}\n        P(X={x_i})&=\\sum_{y_i} P(x={x_i}, Y={y_i})\\\\\n        &=P(X={x_i},Y=-1)+ P(X={x_i},Y=0)+P(X={x_i},Y=1)\n    \\end{aligned}\\]\nCela donne \\[\\begin{array}{|c|ccc|}\n        \\hline\n        X & 0 & 1 & 2\\\\\n        \\hline\n        P(X=x) & 4/9 & 4/9 & 1/9\\\\\n        \\hline\n    \\end{array}\\] De m√™me, pour la loi marginale de \\(Y\\), on trouve: \\[\\begin{array}{|c|ccc|}\n        \\hline\n        Y & -1 & 0 & 1\\\\\n        \\hline\n        P(Y=y) & 2/9 & 5/9 & 2/9\\\\\n        \\hline\n    \\end{array}\\] (et on v√©rifie que la somme des probabilit√©s est √©gale √† 1).\nOn a \\[\\begin{aligned}\n        P(Y\\geq 0 \\:\\big|\\: X=0)\n        &=\\dfrac{P(Y\\geq0 \\text{ et } X=0)}{P(X=0)}\\\\[0.3em]\n        &=\\dfrac{P(Y=0 \\text{ et } X=0)+P(Y=1 \\text{ et } X=0)}{P(X=0)}.\n    \\end{aligned}\\] Donc \\(\\boxed{P(Y\\geq 0 \\:\\big|\\: X=0)=\\dfrac{3}{4}}.\\)\nPar d√©finition, on a \\(Cov(X,Y)=E(XY)-E(X)E(Y)\\), or \\[\\begin{aligned}\n        E(XY)&=\\sum_{x_i}\\sum_{y_i} {x_i}{y_i}P(X={x_i},Y={y_i})=0,\\\\\n        E(Y)&=-P(Y=-1)+P(Y=1)=0\n    \\end{aligned}\\] donc \\(\\boxed{Cov(X,Y)=0}.\\)\nAttention! \\(X\\) et \\(Y\\) sont ind√©pendantes \\(\\implies Cov(X,Y) = 0\\). Ceci est √©quivalent √† \\(Cov(X,Y)\\neq 0 \\implies X\\) et \\(Y\\) pas ind√©pendantes. La r√©ciproque n‚Äôest pas toujours vraie! Nous ne pouvons donc rien conclure de la question pr√©c√©dente.\nPour savoir si \\(X\\) et \\(Y\\) ind√©pendantes, il faut repartir de la d√©finition: \\(X\\) et \\(Y\\) ind√©pendantes si et seulement si pour toutes valeurs de X et Y on a \\[P(X=x,Y=y)=P(X=x)P(Y=y).\\] Or ici on remarque (par exemple) que \\[P(X=0,Y=-1)=\\frac{1}{9}\\neq P(X=0)P(Y=-1)=\\frac{8}{81}.\\] Donc \\(\\boxed{X\\text{ et } Y \\text{ ne sont pas ind√©pendantes}}\\).\n\n\n\n\n\n\n\n\n\n\nExercice 4\n\n\n\nUne bo√Æte contient trois transistors, que l‚Äôon d√©signe par \\(A\\),\\(B\\) et \\(C\\). Le transistor \\(A\\) a √©t√© fabriqu√© par une machine qui produit \\(3\\%\\) de d√©fectueux, le transistor \\(B\\) par une machine qui produit \\(5\\%\\) de d√©fectueux, et le transistor \\(C\\) par une machine qui produit \\(7\\%\\) de d√©fectueux.\n\nSoit \\(p\\) la probabilit√© qu‚Äôun transistor pris au hasard dans la bo√Æte soit d√©fectueux. Montrer que \\(p=0.05\\).\nDix transistors sont pris au hasard (avec remise) dans la bo√Æte. Soit \\(N\\) le nombre de transistors d√©fectueux obtenus. Quelle est la loi de \\(N\\)? (La r√©ponse doit √™tre justifi√©e et d√©taill√©e).\nCalculer \\(P(N=0)\\).\nQuelle est l‚Äôesp√©rance math√©matique de \\(N\\).\n\n\n\n\n\n\n\n\n\nSolution (Click to expand)\n\n\n\n\n\n\nOn note \\(A,B,C\\) les √©v√©nements ‚Äúle transistor pris au hasard est le transistor \\(A\\)‚Äù (respectivement, \\(B\\), \\(C\\)). On note \\(D\\) l‚Äô√©venement ‚Äúle transistor est d√©fectueux‚Äù. \\(A,B\\) et \\(C\\) forment une partition sur \\(\\Omega\\). On a \\[\\begin{aligned}\n    P(D) &=P(D \\cap A) + P(D  \\cap B) + P(D  \\cap  C) \\text{ (formule de probabilit√©s totales)}\\\\[0.3em]\n        &=P(D\\big|A)P(A)+P(D\\big|B)P(B)+P(D\\big|C)P(C)\n    \\end{aligned}\\] Cela donne \\(\\boxed{p=\\dfrac{3+5+7}{300}=\\dfrac{5}{100}}\\)\nNous faisons face √† une exp√©rience de Bernouilli (deux r√©sultats possibles) avec r√©p√©titions ind√©pendantes (car il y a remise). La variable al√©atoire \\(N\\) compte le nombre de transistors d√©fectueux. Elle suit donc une loi binomiale avec \\(n=10\\) et \\(p=0.05\\): \\[\\boxed{N \\sim \\mathcal{B}(10,0.05)}.\\] (Autrement dit \\(P(N=k)=C^k_n p^k (1-p)^{n-k} \\quad for \\quad k \\in \\{0, \\ldots , 10\\}\\).)\nApplication directe de la formule de la loi binomiale: \\[\\boxed{P(N=0)=0.95^{10} \\simeq 0.60}.\\]\nNous avons vu dans le cours que, pour une variable \\(N\\) qui suit une loi binomiale, on a \\(\\boxed{E(N)=np=0.5}\\)."
  }
]