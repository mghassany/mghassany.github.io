[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"Welcome! course learn state art Machine Learning also gain practice implementing deploying machine learning algorithms.aim Machine Learning build computer systems can adapt environments learn form experience. Learning techniques methods field successfully applied variety learning tasks broad range areas, including, example, spam recognition, text classification, gene discovery, financial forecasting.\ncourse give overview many concepts, techniques, algorithms machine learning, beginning topics linear regression classification ending topics kmeans Expectation Maximization.\ncourse give basic ideas intuition behind methods, well formal statistical computational understanding. opportunity experiment machine learning techniques R apply selected problem.","code":""},{"path":"index.html","id":"course-overview","chapter":"Welcome","heading":"Course Overview","text":"","code":""},{"path":"index.html","id":"course-schedule","chapter":"Welcome","heading":"Course Schedule","text":"","code":""},{"path":"introduction.html","id":"introduction","chapter":"Introduction","heading":"Introduction","text":"","code":""},{"path":"introduction.html","id":"what-is-machine-learning","chapter":"Introduction","heading":"What is Machine Learning ?","text":"Machine Learning?Two definitions Machine Learning offered. Arthur Samuel described : “field study gives computers ability learn without explicitly programmed.” older, informal definition.Tom Mitchell provides modern definition: “computer program said learn experience E respect class tasks T performance measure P, performance tasks T, measured P, improves experience E.”Machine Learning also called Statistical Learning.Example: playing checkers.E = experience playing many games checkersT = task playing checkers.P = probability program win next game.general, machine learning problem can assigned one two broad classifications:Supervised learning Unsupervised learning.","code":""},{"path":"introduction.html","id":"supervised-learning","chapter":"Introduction","heading":"Supervised Learning","text":"Supervised Learning probably common type machine learning problem.\nLet’s start example . Let’s say want predict housing prices. plot data set looks like .horizontal axis, size different houses square feet, vertical axis, price different houses thousands dollars.. Given data, let’s say house , say 750 square feet hoping sell house want know much can get house.can learning algorithm help?One thing learning algorithm might able put straight line data “fit” straight line data , based , looks like maybe house can sold maybe $150,000.maybe isn’t learning algorithm can use. might better one. example, instead sending straight line data, might decide ’s better fit quadratic function second-order polynomial data., make prediction , looks like, well, maybe can sell house closer $200,000.example supervised learning algorithm.term supervised learning refers fact gave algorithm data set “right answers” given.example also called regression problem. regression problem try predict continuous value output. Namely price example.’s another supervised learning example. Let’s say want look medical records try predict breast cancer malignant benign. someone discovers breast tumor, lump breast, malignant tumor tumor harmful dangerous benign tumor tumor harmless. Let’s see collected data set suppose data set size tumor horizontal axis vertical axis plot one zero, yes , whether examples tumors ’ve seen malignant (one) zero malignant benign.data set five examples benign tumors, five examples malignant tumors.Let’s say person tragically breast tumor, let’s say breast tumor size known (rose arrow following figure).machine learning question , can estimate probability tumor malignant versus benign? introduce bit terminology example classification problem.term classification refers fact ’re trying predict discrete value output: zero one, malignant benign. turns classification problems sometimes can two values two possible values output.classification problems another way plot data. Let’s use slightly different set symbols plot data. tumor size going attribute going use predict malignancy benignness, can also draw data like .took data set top just mapped using different symbols. instead drawing crosses, now going draw O’s benign tumors.Now, example use one feature one attribute, mainly, tumor size order predict whether tumor malignant benign.machine learning problems may one feature.’s example. Let’s say instead just knowing tumor size, know age patients tumor size. case maybe data set look like ., let’s say person tragically tumor. maybe, tumor size age falls around (rose point):given data set like , learning algorithm might throw straight line data try separate malignant tumors benign ones. , hopefully can decide person’s tumor falls benign side therefore likely benign malignant.example two features, namely, age patient size tumor. machine learning problems often features.interesting learning algorithms learning algorithm can deal , just two three five features, infinite number features. deal infinite number features. even store infinite number things computer computer gonna run memory.","code":""},{"path":"introduction.html","id":"unsupervised-learning","chapter":"Introduction","heading":"Unsupervised Learning","text":"second major type machine learning problem called Unsupervised Learning.difference Unsupervised Learning Supervised Learning Supervised Learning told explicitly -called right answers (data labeled).Unsupervised Learning, ’re given data doesn’t labels label really labels. Like example:’re given data set ’re told ’re told data point . Instead ’re just told, data set. Can find structure data?Given data set, Unsupervised Learning algorithm might decide data lives two different clusters.called clustering algorithm.two examples Unsupervised Learning clustering used.Social network analysis:given knowledge friends email given Facebook friends Google+ circles, can automatically identify cohesive groups friends, also groups people know ?Market segmentation:Many companies huge databases customer information. , can look customer data set automatically discover market segments automatically group customers different market segments can automatically efficiently sell market different market segments together?Unsupervised Learning customer data, don’t know advance market segments customers data set, don’t know advance market segment one, market segment two, . let algorithm discover just data.\n◼\n","code":""},{"path":"linear-regression.html","id":"linear-regression","chapter":"1 Linear Regression","heading":"1 Linear Regression","text":"","code":""},{"path":"linear-regression.html","id":"notation","chapter":"1 Linear Regression","heading":"1.1 Notation","text":"general, let \\(x_{ij}\\) represent value \\(j\\)th variable \\(\\)th observation, \\(=1,2,\\ldots,n\\) \\(j=1,2,\\ldots,p\\).\nuse \\(\\) index samples observations (\\(1\\) \\(n\\)) \\(j\\) used index variables (features) (\\(1\\) \\(p\\)). let \\(\\textbf{X}\\) denote \\(n \\times p\\) matrix whose \\((,j)\\)th element \\(x_{ij}\\). ,\\[ \\textbf{X}  = \\begin{pmatrix}\n    x_{11} & x_{12} & x_{13} & \\dots  & x_{1p} \\\\\n    x_{21} & x_{22} & x_{23} & \\dots  & x_{2p} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    x_{n1} & x_{n2} & x_{n3} & \\dots  & x_{np}\n\\end{pmatrix} \\]Note useful visualize \\(\\textbf{X}\\) spreadsheet numbers \\(n\\) rows \\(p\\) columns.\nwrite rows \\(\\textbf{X}\\) \\(x_1 , x_2 , \\ldots, x_n\\). \\(x_i\\) vector length \\(p\\), containing \\(p\\) variable measurements \\(\\)th observation. ,\\[ x_i = \\begin{pmatrix}\n    x_{i1} \\\\\n    x_{i2} \\\\\n    \\vdots \\\\\n    x_{ip}\n\\end{pmatrix}\\](Vectors default represented columns.)write columns \\(\\textbf{X}\\) \\(\\textbf{x}_1 , \\textbf{x}_2, \\ldots, \\textbf{x}_p\\). vector length \\(n\\). ,\\[ \\textbf{x}_j = \\begin{pmatrix}\n    \\textbf{x}_{1j} \\\\\n    \\textbf{x}_{2j} \\\\\n    \\vdots \\\\\n    \\textbf{x}_{nj}\n\\end{pmatrix}\\]Using notation, matrix \\(\\textbf{X}\\) can written \\[ \\textbf{X} = (\\textbf{x}_1  \\textbf{x}_2 \\ldots \\textbf{x}_p) \\]\\[ \\textbf{X} = \\begin{pmatrix}\n    x_{1}^T \\\\\n    x_{2}^T \\\\\n    \\vdots \\\\\n    x_{n}^T\n\\end{pmatrix}\\]\\(^T\\) notation denotes transpose matrix vector.use \\(y_i\\) denote \\(\\)th observation variable wish make predictions. write set \\(n\\) observations vector form \\[ \\textbf{y} = \\begin{pmatrix}\n    y_{1} \\\\\n    y_{2} \\\\\n    \\vdots \\\\\n    y_{n}\n\\end{pmatrix}\\]observed data consists \\(\\{(x_1, y_1), (x_2 , y_2 ), \\ldots , (x_n , y_n )\\}\\), \n\\(x_i\\) vector length \\(p\\). (\\(p = 1\\), \\(x_i\\) simply scalar).","code":""},{"path":"linear-regression.html","id":"model-representation","chapter":"1 Linear Regression","heading":"1.2 Model Representation","text":"Let’s consider example predicting housing prices. ’re going use data set example,Suppose person trying sell house size 1250 square feet wants know much might able sell house . One thing fit model. Maybe fit straight line data. Looks something like ,based , maybe can sell house around $220,000. Recall example supervised learning algorithm. ’s supervised learning ’re given “right answer” examples. precisely, example regression problem term regression refers fact predicting real-valued output namely price.formally, supervised learning, data set data set called training set. housing prices example, training set different housing prices job learn data predict prices houses.Let’s define notation data set:size house input variable.house price output variable.input variables typically denoted using variable symbol \\(X\\),inputs go different names, predictors, independent variables, features, sometimes just variables.output variable often called response, dependent variable target, typically denoted using symbol \\(Y\\).\\((x_i,y_i)\\) \\(\\)th training example.set \\(\\{(x_i, y_i)\\}\\) training set.\\(n\\) number training examples.’s supervised learning algorithm works. Suppose observe quantitative response \\(Y\\) \\(p\\) different predictors, \\(X_1 , X_2 ,\\ldots, X_p\\) . assume relationship \\(Y\\) \\(X = (X_1 , X_2 ,\\ldots, X_p)\\), can written general form\\[Y = f(X) + \\epsilon\\]\\(f\\) fixed unknown function \\(X_1 , X_2 ,\\ldots, X_p\\) , \\(\\epsilon\\) random error term, independent \\(X\\) mean zero. \\(f\\) function also called hypothesis Machine Learning. general, function \\(f\\) may involve one input variable. essence, Supervised Learning refers set approaches estimating \\(f\\).","code":""},{"path":"linear-regression.html","id":"why-estimate-f","chapter":"1 Linear Regression","heading":"1.3 Why Estimate \\(f\\) ?","text":"two main reasons may wish estimate \\(f\\): prediction\ninference.","code":""},{"path":"linear-regression.html","id":"prediction","chapter":"1 Linear Regression","heading":"Prediction","text":"many situations, set inputs \\(X\\) readily available, output \\(Y\\) easily obtained. setting, since error term averages zero, can predict \\(Y\\) using\\[ \\hat{Y} = \\hat{f}(X) \\]\\(\\hat{f}\\) represents estimate \\(f\\), \\(\\hat{Y}\\) represents resulting prediction \\(Y\\). Like example predicting housing prices.can measure accuracy \\(\\hat{Y}\\) using cost function. regression models, commonly-used measure mean squared error (MSE), given \\[ MSE = \\frac{1}{n} \\sum_{=1}^{n} (y_i - \\hat{f}(x_i))^2\\]","code":""},{"path":"linear-regression.html","id":"inference","chapter":"1 Linear Regression","heading":"Inference","text":"often interested understanding way \\(Y\\) affected \\(X_1 , X_2 ,\\ldots, X_p\\) change. situation wish estimate \\(f\\) , goal necessarily make predictions \\(Y\\). instead want understand relationship \\(X\\) \\(Y\\), specifically, understand \\(Y\\) changes function \\(X_1 , X_2 ,\\ldots, X_p\\). case, one may interested answering following questions:predictors associated response?relationship response predictor?Can relationship Y predictor adequately summarized using linear equation, relationship complicated?","code":""},{"path":"linear-regression.html","id":"simple-linear-regression-model","chapter":"1 Linear Regression","heading":"1.4 Simple Linear Regression Model","text":"Simple linear regression straightforward approach predicting quantitative response \\(Y\\) basis single predictor variable \\(X\\). assumes approximately linear relationship \\(X\\) \\(Y\\). Mathematically, can write linear relationship \\[ Y = \\beta_0 + \\beta_1 X + \\epsilon \\]\n\\[Y \\approx \\beta_0 + \\beta_1 X\\]\\(\\beta_0\\) \\(\\beta_1\\) two unknown constants represent intercept slope, also known coefficients parameters, \\(\\epsilon\\) error term.Given estimates \\(\\hat{\\beta_0}\\) \\(\\hat{\\beta_1}\\) model coefficients, predict future inputs \\(x\\) using\\[\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1} x\\]\\(\\hat{y}\\) indicates prediction \\(Y\\) basis \\(X = x\\).\nhat symbol, \\(\\hat{}\\), denotes estimated value.","code":""},{"path":"linear-regression.html","id":"estimating-the-coefficients","chapter":"1 Linear Regression","heading":"1.5 Estimating the Coefficients","text":"Let \\(\\hat{y}_i = \\hat{\\beta_0} + \\hat{\\beta_1} x_i\\) prediction \\(Y\\) based \\(\\)th value \\(X\\). \\(e_i = y_i - \\hat{y}_i\\) represents \\(\\)th residual.define Residual Sum Squares (RSS)1 \\[  \\begin{aligned}\nRSS &= e_1^2 + e_2^2 + \\ldots + e_n^2 \\\\\n    &= \\sum_{=1}^{n} e_i^2\n\\end{aligned}  \\]equivantly \\[ \\begin{aligned}\nRSS &= (y_1 - \\hat{\\beta_0} - \\hat{\\beta_1} x_1)^2 + (y_2 - \\hat{\\beta_0} - \\hat{\\beta_1} x_2)^2 + \\ldots + (y_n - \\hat{\\beta_0} - \\hat{\\beta_1} x_n)^2 \\\\\n    &= \\sum_{=1}^{n} (y_i - \\hat{\\beta_0} - \\hat{\\beta_1} x_i)^2\n\\end{aligned} \\]least squares approach chooses \\(\\hat{\\beta_0}\\) \\(\\hat{\\beta_1}\\) minimize RSS. minimizing values can shown be2\\[  \\begin{aligned}\n\\hat{\\beta_1} &=  \\frac{\\sum_{=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})  }{\\sum_{=1}^{n} (x_i - \\bar{x})^2 } = \\frac{s_{xy}}{s_x^2} = (s_x^2)^{-1} s_{xy} \\\\\n\\text{} \\\\\n\\hat{\\beta_0} &= \\bar{y} - \\hat{\\beta_1} \\bar{x}\n\\end{aligned}  \\]:\\(\\bar{x}=\\frac{1}{n}\\sum_{=1}^nx_i\\) sample mean.\\(s_x^2=\\frac{1}{n}\\sum_{=1}^n(x_i-\\bar{x})^2\\) sample variance. sample standard deviation \\(s_x=\\sqrt{s_x^2}\\).\\(s_{xy}=\\frac{1}{n}\\sum_{=1}^n(x_i-\\bar{x})(y_i-\\bar{y})\\) sample covariance. measures degree linear association \\(x_1,\\ldots,x_n\\) \\(y_1,\\ldots,y_n\\). scaled \\(s_xs_y\\), gives sample correlation coefficient, \\(r_{xy}=\\frac{s_{xy}}{s_xs_y}\\).\n1- find optimal estimates \\(\\beta_0\\) \\(\\beta_1\\) need choice-criterion. \ncase least squares approach (precisely, \nordinary least squares OLS) criterion residual sum\nsquares RSS: calculate \\(\\beta_0\\) \\(\\beta_1\\) minimise \nRSS.\n\n2- Minimizing RSS function requires calculate \nfirst order derivatives respect \\(\\beta_0\\) \\(\\beta_1\\) set zero.\n\n3- Click\n\n watch\nvideo understand residuals least squares.\n\n4- Click\n\n see\ninfluence distance employed sum squares. Try \nminimize sum squares different datasets. choices \nintercept slope minimize sum squared distances \nkind distance optimal different kind \ndistance.\n","code":""},{"path":"linear-regression.html","id":"assessing-the-accuracy-of-the-coefficient-estimates","chapter":"1 Linear Regression","heading":"1.6 Assessing the Accuracy of the Coefficient Estimates","text":"standard error estimator reflects varies repeated sampling. \\[ \\text{SE}(\\hat{\\beta_1})^2 =  \\frac{\\sigma^2}{\\sum_{=1}^{n} (x_i - \\bar{x})^2} \\]\\[ \\text{SE}(\\hat{\\beta_0})^2 = \\sigma^2 \\bigg[ \\frac{1}{n} +  \\frac{\\bar{x}^2}{\\sum_{=1}^{n} (x_i - \\bar{x})^2} \\bigg] \\]\\(\\sigma^2 = Var(\\epsilon)\\)general, \\(\\sigma^2\\) unknown, can estimated data. estimate \\(\\sigma\\) known residual standard error, given \\[ \\text{RSE} = \\sqrt{\\frac{\\text{RSS}}{(n-2)}} \\]standard errors can used compute confidence intervals. \\(95\\%\\) confidence interval defined range values \\(95\\%\\) probability, range contain true unknown value parameter. form\\[ \\hat{\\beta_1} \\pm 2 \\cdot \\text{SE}(\\hat{\\beta_1}) \\], approximately \\(95\\%\\) chance interval\\[ \\bigg[  \\hat{\\beta_1} - 2 \\cdot \\text{SE}(\\hat{\\beta_1}), \\hat{\\beta_1} + 2 \\cdot \\text{SE}(\\hat{\\beta_1})   \\bigg] \\]contain true value \\(\\beta_1\\). Similarly, confidence interval \\(\\beta_0\\) approximately takes form\\[ \\hat{\\beta_0} \\pm 2 \\cdot \\text{SE}(\\hat{\\beta_0}) \\]","code":""},{"path":"linear-regression.html","id":"hypothesis-testing","chapter":"1 Linear Regression","heading":"Hypothesis testing","text":"Standard errors can also used perform hypothesis tests coefficients. common hypothesis test involves testing null hypothesis \\[ H_0 : \\text{linear relationship } \\, X \\, \\text{} \\, Y \\]versus alternative hypothesis\\[ H_1 : \\text{relationship } \\, X \\, \\text{} \\, Y \\]Mathematically, corresponds testing\\[ H_0 : \\beta_1 = 0 \\]versus\\[ H_1 : \\beta_1 \\neq 0 \\]since \\(\\beta_1 = 0\\) simple linear regression model reduces \\(Y = \\beta_0 + \\epsilon\\), \\(X\\) associated \\(Y\\)3.test null hypothesis \\(H_0\\), compute t-statistic, given \\[ t = \\frac{\\hat{\\beta_1} - 0}{\\text{SE}(\\hat{\\beta_1})} \\]\\(t\\)-distribution (Student) \\(n-2\\) degrees freedom, assuming \\(\\beta_1=0\\).Using statistical software, easy compute probability observing value equal \\(|t|\\) larger. call probability p-value.p-value small enough (typically \\(0.01\\) (\\(1\\%\\) error) \\(0.05\\) (\\(5\\%\\) error)) reject null hypothesis, declare relationship exist \\(X\\) \\(Y\\).","code":""},{"path":"linear-regression.html","id":"anova-and-model-fit","chapter":"1 Linear Regression","heading":"1.7 ANOVA and model fit","text":"","code":""},{"path":"linear-regression.html","id":"anova","chapter":"1 Linear Regression","heading":"1.7.1 ANOVA","text":"section see variance \\(Y\\) decomposed two parts, one corresponding regression error, respectively. decomposition called ANalysis VAriance (ANOVA).explaining ANOVA, important recall interesting result: mean fitted values \\(\\hat Y_1,\\ldots,\\hat Y_n\\) mean \\(Y_1,\\ldots, Y_n\\). easily seen plug-expression \\(\\hat\\beta_0\\):\n\\[\\begin{align*}\n\\frac{1}{n}\\sum_{=1}^n \\hat Y_i=\\frac{1}{n}\\sum_{=1}^n \\left(\\hat \\beta_0+\\hat\\beta_1X_i\\right)=\\hat \\beta_0+\\hat\\beta_1\\bar X=\\left(\\bar Y - \\hat\\beta_1\\bar X \\right) + \\hat\\beta_1\\bar X=\\bar Y.\n\\end{align*}\\]\nANOVA decomposition considers following measures variation related response:\\(\\text{SST}=\\text{TSS}=\\sum_{=1}^n\\left(Y_i-\\bar Y\\right)^2\\), Total Sum Squares. total variation \\(Y_1,\\ldots,Y_n\\), since \\(\\text{SST}=ns_y^2\\), \\(s_y^2\\) sample variance \\(Y_1,\\ldots,Y_n\\).\\(\\text{SSR}=\\text{ESS}=\\sum_{=1}^n\\left(\\hat Y_i-\\bar Y\\right)^2\\), Regression Sum Squares Explained Sum Squares4. variation explained regression line, , variation \\(\\bar Y\\) explained estimated conditional mean \\(\\hat Y_i=\\hat\\beta_0+\\hat\\beta_1X_i\\). \\(\\text{SSR}=ns_{\\hat y}^2\\), \\(s_{\\hat y}^2\\) sample variance \\(\\hat Y_1,\\ldots,\\hat Y_n\\).\\(\\text{SSE}=\\text{RSS}=\\sum_{=1}^n\\left(Y_i-\\hat Y_i\\right)^2\\), Sum Squared Errors Residual Sum Squares5. variation around conditional mean. Recall \\(\\text{SSE}=\\sum_{=1}^n \\hat\\varepsilon_i^2=(n-2)\\hat\\sigma^2\\), \\(\\hat\\sigma^2\\) sample variance \\(\\hat \\varepsilon_1,\\ldots,\\hat \\varepsilon_n\\).ANOVA decomposition \n\\[\\begin{align*}\n\\underbrace{\\text{SST}}_{\\text{Variation }Y_i's} = \\underbrace{\\text{SSR}}_{\\text{Variation }\\hat Y_i's} + \\underbrace{\\text{SSE}}_{\\text{Variation }\\hat \\varepsilon_i's}\n\\end{align*}\\]graphical interpretation equation shown following figures.\nFigure 1.1: Visualization ANOVA decomposition. SSR measures variation \\(Y_1,\\ldots,Y_n\\) respect \\(\\bar Y\\). SST measures variation respect conditional means, \\(\\hat \\beta_0+\\hat\\beta_1X_i\\). SSE collects variation residuals.\n\nANOVA decomposition dependence \\(\\sigma^2\\) \\(\\hat\\sigma^2\\). Application also\navailable\nhere1.\n\nNote animation displayed \nfirst time browsed (reason hosted \nhttps websites auto-signed SSL certificates).\nsee , click link . get \nwarning browser saying “connection \nprivate”. Click “Advanced” allow \nexception browser. next time animation \nshow correctly.\nANOVA table summarizes decomposition variance. given layout employed .anova function  takes model input returns ANOVA table.“\\(F\\)-value” ANOVA table represents value \\(F\\)-statistic \\(\\frac{\\text{SSR}/1}{\\text{SSE}/(n-2)}\\). statistic employed test\n\\[\\begin{align*}\nH_0:\\beta_1=0\\quad\\text{vs.}\\quad H_1:\\beta_1\\neq 0,\n\\end{align*}\\]\n, hypothesis linear dependence \\(Y\\) \\(X\\). result test completely equivalent \\(t\\)-test \\(\\beta_1\\) saw previously Hypothesis testing (something specific simple linear regression – \\(F\\)-test equivalent \\(t\\)-test \\(\\beta_1\\) Mulitple Linear Regression).happens \n\\[\\begin{align*}\nF=\\frac{\\text{SSR}/1}{\\text{SSE}/(n-2)}\\stackrel{H_0}{\\sim} F_{1,n-2},\n\\end{align*}\\]\n\\(F_{1,n-2}\\) Snedecor’s \\(F\\) distribution6 \\(1\\) \\(n-2\\) degrees freedom.\\(H_0\\) true, \\(F\\) expected small since SSR close zero. \\(p\\)-value test \\(p\\)-value \\(t\\)-test \\(H_0:\\beta_1=0\\).","code":""},{"path":"linear-regression.html","id":"the-r2-statistic","chapter":"1 Linear Regression","heading":"1.7.2 The \\(R^2\\) Statistic","text":"calculate \\(R^2\\), use formula\\[ R^2 = \\frac{\\text{TSS} - \\text{RSS}}{\\text{TSS}} = 1- \\frac{\\text{RSS}}{\\text{TSS}} \\]\\(\\text{TSS} = \\sum (y_i - \\bar{y})^2\\) total sum squares.\\(R^2\\) measures proportion variability \\(Y\\) can explained using \\(X\\). \\(R^2\\) statistic close 1 indicates large proportion variability response explained regression. number near 0 indicates regression explain much variability response; might occur linear model wrong, inherent error \\(\\sigma^2\\) high, .can shown simple linear regression setting \\(R^2 = r^2\\), \\(r\\) correlation \\(X\\) \\(Y\\):\\[ r = \\frac{cov(X,Y)}{\\sigma_X \\sigma_Y} \\]\n\\(R^2\\) measure \ncorrectness linear model usefulness (\nprediction, explaining variance \\(Y\\)), assuming model correct.\n\nTrusting blindly \\(R^2\\) can\nlead catastrophic conclusions, since model may \ncorrect.\n\nlarge \\(R^2\\) means\nnothing assumptions model \nhold. \\(R^2\\) \nproportion variance \\(Y\\)\nexplained \\(X\\), , course,\nlinear model correct.\n\n◼\n","code":""},{"path":"practical-work-1.html","id":"practical-work-1","chapter":"Practical Work 1","heading":"Practical Work 1","text":"","code":""},{"path":"practical-work-1.html","id":"some-basics","chapter":"Practical Work 1","heading":"1.8 Some  basics","text":"","code":""},{"path":"practical-work-1.html","id":"basic-commands","chapter":"Practical Work 1","heading":"1.8.1 Basic Commands","text":" uses functions perform operations. run function called funcname,type funcname(input1, input2) , inputs (arguments) input1 input2 tell  run function. function can number inputs. example, create vector numbers, use function c() (concatenate).Note > part command; rather, printed  indicate ready another command entered. can also save things using = rather <-. Note answer code followed #ans>  console .Hitting arrow multiple times display previous commands, can edited. useful since one often wishes repeat similar command.ls() function allows us look list objects, ls() data functions, saved far. rm() function can used delete object don’t want.","code":"\nx <- c(1,3,2,5)\nx\n#ans> [1] 1 3 2 5\nx = c(1,6,2)\nx\n#ans> [1] 1 6 2\ny = c(1,4,3)\nlength(x)\n#ans> [1] 3\nlength(y)\n#ans> [1] 3\nx+y\n#ans> [1]  2 10  5\nls()\n#ans>  [1] \"A\"                        \"Auto\"                    \n#ans>  [3] \"B\"                        \"Boston_bagging\"          \n#ans>  [5] \"Boston_bagging_pred\"      \"Boston_boost\"            \n#ans>  [7] \"Boston_boost_pred\"        \"Boston_forest\"           \n#ans>  [9] \"Boston_forest_pred\"       \"Boston_idx\"              \n#ans> [11] \"Boston_lm\"                \"Boston_lm_pred\"          \n#ans> [13] \"Boston_test\"              \"Boston_train\"            \n#ans> [15] \"Boston_tree\"              \"Boston_tree_pred\"        \n#ans> [17] \"challenger\"               \"d\"                       \n#ans> [19] \"data1\"                    \"data2\"                   \n#ans> [21] \"dataFrame\"                \"dendro\"                  \n#ans> [23] \"error\"                    \"error_squared\"           \n#ans> [25] \"EU\"                       \"eurojob\"                 \n#ans> [27] \"iris\"                     \"kmeansObj\"               \n#ans> [29] \"ligue1\"                   \"ligue1_scaled\"           \n#ans> [31] \"M\"                        \"mod\"                     \n#ans> [33] \"model\"                    \"MSE\"                     \n#ans> [35] \"multiplot\"                \"myDf\"                    \n#ans> [37] \"myList\"                   \"n\"                       \n#ans> [39] \"names.available.packages\" \"nasa\"                    \n#ans> [41] \"Notes\"                    \"p\"                       \n#ans> [43] \"p1\"                       \"p2\"                      \n#ans> [45] \"p3\"                       \"p4\"                      \n#ans> [47] \"pca\"                      \"pcairis\"                 \n#ans> [49] \"pdf2png\"                  \"Rcmdr.related.packages\"  \n#ans> [51] \"res.pca\"                  \"rmse\"                    \n#ans> [53] \"source\"                   \"sumMod\"                  \n#ans> [55] \"test\"                     \"testing_data\"            \n#ans> [57] \"train\"                    \"training_data\"           \n#ans> [59] \"treeAve\"                  \"treeComp\"                \n#ans> [61] \"treeSingle\"               \"variables\"               \n#ans> [63] \"vec\"                      \"x\"                       \n#ans> [65] \"X\"                        \"x1\"                      \n#ans> [67] \"x2\"                       \"x3\"                      \n#ans> [69] \"x4\"                       \"y\"                       \n#ans> [71] \"y_hat\"                    \"yclass\"\nrm(x)\nls()\n#ans>  [1] \"A\"                        \"Auto\"                    \n#ans>  [3] \"B\"                        \"Boston_bagging\"          \n#ans>  [5] \"Boston_bagging_pred\"      \"Boston_boost\"            \n#ans>  [7] \"Boston_boost_pred\"        \"Boston_forest\"           \n#ans>  [9] \"Boston_forest_pred\"       \"Boston_idx\"              \n#ans> [11] \"Boston_lm\"                \"Boston_lm_pred\"          \n#ans> [13] \"Boston_test\"              \"Boston_train\"            \n#ans> [15] \"Boston_tree\"              \"Boston_tree_pred\"        \n#ans> [17] \"challenger\"               \"d\"                       \n#ans> [19] \"data1\"                    \"data2\"                   \n#ans> [21] \"dataFrame\"                \"dendro\"                  \n#ans> [23] \"error\"                    \"error_squared\"           \n#ans> [25] \"EU\"                       \"eurojob\"                 \n#ans> [27] \"iris\"                     \"kmeansObj\"               \n#ans> [29] \"ligue1\"                   \"ligue1_scaled\"           \n#ans> [31] \"M\"                        \"mod\"                     \n#ans> [33] \"model\"                    \"MSE\"                     \n#ans> [35] \"multiplot\"                \"myDf\"                    \n#ans> [37] \"myList\"                   \"n\"                       \n#ans> [39] \"names.available.packages\" \"nasa\"                    \n#ans> [41] \"Notes\"                    \"p\"                       \n#ans> [43] \"p1\"                       \"p2\"                      \n#ans> [45] \"p3\"                       \"p4\"                      \n#ans> [47] \"pca\"                      \"pcairis\"                 \n#ans> [49] \"pdf2png\"                  \"Rcmdr.related.packages\"  \n#ans> [51] \"res.pca\"                  \"rmse\"                    \n#ans> [53] \"source\"                   \"sumMod\"                  \n#ans> [55] \"test\"                     \"testing_data\"            \n#ans> [57] \"train\"                    \"training_data\"           \n#ans> [59] \"treeAve\"                  \"treeComp\"                \n#ans> [61] \"treeSingle\"               \"variables\"               \n#ans> [63] \"vec\"                      \"X\"                       \n#ans> [65] \"x1\"                       \"x2\"                      \n#ans> [67] \"x3\"                       \"x4\"                      \n#ans> [69] \"y\"                        \"y_hat\"                   \n#ans> [71] \"yclass\""},{"path":"practical-work-1.html","id":"vectors","chapter":"Practical Work 1","heading":"1.8.2 Vectors","text":"following:Create vector \\(x=(1, 7, 3, 4)\\).Create vector \\(y=(100, 99, 98, ..., 2, 1)\\).Compute \\(x_3+y_4\\) \\(\\cos(x_3) + \\sin(x_2) e^{-y_2}\\). (Answers: 100, -0.9899925)Set \\(x_{3}=0\\) \\(y_{2}=-1\\). Recompute previous expressions. (Answers: 97, 2.785875)Index \\(y\\) \\(x+1\\) store z. output? (Answer: z c(-1, 93, 100, 96))","code":"\n\n# A handy way of creating sequences is the operator :\n# Sequence from 1 to 5\n1:5\n#ans> [1] 1 2 3 4 5\n\n# Storing some vectors\nvec <- c(-4.12, 0, 1.1, 1, 3, 4)\nvec\n#ans> [1] -4.12  0.00  1.10  1.00  3.00  4.00\n\n# Entry-wise operations\nvec + 1\n#ans> [1] -3.12  1.00  2.10  2.00  4.00  5.00\nvec^2\n#ans> [1] 16.97  0.00  1.21  1.00  9.00 16.00\n\n# If you want to access a position of a vector, use [position]\nvec[6]\n#ans> [1] 4\n\n# You also can change elements\nvec[2] <- -1\nvec\n#ans> [1] -4.12 -1.00  1.10  1.00  3.00  4.00\n\n# If you want to access all the elements except a position, use [-position]\nvec[-2]\n#ans> [1] -4.12  1.10  1.00  3.00  4.00\n\n# Also with vectors as indexes\nvec[1:2]\n#ans> [1] -4.12 -1.00\n\n# And also\nvec[-c(1, 2)]\n#ans> [1] 1.1 1.0 3.0 4.0"},{"path":"practical-work-1.html","id":"matrices-data-frames-and-lists","chapter":"Practical Work 1","heading":"1.8.3 Matrices, data frames and lists","text":"","code":"\n# A matrix is an array of vectors\nA <- matrix(1:4, nrow = 2, ncol = 2)\nA\n#ans>      [,1] [,2]\n#ans> [1,]    1    3\n#ans> [2,]    2    4\n\n# Another matrix\nB <- matrix(1:4, nrow = 2, ncol = 2, byrow = TRUE)\nB\n#ans>      [,1] [,2]\n#ans> [1,]    1    2\n#ans> [2,]    3    4\n\n# Binding by rows or columns\nrbind(1:3, 4:6)\n#ans>      [,1] [,2] [,3]\n#ans> [1,]    1    2    3\n#ans> [2,]    4    5    6\ncbind(1:3, 4:6)\n#ans>      [,1] [,2]\n#ans> [1,]    1    4\n#ans> [2,]    2    5\n#ans> [3,]    3    6\n\n# Entry-wise operations\nA + 1\n#ans>      [,1] [,2]\n#ans> [1,]    2    4\n#ans> [2,]    3    5\nA * B\n#ans>      [,1] [,2]\n#ans> [1,]    1    6\n#ans> [2,]    6   16\n\n# Accessing elements\nA[2, 1] # Element (2, 1)\n#ans> [1] 2\nA[1, ] # First row\n#ans> [1] 1 3\nA[, 2] # Second column\n#ans> [1] 3 4\n\n# A data frame is a matrix with column names\n# Useful when you have multiple variables\nmyDf <- data.frame(var1 = 1:2, var2 = 3:4)\nmyDf\n#ans>   var1 var2\n#ans> 1    1    3\n#ans> 2    2    4\n\n# You can change names\nnames(myDf) <- c(\"newname1\", \"newname2\")\nmyDf\n#ans>   newname1 newname2\n#ans> 1        1        3\n#ans> 2        2        4\n\n# The nice thing is that you can access variables by its name with the $ operator\nmyDf$newname1\n#ans> [1] 1 2\n\n# And create new variables also (it has to be of the same\n# length as the rest of variables)\nmyDf$myNewVariable <- c(0, 1)\nmyDf\n#ans>   newname1 newname2 myNewVariable\n#ans> 1        1        3             0\n#ans> 2        2        4             1\n\n# A list is a collection of arbitrary variables\nmyList <- list(vec = vec, A = A, myDf = myDf)\n\n# Access elements by names\nmyList$vec\n#ans> [1] -4.12 -1.00  1.10  1.00  3.00  4.00\nmyList$A\n#ans>      [,1] [,2]\n#ans> [1,]    1    3\n#ans> [2,]    2    4\nmyList$myDf\n#ans>   newname1 newname2 myNewVariable\n#ans> 1        1        3             0\n#ans> 2        2        4             1\n\n# Reveal the structure of an object\nstr(myList)\n#ans> List of 3\n#ans>  $ vec : num [1:6] -4.12 -1 1.1 1 3 4\n#ans>  $ A   : int [1:2, 1:2] 1 2 3 4\n#ans>  $ myDf:'data.frame': 2 obs. of  3 variables:\n#ans>   ..$ newname1     : int [1:2] 1 2\n#ans>   ..$ newname2     : int [1:2] 3 4\n#ans>   ..$ myNewVariable: num [1:2] 0 1\nstr(myDf)\n#ans> 'data.frame': 2 obs. of  3 variables:\n#ans>  $ newname1     : int  1 2\n#ans>  $ newname2     : int  3 4\n#ans>  $ myNewVariable: num  0 1\n\n# A less lengthy output\nnames(myList)\n#ans> [1] \"vec\"  \"A\"    \"myDf\""},{"path":"practical-work-1.html","id":"graphics","chapter":"Practical Work 1","heading":"1.8.4 Graphics","text":"plot() function primary way plot data  . instance, plot(x,y) produces scatterplot numbers x versus numbers y. many additional options can passed plot() function. example, passing argument xlab result label x-axis. find information plot() function, type ?plot.","code":"\nx=rnorm(100)\n# The rnorm() function generates a vector of random normal variables,\n# rnorm() with first argument n the sample size. Each time we call this\n# function, we will get a different answer.\ny=rnorm(100)\nplot(x,y)\n\n# with titles\nplot(x,y,xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",\nmain=\"Plot of X vs Y\")"},{"path":"practical-work-1.html","id":"distributions","chapter":"Practical Work 1","heading":"1.8.5 Distributions","text":"following:Compute 90%, 95% 99% quantiles \\(F\\) distribution df1 = 1 df2 = 5. (Answer: c(4.060420, 6.607891, 16.258177))Sample 100 points Poisson lambda = 5.Plot density \\(t\\) distribution df = 1 (use sequence spanning -4 4). Add lines different colors densities df = 5, df = 10, df = 50 df = 100.\n","code":"\n# R allows to sample [r], compute density/probability mass [d],\n# compute distribution function [p] and compute quantiles [q] for several\n# continuous and discrete distributions. The format employed is [rdpq]name,\n# where name stands for:\n# - norm -> Normal\n# - unif -> Uniform\n# - exp -> Exponential\n# - t -> Student's t\n# - f -> Snedecor's F (Fisher)\n# - chisq -> Chi squared\n# - pois -> Poisson\n# - binom -> Binomial\n# More distributions: ?Distributions\n\n\n# Sampling from a Normal - 100 random points from a N(0, 1)\nrnorm(n = 10, mean = 0, sd = 1)\n#ans>  [1]  0.3509 -0.3621  1.3767 -1.4890 -1.0637 -2.7372 -0.1486  0.0355 -0.5935\n#ans> [10] -0.2793\n\n# If you want to have always the same result, set the seed of the random number\n# generator\nset.seed(45678)\nrnorm(n = 10, mean = 0, sd = 1)\n#ans>  [1]  1.440 -0.720  0.671 -0.422  0.378 -1.667 -0.508  0.443 -1.799 -0.618\n\n# Plotting the density of a N(0, 1) - the Gauss bell\nx <- seq(-4, 4, l = 100)\ny <- dnorm(x = x, mean = 0, sd = 1)\nplot(x, y, type = \"l\")\n\n# Plotting the distribution function of a N(0, 1)\nx <- seq(-4, 4, l = 100)\ny <- pnorm(q = x, mean = 0, sd = 1)\nplot(x, y, type = \"l\", lwd = 3, main=\"The distribution function of a N(0, 1)\")\n\n# Computing the 95% quantile for a N(0, 1)\nqnorm(p = 0.95, mean = 0, sd = 1)\n#ans> [1] 1.64\n\n# All distributions have the same syntax: rname(n,...), dname(x,...), dname(p,...)  \n# and qname(p,...), but the parameters in ... change. Look them in ?Distributions\n# For example, here is que same for the uniform distribution\n\n# Sampling from a U(0, 1)\nset.seed(45678)\nrunif(n = 10, min = 0, max = 1)\n#ans>  [1] 0.9251 0.3340 0.2359 0.3366 0.7489 0.9327 0.3365 0.2246 0.6474 0.0808\n\n# Plotting the density of a U(0, 1)\nx <- seq(-2, 2, l = 100)\ny <- dunif(x = x, min = 0, max = 1)\nplot(x, y, type = \"l\")\n\n# Computing the 95% quantile for a U(0, 1)\nqunif(p = 0.95, min = 0, max = 1)\n#ans> [1] 0.95"},{"path":"practical-work-1.html","id":"working-directory","chapter":"Practical Work 1","heading":"1.8.6 Working directory","text":"working directory folder computer currently working. ask R open certain file, look working directory file, tell R save data file figure, save working directory.set working directory within RStudio can go Tools / Set working directory, use command setwd(), put complete path directory brackets, forget put path quotation marks \"\".know actual working directory use getwd().","code":""},{"path":"practical-work-1.html","id":"loading-data","chapter":"Practical Work 1","heading":"1.8.7 Loading Data","text":"read.table() function one primary ways import data set . help file ?read.table() contains details use function. can use function write.table() export data.Next show load data set Auto.data (Download  ).file csv format, use read.csv.Always try look file importing \n (Open text editor. See example first row containes variables names, columns separated , ; ..text editors, suggest Sublime Text Atom.\n","code":"\nAuto=read.table(\"Auto.data\",header=T,na.strings =\"?\")\n# For this file we needed to tell R that the first row is the\n# names of the variables.\n# na.strings tells R that any time it sees a particular character\n# or set of characters (such as a question mark), it should be\n# treated as a missing element of the data matrix. \ndim(Auto) # To see the dimensions of the data set\n#ans> [1] 397   9\nnrow(Auto) # To see the number of rows\n#ans> [1] 397\nncol(Auto) # To see the number of columns\n#ans> [1] 9\nAuto[1:4,] # The first 4 rows of the data set\n#ans>   mpg cylinders displacement horsepower weight acceleration year origin\n#ans> 1  18         8          307        130   3504         12.0   70      1\n#ans> 2  15         8          350        165   3693         11.5   70      1\n#ans> 3  18         8          318        150   3436         11.0   70      1\n#ans> 4  16         8          304        150   3433         12.0   70      1\n#ans>                        name\n#ans> 1 chevrolet chevelle malibu\n#ans> 2         buick skylark 320\n#ans> 3        plymouth satellite\n#ans> 4             amc rebel sst\n# Once the data are loaded correctly, we can use names()\n# to check the variable names.\nnames(Auto)\n#ans> [1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n#ans> [6] \"acceleration\" \"year\"         \"origin\"       \"name\""},{"path":"practical-work-1.html","id":"regression","chapter":"Practical Work 1","heading":"1.9 Regression","text":"","code":""},{"path":"practical-work-1.html","id":"the-lm-function","chapter":"Practical Work 1","heading":"1.9.1 The lm function","text":"going employ EU dataset. EU dataset contains 28 rows member states European Union (Country), number seats assigned different years (Seats2011, Seats2014), Cambridge Compromise apportionment (CamCom2011), countries population (Population2010,Population2013). Click   download EU dataset.two ways tell \n file want load/use/import save file write/export/save :write complete path files.set working directory put files .\nfollowing table contains handy cheat sheet equivalences  code statistical concepts associated linear regression.following:Download ‘EU’ dataset   .RData file load using function load.Compute regression CamCom2011 Population2010. Save model variable myModel.Access objects residuals coefficients myModel.Compute summary myModel store variable summaryMyModel.Access object sigma summaryMyModel.\n","code":"\n# Load the dataset, when we load an .RData using load()\n# function we do not attribute it to a name like we did\n# when we used read.table() or when we use read.csv()\n\nload(\"EU.RData\")\n# lm (for linear model) has the syntax: \n# lm(formula = response ~ predictor, data = data)\n# The response is the y in the model. The predictor is x.\n# For example (after loading the EU dataset)\nmod <- lm(formula = Seats2011 ~ Population2010, data = EU)\n\n# We have saved the linear model into mod, which now contains all the output of lm\n# You can see it by typing\nmod\n#ans> \n#ans> Call:\n#ans> lm(formula = Seats2011 ~ Population2010, data = EU)\n#ans> \n#ans> Coefficients:\n#ans>    (Intercept)  Population2010  \n#ans>       7.91e+00        1.08e-06\n\n# mod is indeed a list of objects whose names are\nnames(mod)\n#ans>  [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n#ans>  [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n#ans>  [9] \"na.action\"     \"xlevels\"       \"call\"          \"terms\"        \n#ans> [13] \"model\"\n\n# We can access these elements by $\n# For example\nmod$coefficients\n#ans>    (Intercept) Population2010 \n#ans>       7.91e+00       1.08e-06\n\n# The residuals\nmod$residuals\n#ans>        Germany         France United Kingdom          Italy          Spain \n#ans>         2.8675        -3.7031        -1.7847         0.0139        -3.5084 \n#ans>         Poland        Romania    Netherlands         Greece        Belgium \n#ans>         1.9272         1.9434         0.2142         1.8977         2.3994 \n#ans>       Portugal Czech Republic        Hungary         Sweden        Austria \n#ans>         2.6175         2.7587         3.2898         2.0163         2.0575 \n#ans>       Bulgaria        Denmark       Slovakia        Finland        Ireland \n#ans>         1.9328        -0.8790        -0.7606        -0.6813        -0.7284 \n#ans>      Lithuania         Latvia       Slovenia        Estonia         Cyprus \n#ans>         0.4998        -1.3347        -2.1175        -3.3552        -2.7761 \n#ans>     Luxembourg          Malta \n#ans>        -2.4514        -2.3553\n\n# The fitted values\nmod$fitted.values\n#ans>        Germany         France United Kingdom          Italy          Spain \n#ans>          96.13          77.70          74.78          72.99          57.51 \n#ans>         Poland        Romania    Netherlands         Greece        Belgium \n#ans>          49.07          31.06          25.79          20.10          19.60 \n#ans>       Portugal Czech Republic        Hungary         Sweden        Austria \n#ans>          19.38          19.24          18.71          17.98          16.94 \n#ans>       Bulgaria        Denmark       Slovakia        Finland        Ireland \n#ans>          16.07          13.88          13.76          13.68          12.73 \n#ans>      Lithuania         Latvia       Slovenia        Estonia         Cyprus \n#ans>          11.50          10.33          10.12           9.36           8.78 \n#ans>     Luxembourg          Malta \n#ans>           8.45           8.36\n\n# Summary of the model\nsumMod <- summary(mod)\nsumMod\n#ans> \n#ans> Call:\n#ans> lm(formula = Seats2011 ~ Population2010, data = EU)\n#ans> \n#ans> Residuals:\n#ans>    Min     1Q Median     3Q    Max \n#ans> -3.703 -1.951  0.014  1.980  3.290 \n#ans> \n#ans> Coefficients:\n#ans>                Estimate Std. Error t value Pr(>|t|)    \n#ans> (Intercept)    7.91e+00   5.66e-01    14.0  2.6e-13 ***\n#ans> Population2010 1.08e-06   1.92e-08    56.3  < 2e-16 ***\n#ans> ---\n#ans> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#ans> \n#ans> Residual standard error: 2.29 on 25 degrees of freedom\n#ans>   (1 observation effacée parce que manquante)\n#ans> Multiple R-squared:  0.992,   Adjusted R-squared:  0.992 \n#ans> F-statistic: 3.17e+03 on 1 and 25 DF,  p-value: <2e-16"},{"path":"practical-work-1.html","id":"boston","chapter":"Practical Work 1","heading":"1.9.2 Predicting House Value: Boston dataset","text":"going use dataset called Boston part MASS package. recordes median value houses 506 neighborhoods around Boston. task predict median house value (medv) using one predictor (lstat: percent households low socioeconomic status).STEP 1: Split datasetSTEP 2: Check LinearityIn order perfom linear regression , use function lm()fit simple linear regression medv response (dependent variable) lstat predictor independent variable, save model.run model, let’s visually check relationship x y linear.figure , modify following:Figure title.Axis titles.Shape observations colors.Sizes chosen shape.\nAccording plot, see relationship linear. Let’s try transformation explanatory variable lstat.Look plot, linear, can proceed perform lm():STEP 3: Run linear regression modelNotice basic information print model. give us slope \\((-12.2)\\) intercept \\((51.8)\\) linear model. Note looking log(lstat) lstat anymore. every one unit increase lstat, median value house decrease \\(e^{12.2}\\).\ndetailed information, can use summary() function:Now, access p-values standard errors coefficients, well \\(R^2\\).output states slope statistically significant different \\(0\\) t-value= \\(-25.9\\) (p-value \\(< 0.05\\)), means significant relationship percentage households low socioeconomic income median house value.relationship negative. percantage household low socioeconomic income increases, median house value decreases.Looking \\(R^2\\), can deduce \\(62.7\\%\\) model variation explained predictor log(lstat). probably low, indeed increase independent (explanatory) variables. can use names() function see pieces information stored linear model (model).obtain confidence interval linear model (model), can use confint() function:, \\(95\\%\\) confidence interval slope log(lstat) \\((-13.13, -11.28)\\). Notice confidence interval gives us result hypothesis test performed earlier, stating \\(95\\%\\) confident slope lstat zero (fact less zero, means relationship negative.)STEP 4: Plot regression modelNow, let’s plot regression line top data.Let’s play look plot, makes prettier!STEP 5: Assess modelFinal thing predict using fitted model. can use\npredict() function purpose:Now let’s assess model, computing th mean squared error (MSE). assess model created, using test data!\n◼\n","code":"\n# First, install the MASS package using the command: install.packages(\"MASS\")\n\n# load MASS package\nlibrary(MASS)\n\n# Check the dimensions of the Boston dataset\ndim(Boston)\n#ans> [1] 506  14\n# Split the data by using the first 400 observations as the training\n# data and the remaining as the testing data\ntrain = 1:400\ntest = -train\n\n# Speficy that we are going to use only two variables (lstat and medv)\nvariables = which(names(Boston) ==c(\"lstat\", \"medv\"))\ntraining_data = Boston[train, variables]\ntesting_data = Boston[test, variables]\n\n# Check the dimensions of the new dataset\ndim(training_data)\n#ans> [1] 400   2\n# Scatterplot of lstat vs. medv\nplot(training_data$lstat, training_data$medv)\n# Scatterplot of log(lstat) vs. medv\nplot(log(training_data$lstat), training_data$medv)\nmodel = lm(medv ~ log(lstat), data = training_data)\nmodel\n#ans> \n#ans> Call:\n#ans> lm(formula = medv ~ log(lstat), data = training_data)\n#ans> \n#ans> Coefficients:\n#ans> (Intercept)   log(lstat)  \n#ans>        51.8        -12.2\nsummary(model)\n#ans> \n#ans> Call:\n#ans> lm(formula = medv ~ log(lstat), data = training_data)\n#ans> \n#ans> Residuals:\n#ans>     Min      1Q  Median      3Q     Max \n#ans> -11.385  -3.908  -0.779   2.245  25.728 \n#ans> \n#ans> Coefficients:\n#ans>             Estimate Std. Error t value Pr(>|t|)    \n#ans> (Intercept)   51.783      1.097    47.2   <2e-16 ***\n#ans> log(lstat)   -12.203      0.472   -25.9   <2e-16 ***\n#ans> ---\n#ans> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#ans> \n#ans> Residual standard error: 5.6 on 398 degrees of freedom\n#ans> Multiple R-squared:  0.627,   Adjusted R-squared:  0.626 \n#ans> F-statistic:  669 on 1 and 398 DF,  p-value: <2e-16\nnames(model)\n#ans>  [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n#ans>  [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n#ans>  [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"\nmodel$coefficients\n#ans> (Intercept)  log(lstat) \n#ans>        51.8       -12.2\nconfint(model, level = 0.95)\n#ans>             2.5 % 97.5 %\n#ans> (Intercept)  49.6   53.9\n#ans> log(lstat)  -13.1  -11.3\n# Scatterplot of lstat vs. medv\nplot(log(training_data$lstat), training_data$medv)\n\n# Add the regression line to the existing scatterplot\nabline(model)\n# Scatterplot of lstat vs. medv\nplot(log(training_data$lstat), training_data$medv,\nxlab = \"Log Transform of % of Houshold with Low Socioeconomic Income\",\nylab = \"Median House Value\",\ncol = \"red\",\npch = 20)\n\n# Make the line color blue, and the line's width =3 (play with the width!)\nabline(model, col = \"blue\", lwd =3)\n# Predict what is the median value of the house with lstat= 5%\npredict(model, data.frame(lstat = c(5)))\n#ans>    1 \n#ans> 32.1\n# Predict what is the median values of houses with lstat= 5%, 10%, and 15%\npredict(model, data.frame(lstat = c(5,10,15)), interval = \"prediction\")\n#ans>    fit  lwr  upr\n#ans> 1 32.1 21.1 43.2\n#ans> 2 23.7 12.7 34.7\n#ans> 3 18.7  7.7 29.8\n# Save the testing median values for houses (testing y) in y\ny = testing_data$medv\n\n# Compute the predicted value for this y (y hat)\ny_hat = predict(model, data.frame(lstat = testing_data$lstat))\n\n# Now we have both y and y_hat for our testing data. \n# let's find the mean square error\nerror = y-y_hat\nerror_squared = error^2\nMSE = mean(error_squared)\nMSE\n#ans> [1] 17.7"},{"path":"multiple-linear-regression.html","id":"multiple-linear-regression","chapter":"2 Multiple Linear Regression","heading":"2 Multiple Linear Regression","text":"Simple linear regression useful approach predicting response basis single predictor variable. However, practice often one predictor.\nprevious chapter, took example prediction housing prices considering size house. single feature \\(X\\), size house. now imagine size house feature also knew number bedrooms, number flours age house years. seems like give us lot information predict price.","code":""},{"path":"multiple-linear-regression.html","id":"the-model","chapter":"2 Multiple Linear Regression","heading":"2.1 The Model","text":"general, suppose \\(p\\) distinct predictors. multiple linear regression model takes form\\[ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p + \\epsilon \\]\\(X_j\\) represents \\(j\\)th predictor \\(\\beta_j\\) quantifies association variable response. interpret \\(\\beta_j\\) average effect \\(Y\\) one unit increase \\(X_j\\), holding predictors fixed.matrix terms, supposing \\(n\\) observations \\(p\\) variables, need define following matrices:\\[\\begin{equation}\n\\textbf{Y}_{n \\times 1} = \\begin{pmatrix}\n    Y_{1} \\\\\n    Y_{2} \\\\\n    \\vdots \\\\\n    Y_{n}\n\\end{pmatrix}   \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,  \\textbf{X}_{n \\times (p+1)}  = \\begin{pmatrix}\n    1      & X_{11} & X_{12} & \\dots  & X_{1p} \\\\\n    1      & X_{21} & X_{22} & \\dots  & X_{2p} \\\\\n    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n    1      & X_{n1} & X_{n2} & \\dots  & X_{np}\n\\end{pmatrix}\n\\end{equation}\\]\\[\\begin{equation}\n{\\mathbb{\\beta}}_{(p+1) \\times 1} = \\begin{pmatrix}\n    \\beta_{0} \\\\\n    \\beta_{1} \\\\\n    \\vdots \\\\\n    \\beta_{p}\n    \\end{pmatrix}   \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,  {\\epsilon}_{n \\times 1} = \\begin{pmatrix}\n        \\epsilon_{1} \\\\\n        \\epsilon_{2} \\\\\n        \\vdots \\\\\n        \\epsilon_{n}\n    \\end{pmatrix}\n\\end{equation}\\]matrix terms, general linear regression model \\[ \\textbf{Y}_{n \\times 1} = \\textbf{X}_{n \\times (p+1)} {\\mathbb{\\beta}}_{(p+1) \\times 1} + {\\epsilon}_{n \\times 1} \\],\\(\\textbf{Y}\\) vector responses.\\(\\mathbb{\\beta}\\) vector parameters.\\(\\textbf{X}\\) matrix constants.\\(\\epsilon\\) vector independent normal (Gaussian) random variables.","code":""},{"path":"multiple-linear-regression.html","id":"estimating-the-regression-coefficients","chapter":"2 Multiple Linear Regression","heading":"2.2 Estimating the Regression Coefficients","text":"case simple linear regression setting, regression coefficients \\(\\beta_{0}, \\beta_{1}, \\ldots, \\beta_{p}\\) unknown, must estimated. Given estimates \\(\\hat{\\beta_{0}}, \\hat{\\beta_{1}}, \\ldots, \\hat{\\beta_{p}}\\), can make predictions using formula\\[ \\hat{y} = \\hat{\\beta_{0}} + \\hat{\\beta_{1}} x_1 + \\hat{\\beta_{2}} x_2 + \\ldots + \\hat{\\beta_{p}} x_p \\]choose \\(\\beta_{0}, \\beta_{1}, \\ldots, \\beta_{p}\\) minimize residual sum squares\\[ \\begin{aligned}\nRSS &= \\sum_{=1}^{n} (y_i - \\hat{y}_i)^2 \\\\\n    &= \\sum_{=1}^{n} (y_i - \\hat{\\beta_0} - \\hat{\\beta_1} {x}_{i1}  - \\hat{\\beta_2} {x}_{i2} - \\ldots  -  \\hat{\\beta_p} {x}_{ip})^2 \\\\\n\\end{aligned}\n\\]values \\(\\hat{\\beta_{0}}, \\hat{\\beta_{1}}, \\ldots, \\hat{\\beta_{p}}\\) minimize RSS multiple least squares regression coefficient estimates, calculated using formula (matrix terms):\\[ \\hat{\\beta} = (\\textbf{X}^T \\textbf{X})^{-1}\\textbf{X}^T \\textbf{Y} \\]obtain \\(\\hat{\\beta}\\), can write residual sum squares \\[ RSS = (\\textbf{Y}-\\textbf{X}\\beta)^T(\\textbf{Y}-\\textbf{X}\\beta) \\]quadratic function \\(p+1\\) parameters. Differentiating respect \\(\\beta\\) obtain\n\\[ \\begin{aligned}\n\\frac{\\partial RSS}{\\partial \\beta} &= -2\\textbf{X}^T(\\textbf{Y}-\\textbf{X}\\beta) \\\\\n\\frac{\\partial^2 RSS}{\\partial \\beta \\partial \\beta^T}   &= 2\\textbf{X}^T\\textbf{X}.\\\\\n\\end{aligned}\n\\]Assuming (moment) \\(\\textbf{X}\\) full column rank, hence \\(\\textbf{X}^T\\textbf{X}\\) positive definite7, set first derivative zero\\[\\textbf{X}^T(\\textbf{Y}-\\textbf{X}\\beta)=0\\]\nobtain unique solution\\[ \\hat{\\beta} = (\\textbf{X}^T \\textbf{X})^{-1}\\textbf{X}^T \\textbf{Y} \\]Note 1:remarkable property matrix algebra results general linear regression model matrix notation appear exactly simple linear regression model. degrees freedom constants related number \\(X\\) variables dimensions matrices different. means similarities \\(\\hat{\\beta} = (\\textbf{X}^T \\textbf{X})^{-1}\\textbf{X}^T \\textbf{Y}\\) \\(\\hat\\beta_1=(s_x^2)^{-1}s_{xy}\\) simple linear model: related covariance \\(\\mathbf{X}\\) \\(\\mathbf{Y}\\) weighted variance \\(\\mathbf{X}\\).Note 2:\\(\\textbf{X}^T \\textbf{X}\\) noninvertible, common causes might :Redundant features, two features closely related (.e. linearly dependent)many features (e.g. \\(p \\geq n\\)). case, delete features use “regularization” (, maybe, explained later lesson).","code":""},{"path":"multiple-linear-regression.html","id":"some-important-questions","chapter":"2 Multiple Linear Regression","heading":"2.3 Some important questions","text":"perform multiple linear regression, usually interested answering important questions.least one predictors \\(X_1 ,X_2 ,\\ldots,X_p\\) useful predicting response?predictors help explain \\(Y\\), subset predictors useful?well model fit data?Given set predictor values, response value predict, accurate prediction?Relationship Response Predictors?\\(F\\)-StatisticRecall simple linear regression setting, order determine whether relationship response predictor can simply check whether \\(\\beta_1 = 0\\). multiple regression setting \\(p\\) predictors, need ask whether regression coefficients zero, .e. whether \\(\\beta_1 = \\beta_2 = \\ldots = \\beta_p = 0\\). simple linear regression setting, use hypothesis test answer question. test null hypothesis,\\[ H_0 : \\beta_1 = \\beta_2 = \\ldots = \\beta_p = 0 \\]versus alternative hypothesis\\[ H_1 : \\text{least one} \\, \\beta_j \\, \\text{non-zero} \\]hypothesis test performed computing \\(F\\)-statistic (Fisher):\\[ F = \\frac{ (\\text{TSS} - \\text{RSS})/p}{\\text{RSS}/(n-p-1)} \\sim F_{p,n-p-1} \\], simple linear regression, \\(\\text{TSS} = \\sum (y_i - \\bar{y})^2\\) \\(\\text{RSS} = \\sum (y_i - \\hat{y}_i)^2\\).Note \\(F_{p,n-p-1}\\) represents Fisher-Snedecor’s \\(F\\) distribution \\(p\\) \\(n-p-1\\) degrees freedom. \\(H_0\\) true, \\(F\\) expected small since ESS8 close zero (little variation explained regression model since \\(\\hat{\\boldsymbol{\\beta}}\\approx\\mathbf{0}\\)).question ask : whole regression explaining anything ? answer comes \\(F\\)-test ANOVA (ANalysis VAriance) table. get ANOVA table:ANOVA table many pieces information. care \\(F\\) Ratio corresponding p-value. compare \\(F\\) Ratio \\(F_{(p,n-p-1)}\\) corresponding \\(\\alpha\\) value (error).\n“ANOVA table” broad concept statistics, different\nvariants. covering basic ANOVA table \nrelation \\(\\text{SST} = \\text{SSR} + \\text{SSE}\\). However, sophistications possible \n\\(\\text{SSR}\\) decomposed \nvariations contributed predictor. particular, \nmultiple linear regression\nr fontawesome::fa(“r-project”, fill =“steelblue”)’s\nanova implements sequential (type ) ANOVA\ntable, previous table!\nanova function  takes model input returns following sequential ANOVA table9:ESS\\(_j\\) represents explained sum squares (regression sum squares) associated inclusion \\(X_j\\) model predictors \\(X_1,\\ldots,X_{j-1}\\), :\n\\[\n\\text{ESS}_j=\\text{ESS}(X_1,\\ldots,X_j)-\\text{ESS}(X_1,\\ldots,X_{j-1}).\n\\]\n\\(p\\)-values \\(p_1,\\ldots,p_p\\) correspond testing hypotheses\n\\[\\begin{align*}\nH_0:\\beta_j=0\\quad\\text{vs.}\\quad H_1:\\beta_j\\neq 0,\n\\end{align*}\\]\ncarried inside linear model \\(Y=\\beta_0+\\beta_1X_1+\\ldots+\\beta_jX_j+\\varepsilon\\). like \\(t\\)-test \\(\\beta_j\\) model predictors \\(X_1,\\ldots,X_j\\). Recall \\(F\\)-test version ANOVA table.p-valuesThe p-values provide information whether individual predictor related response, adjusting predictors. Let’s look following table obtain general using statistical software exampleIn table following model\\[ Y = 2.939 + 0.046 X_1 + 0.189 X_2 - 0.001 X_3 \\]Note individual predictor \\(t\\)-statistic p-value reported. p-values indicate \\(X_1\\) \\(X_2\\) related \\(Y\\), evidence \\(X_3\\) associated \\(Y\\), presence two.Deciding Important VariablesThe direct approach called subsets best subsets regression: compute least squares fit possible subsets choose based criterion balances training error model size.However often can’t examine possible models, since \\(2^p\\) ; example \\(p = 40\\) billion models! Instead need automated approach searches subset . two commonly use approaches:Forward selection:Begin null model — model contains intercept (constant) predictors.Fit \\(p\\) simple linear regressions add null model variable results lowest RSS.Add model variable results lowest RSS amongst two-variable models.Continue stopping rule satisfied, example remaining variables p-value threshold.Backward selection:Start variables model.Remove variable largest p-value — , variable least statistically significant.new \\((p − 1)\\)-variable model fit, variable largest p-value removed.Continue stopping rule reached. instance, may stop remaining variables significant p-value defined significance threshold.systematic criteria choosing “optimal” member path models produced forward backward stepwise selection. include Mallow’s \\(C_p\\) , Akaike information criterion (AIC), Bayesian information criterion (BIC), adjusted \\(R^2\\) Cross-validation (CV).Model FitTwo common numerical measures model fit RSE \\(R^2\\), fraction variance explained. quantities computed interpreted fashion simple linear regression. Recall simple regression, \\(R^2\\) square correlation response variable. multiple linear regression, turns equals \\(Cor(Y, \\hat{Y})^2\\) , square correlation response fitted linear model; fact one property fitted linear model maximizes correlation among possible linear models. \\(R^2\\) value close 1 indicates model explains large portion variance response variable.general RSE defined \\[ \\text{RSE} = \\sqrt{\\frac{1}{n-p-1}\\text{RSS}} \\]","code":""},{"path":"multiple-linear-regression.html","id":"other-consid","chapter":"2 Multiple Linear Regression","heading":"2.3.1 Other Considerations in Regression Model","text":"Qualitative PredictorsIf categorial (qualitative) variable (feature), fit regression equation?example, \\(X_1\\) gender (male female).can code, example, male = 0 female = 1.Suppose \\(X_2\\) quantitative variable, regression equation becomes:\\[ Y_i \\approx \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 = \\begin{cases}\n  \\beta_0 + \\beta_2 X_2 & \\text{ male} \\\\\n  \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 & \\text{ female}\n\\end{cases} \\]Another possible coding scheme let male = -1 female = 1, regression equation :\\[ Y_i \\approx \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 = \\begin{cases}\n  \\beta_0 -\\beta_1 X_1 + \\beta_2 X_2 & \\text{ male} \\\\\n  \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 & \\text{ female}\n\\end{cases} \\]Interaction TermsWhen effect \\(Y\\) increasing \\(X_1\\) depends another \\(X_2\\).may case try model\\[ Y_i = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2 \\]\\(X_1 X_2\\) Interaction term.","code":""},{"path":"multiple-linear-regression.html","id":"how-to-select-the-best-performing-model","chapter":"2 Multiple Linear Regression","heading":"2.4 How to select the best performing model","text":"trying different linear models, need make choice model want use. specifically, questions one can ask: “determine model suits best data? just look R square, SSE, etc.?” “interpretation model (quadratic, root, etc.) different, won’t issue?”second question can answered easily. First, find model best suits data interpret results. good ideas data might explained. However, interpret best model, . Now address first question. Note multiple ways select best model. addition, approach applies univariate models (simple models) whith just one input variable.Use following interactive application play around different datasets models. Notice parameters change become confident assessing simple linear models.","code":""},{"path":"multiple-linear-regression.html","id":"use-the-adjusted-r_adj2-for-multivariate-models","chapter":"2 Multiple Linear Regression","heading":"Use the Adjusted \\(R_{adj}^2\\) for multivariate models","text":"use one input variable, adjusted \\(R_{adj}^2\\) value gives good indication well model performs. illustrates much variation explained model.contrast simple \\(R^2\\)10, adjusted adjusted \\(R_{adj}^2\\)11 takes number input factors account. penalizes many input factors favors parsimonious models.adjusted \\(R_{adj}^2\\) sensitive amount noise data. , compare indicator models dataset comparing across different datasets.\nFigure 2.1: Comparison \\(R^2\\) \\(R^2_{\\text{adj}}\\) \\(n=200\\) \\(p\\) ranging \\(1\\) \\(198\\). \\(M=100\\) datasets simulated first two predictors significant. thicker curves mean color’s curves.\nFigure 2.1 contains results experiment 100 datasets simulated first two predictors significant. can see \\(R^2\\) increases linearly number predictors considered, although first two ones important! contrary, \\(R^2_\\text{adj}\\) increases first two variables flat average, huge variability \\(p\\) approaches \\(n-2\\). experiment evidences \\(R^2_\\text{adj}\\) adequate \\(R^2\\) evaluating fit multiple linear regression.","code":""},{"path":"multiple-linear-regression.html","id":"have-a-look-at-the-residuals-or-error-terms","chapter":"2 Multiple Linear Regression","heading":"Have a look at the residuals or error terms","text":"often ignored error terms -called residuals. often tell might think. residuals difference predicted values actual values. benefit can show magnitude well direction errors.Let’s look example:, try predict polynomial dataset linear function. Analyzing residuals shows areas model upward downward bias.50 < x < 100, residuals zero. area, actual values higher predicted values — model downward bias.100 < x < 150, however, residuals zero. Thus, actual values lower predicted values — model upward bias.always good know, whether model suggests high low values. usually want patterns like .residuals zero average (indicated mean) equally distributed. Predicting dataset polynomial function 3 degrees suggests much better fit:addition, can observe whether variance errors increases. statistics, called Heteroscedasticity. can fix easily robust standard errors. Otherwise, hypothesis tests likely wrong.","code":""},{"path":"multiple-linear-regression.html","id":"histogram-of-residuals","chapter":"2 Multiple Linear Regression","heading":"Histogram of residuals","text":"Finally, histogram summarizes magnitude error terms. provides information bandwidth errors indicates often errors occurred.screenshots show two models dataset. first histogram, errors occur within range -338 520. second histogram, errors occur within -293 401. outliers much lower. Furthermore, errors model second histogram closer zero. favor second model.\n◼\n","code":""},{"path":"pw-2.html","id":"pw-2","chapter":"PW 2","heading":"PW 2","text":"","code":""},{"path":"pw-2.html","id":"multiple-linear-regression-1","chapter":"PW 2","heading":"Multiple Linear Regression","text":"practical work, continue analysis Boston data set started last week (section 1.9.2). Recall dataset records median value houses 506 neighborhoods around Boston. task predict median house value (medv).1. Load Boston dataset MASS package.2. Split dataset training set testing set. (keep variables Boston data set)3. Check linear relationship variables medv age. (use cor() function).4. Fit model housing prices function age plot observations regression line.5. Train regression model using lstat age predictors median house value. (Remember transformed lstat, use transformation ). obtained model?6. Print summary obtained regression model.7. model whole significant? Answer question must detailed.8. predictors significant ?9. Train new model using variables dataset. (can use . short cut instead writing variables names)10. using variables predictors, didn’t transform lstat. Re train model using log(lstat) instead lstat.11. \\(R^2\\) improve ?12. see correlated variables print correlation matrix using cor() function (round correlations 2 digits).13. Visualize correlations using corrplot package. , install corrplot package, load , use function corrplot.mixed(). See link  examples understand use .14. correlation tax rad?15. Run model without tax. happens \\(R^2\\) ? F-statistic?course \\(R^2\\) go little lower deleted one variables. check model significance (F-statistic) gets higher, means p-values gets lower thus model significant without tax.16. Calculate mean squared error (MSE) last model.AnovaNext apply analysis variances (ANOVA) order test significant difference means two groups \\(\\) \\(j\\) (Consider group \\(\\) suburbs bounding river \\(j\\) suburbs ). hypotheses \\[ H_0 : \\mu_i = \\mu_j \\]\\[ H_1 : \\mu_i \\neq \\mu_j \\]\\(\\mu_i\\) mean medv group \\(\\).17. Boston data set categorical variable chas corresponds Charles River (= 1 suburb bounds river; 0 otherwise). Use command str() see variable present dataset. many suburbs data set bound Charles river?18. Create Boxplots median value houses respect variable chas. observe difference median value houses respect neighborhood Charles River?19. Calculate \\(\\mu_i\\) \\(\\mu_j\\) (one line using function aggregate()).20. Apply ANOVA test medv respect chas (use function aov()). Print result summary . conclude ?Qualitative predictors  starting next question, please read section 2.3.1 Appendix D using qualitative predictors regression.going use categorical variable chas corresponds Charles River (= 1 suburb bounds river; 0 otherwise). Using str() command can notice variable codified factor, values 0 1, already dummyfied.21. Fit new model predictors Charles River Crime Rate. Interpret coefficients model conclude presence river adds valuable information explaining house price.22. chas significant well presence predictors?Interaction termsAs saw section 2.3.1 may sometimes try models interaction terms. Let’s say two predictors \\(X_1\\) \\(X_2\\), way adding interactions lm : *. operator : adds term \\(X_1X_2\\) * adds \\(X_1\\), \\(X_2\\), \\(X_1X_2\\).23. Fit model first order interaction term predictors lstat age. Print summary.24. Fit model first order interaction terms.","code":""},{"path":"pw-2.html","id":"reporting","chapter":"PW 2","heading":"Reporting","text":" packages make easy create reproducible web-based reports. , click File -> Knit document File -> Compile report... output html report containing results  codes.\nfile named report.R, report named report.html.25. Compile report based script.Make sure latest version Rstudio.problems compiling (problem installing packages, etc..) close Rstudio reopen administrative tools retry.ready submit report (.html file) end class.report must named: YouLastName_YourFirstName_WeekNumber.html\n◼\n","code":""},{"path":"logistic-regression.html","id":"logistic-regression","chapter":"3 Logistic Regression","heading":"3 Logistic Regression","text":"","code":""},{"path":"logistic-regression.html","id":"introduction-1","chapter":"3 Logistic Regression","heading":"3.1 Introduction","text":"previous chapters discussed linear regression model, assumes response variable \\(Y\\) quantitative. many situations, response variable instead qualitative (categorical). example, eye color qualitative, taking values blue, brown, green.process predicting qualitative responses known classification.Given feature vector \\(X\\) qualitative response \\(Y\\) taking values set \\(\\mathcal{C}\\), classification task build function \\(C(X)\\) takes input feature vector \\(X\\) predicts value \\(Y\\); .e. \\(C(X) \\\\mathcal{C}\\). often interested estimating probabilities \\(X\\) belongs category \\(\\mathcal{C}\\).\\(c\\) category (\\(c \\\\mathcal{C}\\)), probability \\(X\\) belongs \\(c\\) mean \\(p(X \\c) = \\mathbb{P}(Y=c|X)\\).binomial binary logistic regression, outcome can two possible types values (e.g. “Yes” “”, “Success” “Failure”). Multinomial logistic refers cases outcome can three possible types values (e.g., “good” vs. “good” vs. “best”). Generally outcome coded “0” “1” binary logistic regression.","code":""},{"path":"logistic-regression.html","id":"logistic-regression-1","chapter":"3 Logistic Regression","heading":"3.2 Logistic Regression","text":"Consider data set response falls one two categories, Yes . Rather modeling response \\(Y\\) directly, logistic regression models probability \\(Y\\) belongs particular category.","code":""},{"path":"logistic-regression.html","id":"the-logistic-model","chapter":"3 Logistic Regression","heading":"3.2.1 The Logistic Model","text":"Let us suppose response two categories use generic 0/1 coding response. model relationship \\(p(X) = \\mathbb{P}(Y = 1|X)\\) \\(X\\)?simplest situation \\(Y\\) binary: can take two values, codified convenience \\(1\\) (success) \\(0\\) (failure).formally, binary variable known Bernoulli variable, simplest non-trivial random variable. say \\(Y\\sim\\mathrm{Ber}(p)\\), \\(0\\leq p\\leq1\\), \\[\nY=\\left\\{\\begin{array}{ll}1,&\\text{probability }p,\\\\0,&\\text{probability }1-p,\\end{array}\\right.\n\\] , equivalently, \\(\\mathbb{P}[Y=1]=p\\) \\(\\mathbb{P}[Y=0]=1-p\\), can written compactly \\[\\begin{aligned}\n\\mathbb{P}[Y=y]=p^y(1-p)^{1-y},\\quad y=0,1.\n\\end{aligned}\\] Recall binomial variable size \\(n\\) probability \\(p\\), \\(\\mathrm{Bi}(n,p)\\), obtained adding \\(n\\) independent \\(\\mathrm{Ber}(p)\\) (\\(\\mathrm{Ber}(p)\\) \\(\\mathrm{Bi}(1,p)\\)).\nBernoulli variable \\(Y\\) \ncompletely determined \\(p\\).\nmean variance:\n\n\\(\\mathbb{E}[Y]=p\\times1+(1-p)\\times0=p\\)\n\n\\(\\mathbb{V}\\mathrm{ar}[Y]=p(1-p)\\).\n\nparticular, recall \\(\\mathbb{P}[Y=1]=\\mathbb{E}[Y]=p\\).\nAssume \\(Y\\) binary/Bernoulli variable \\(X\\) predictors associated (particular assumptions ). purpose logistic regression estimate \\[\np(x)=\\mathbb{P}[Y=1|X=x]=\\mathbb{E}[Y|X=x],\n\\] , probability \\(Y=1\\) changing according particular values, denoted \\(x\\), random variables \\(X\\).linear regression? tempting possibility consider model \\[\np(x)=\\beta_0+\\beta_1 x.\n\\] However, model run problems inevitably: negative probabilities probabilities larger one (\\(p(x) < 0\\) values \\(X\\) \\(p(X) > 1\\) others). avoid problem, solution consider function encapsulate value \\(z=\\beta_0+\\beta_1 x\\), \\(\\mathbb{R}\\), map \\([0,1]\\). several alternatives , based distribution functions \\(F:\\mathbb{R}\\longrightarrow[0,1]\\) deliver \\(y=F(z)\\[0,1]\\). Many functions meet description. logistic regression, use logistic function,\\[ p(X) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1+e^{\\beta_0 + \\beta_1 X}} \\]\nmatter values \\(\\beta_0\\),\n\\(\\beta_1\\) \\(X\\) take, \\(p(X)\\) values 0 \n1.\n\nlogistic function always produce S-shaped\ncurve.\n\nlogistic distribution function : \\[F(z)=\\mathrm{logistic}(z)=\\frac{e^z}{1+e^z}=\\frac{1}{1+e^{-z}}.\\]\nbit manipulation previous equation, find \\[ \\frac{p(X)}{1-p(X)} = e^{\\beta_0 + \\beta_1 X} \\]\nquantity \\(p(X)/[1−p(X)]\\) \ncalled odds, can take value \\(0\\) \\(\\infty\\).\ntaking logarithm sides equation, arrive at12\\[ \\log( \\frac{p(X)}{1-p(X)} ) = \\beta_0 + \\beta_1 X \\]\nleft-hand side called log-odds logit.\nsee logistic regression model logit linear \nX.\n","code":""},{"path":"logistic-regression.html","id":"estimating-the-regression-coefficients-1","chapter":"3 Logistic Regression","heading":"3.2.2 Estimating the Regression Coefficients","text":"estimate \\(\\beta_0\\) \\(\\beta_1\\) using Maximum Likelihood Estimation method (MLE). basic intuition behind using maximum likelihood fit logistic regression model follows: seek estimates \\(\\beta_0\\) \\(\\beta_1\\) predicted probability \\(\\hat{p}(x_i)\\) response individual, corresponds closely possible individual’s observed response status (recall response \\(Y\\) categorical). likelihood function \\[ l(\\beta_0,\\beta_1) = \\prod_{=1}^n p(x_i)^{Y_i}(1-p(x_i))^{1-Y_i}. \\]likelihood probability data based model. gives probability observed zeros ones data. estimates \\(\\hat{\\beta_0}\\) \\(\\hat{\\beta_1}\\) chosen maximize likelihood function. interpretation likelihood function following:\\(\\prod_{=1}^n\\) appears sample elements assumed independent computing probability observing whole sample \\((x_{1},y_1),\\ldots,(x_{n},y_n)\\). probability equal product probabilities observing \\((x_{},y_i)\\).\\(p(x_i)^{Y_i}(1-p(x_i))^{1-Y_i}\\) probability observing \\((x_{},Y_i)\\).\nlinear regression setting, least squares approach \nspecial case maximum likelihood.\ngive mathematical details maximum likelihood estimate parameters. use  fit logistic regression models (using glm function).Use following application (also available ) see log-likelihood changes respect values \\((\\beta_0,\\beta_1)\\) three data patterns. logistic regression fit dependence \\(\\beta_0\\) (horizontal displacement) \\(\\beta_1\\) (steepness curve). Recall effect sign \\(\\beta_1\\) curve: positive, logistic curve \\(s\\) form; negative, form reflected \\(s\\).Note animation displayed first time browsed (reason hosted https websites auto-signed SSL certificates). see , click link . get warning browser saying “connection private”. Click “Advanced” allow exception browser. next time animation show correctly.","code":""},{"path":"logistic-regression.html","id":"prediction-1","chapter":"3 Logistic Regression","heading":"3.2.3 Prediction","text":"ExampleIn example, \\(\\hat{\\beta_0} = -10.6513\\) \\(\\hat{\\beta_1} = 0.0055\\). produces blue curve separates data following figure,prediction, use model built estimated parameters predict probabilities. example,\\(X=1000\\),\\[ \\hat{p}(X) = \\frac{e^{\\hat{\\beta_0} + \\hat{\\beta_1} X}}{1+e^{\\hat{\\beta_0} + \\hat{\\beta_1} X}} = \\frac{e^{-10.6513+0.0055 \\times 1000}}{1+e^{-10.6513+0.0055 \\times 1000}} = 0.006\\]\\(X=2000\\),\\[ \\hat{p}(X) = \\frac{e^{\\hat{\\beta_0} + \\hat{\\beta_1} X}}{1+e^{\\hat{\\beta_0} + \\hat{\\beta_1} X}} = \\frac{e^{-10.6513+0.0055 \\times 2000}}{1+e^{-10.6513+0.0055 \\times 2000}} = 0.586\\]","code":""},{"path":"logistic-regression.html","id":"multiple-logistic-regression","chapter":"3 Logistic Regression","heading":"3.3 Multiple Logistic Regression","text":"now consider problem predicting binary response using multiple predictors. analogy extension simple multiple linear regression previous chapters, can generalize simple logistic regression equation follows:\\[ \\log( \\frac{p(X)}{1-p(X)} ) = \\beta_0 + \\beta_1 X_1 +  \\ldots + \\beta_p X_p\\]\\(X=(X_1,\\ldots,X_p)\\) \\(p\\) predictors. equation can rewritten \\[ p(X) = \\frac{e^{\\beta_0 + \\beta_1 X_1 +  \\ldots + \\beta_p X_p}}{1+e^{\\beta_0 + \\beta_1 X_1 +  \\ldots + \\beta_p X_p}} \\]Just simple logistic regression use maximum likelihood method estimate \\(\\beta_0,\\beta_1,\\ldots,\\beta_p\\).","code":""},{"path":"logistic-regression.html","id":"logreg-examps","chapter":"3 Logistic Regression","heading":"3.4 Example","text":"","code":""},{"path":"logistic-regression.html","id":"logreg-examps-challenger","chapter":"3 Logistic Regression","heading":"3.4.1 Case study: The Challenger disaster","text":"Challenger disaster occurred 28th January 1986, NASA Space Shuttle orbiter Challenger broke apart disintegrated 73 seconds flight, leading deaths seven crew members. accident deeply shocked US society, part due attention mission received presence Christa McAuliffe, first astronaut-teacher. , NASA TV broadcasted live launch US public schools, resulted millions school children witnessing accident. accident serious consequences NASA credibility resulted interruption 32 months shuttle program. Presidential Rogers Commission (formed astronaut Neil . Armstrong Nobel laureate Richard P. Feynman, among others) created investigate disaster.\nFigure 3.1: Challenger launch posterior explosion, broadcasted live NBC 28/01/1986.\nRogers Commission elaborated report (Presidential Commission Space Shuttle Challenger Accident 1986) findings. commission determined disintegration began failure O-ring seal solid rocket motor due unusual cold temperatures (-0.6 Celsius degrees) launch. failure produced breach burning gas solid rocket motor compromised whole shuttle structure, resulting disintegration due extreme aerodynamic forces. problematic O-rings something known: night launch, three-hour teleconference motor engineers NASA management, discussing effect low temperature forecasted launch O-ring performance. conclusion, influenced Figure 3.2a, :“Temperature data [] conclusive predicting primary O-ring blowby.”\nFigure 3.2: Number incidents O-rings (filed joints) versus temperatures. Panel includes flights incidents. Panel b contains flights (without incidents).\nRogers Commission noted major flaw Figure 3.2a: flights zero incidents excluded plot felt flights contribute information temperature effect (Figure 3.2b). Rogers Commission concluded:“careful analysis flight history O-ring performance revealed correlation O-ring damage low temperature”.purpose case study, inspired (Dalal, Fowlkes, Hoadley 1989), quantify influence temperature probability least one incident related O-rings. Specifically, want address following questions:Q1. temperature associated O-ring incidents?Q2. way temperature affecting probability O-ring incidents?Q3. predicted probability incidient O-ring temperature launch day?try answer questions challenger (  dataset ). dataset contains (shown table ) information regarding state solid rocket boosters launch13 23 flights. row , among others, following variables:fail.field, fail.nozzle: binary variables indicating whether incident O-rings field joints nozzles solid rocket boosters. 1 codifies incident 0 absence. analysis, focus O-rings field joint determinants accident.temp: temperature day launch. Measured Celsius degrees.pres.field, pres.nozzle: leak-check pressure tests O-rings. tests assured rings seal joint.\nTable 3.1: challenger dataset.\nLet’s begin analysis replicating Figures 3.2a 3.2b checking linear regression right tool answering Q1–Q3. , make two scatterplots nfails.field (number total incidents field joints) function temp, first one excluding launches without incidents (subset = nfails.field > 0) second one data.fundamental problem using linear regression data: response continuous. consequence, linearity errors around mean normal (indeed, strongly non normal). can check corresponding diagnostic plots:Although linear regression adequate tool data, able detect obvious difference two plots:trend launches incidents flat, hence suggesting dependence temperature (Figure 3.2a). one arguments behind NASA’s decision launching rocket temperature -0.6 degrees.However, trend launches indicates clear negative dependence temperature number incidents! (Figure 3.2b). Think way: minimum temperature launch without incidents ever recorded 18 degrees, Challenger launched -0.6 without clearly knowing effects low temperatures.Instead trying predict number incidents, concentrate modeling probability expecting least one incident given temperature, simpler also revealing approach. words, look estimate following curve: \\[\np(x)=\\mathbb{P}(\\text{incident}=1|\\text{temperature}=x)\n\\] fail.field temp. probability can properly modeled linear function like \\(\\beta_0+\\beta_1x\\), since inevitably fall outside \\([0,1]\\) value \\(x\\) (negative probabilities probabilities larger one). technique solves problem logistic regression. logistic model case \\[\n\\mathbb{P}(\\text{incident}=1|\\text{temperature}=x)=\\text{logistic}\\left(\\beta_0+\\beta_1x\\right)=\\frac{1}{1+e^{-(\\beta_0+\\beta_1x)}},\n\\] \\(\\beta_0\\) \\(\\beta_1\\) unknown.Let’s fit model data estimating \\(\\beta_0\\) \\(\\beta_1\\).\nglm() function fits generalized\nlinear models, class models \nincludes logistic regression. syntax glm()\nfunction similar lm(), except must\npass argument family=binomial order tell \n run logistic regression rather type \ngeneralized linear model.\nsummary logistic model notably different linear regression, methodology behind quite different. Nevertheless, tests significance coefficient. obtain temp significantly different zero, least level \\(\\alpha=0.05\\). Therefore can conclude temperature indeed affecting probability incident O-rings (answers Q1).coefficient temp, \\(\\hat\\beta_1\\), can regarded “correlation temperature probability least one incident”. correlation, evidenced sign \\(\\hat\\beta_1\\), negative. Let’s plot fitted logistic curve see indeed probability incident temperature negatively correlated:sight curve summary model can conclude temperature increasing probability O-ring incident (Q2). Indeed, confidence intervals coefficients show significative negative correlation level \\(\\alpha=0.05\\):Finally, probability least one incident O-rings launch day \\(0.9996\\) according fitted logistic model (Q3). easily obtained:aware type = \"response\" different meaning logistic regression. linear models returns CI prediction. , type = \"response\" means probability returned, instead value link function, returned type = \"link\" (default).Recall serious problem extrapolation prediction, makes less precise (variable). extrapolation, together evidences raised simple analysis like , strong arguments postponing launch.\n◼","code":"\nrequire(car)\nscatterplot(nfails.field ~ temp, reg.line = lm, smooth = FALSE, spread = FALSE,\n            boxplots = FALSE, data = challenger, subset = nfails.field > 0)\nscatterplot(nfails.field ~ temp, reg.line = lm, smooth = FALSE, spread = FALSE,\n            boxplots = FALSE, data = challenger)\nmod <- lm(nfails.field ~ temp, data = challenger)\npar(mfrow = 1:2)\nplot(mod, 1)\nplot(mod, 2)\nnasa <- glm(fail.field ~ temp, family = \"binomial\", data = challenger)\nsummary(nasa)\n#ans> \n#ans> Call:\n#ans> glm(formula = fail.field ~ temp, family = \"binomial\", data = challenger)\n#ans> \n#ans> Deviance Residuals: \n#ans>    Min      1Q  Median      3Q     Max  \n#ans> -1.057  -0.757  -0.382   0.457   2.220  \n#ans> \n#ans> Coefficients:\n#ans>             Estimate Std. Error z value Pr(>|z|)  \n#ans> (Intercept)    7.584      3.915    1.94    0.053 .\n#ans> temp          -0.417      0.194   -2.15    0.032 *\n#ans> ---\n#ans> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#ans> \n#ans> (Dispersion parameter for binomial family taken to be 1)\n#ans> \n#ans>     Null deviance: 28.267  on 22  degrees of freedom\n#ans> Residual deviance: 20.335  on 21  degrees of freedom\n#ans> AIC: 24.33\n#ans> \n#ans> Number of Fisher Scoring iterations: 5\nexp(coef(nasa)) # Exponentiated coefficients (\"odds ratios\")\n#ans> (Intercept)        temp \n#ans>    1965.974       0.659\n# Plot data\nplot(challenger$temp, challenger$fail.field, xlim = c(-1, 30), xlab = \"Temperature\",\n     ylab = \"Incident probability\")\n\n# Draw the fitted logistic curve\nx <- seq(-1, 30, l = 200)\ny <- exp(-(nasa$coefficients[1] + nasa$coefficients[2] * x))\ny <- 1 / (1 + y)\nlines(x, y, col = 2, lwd = 2)\n\n# The Challenger\npoints(-0.6, 1, pch = 16)\ntext(-0.6, 1, labels = \"Challenger\", pos = 4)\nconfint(nasa, level = 0.95)\n#ans>              2.5 % 97.5 %\n#ans> (Intercept)  1.336 17.783\n#ans> temp        -0.924 -0.109\npredict(nasa, newdata = data.frame(temp = -0.6), type = \"response\")\n#ans> 1 \n#ans> 1"},{"path":"pw-3.html","id":"pw-3","chapter":"PW 3","heading":"PW 3","text":"","code":""},{"path":"pw-3.html","id":"social-networks-ads","chapter":"PW 3","heading":"Social Networks Ads","text":"PW going analyse Social_Network_Ads dataset . dataset contains informations users social network. social network several business clients business clients put ads social network marketing compaigns purposes. dataset, company put ads one new products social network gathered informations wich users responded positively ad buying product responded negatively buying product.1. Download Social_Network_Ads dataset  import .2. Explore Describe dataset (can use str() summary() functions, can calculate visualize correlations, show histograms, scatterplots, pie charts, etc..).\nconsider variables Age \nEstimatedSalary input variables (features) see \ncorrelations decision user buy ()\nproduct.\n3. Now going split dataset training set test set. Last week manually. now split randomly, can use code (undestanding course):4. Scale input variables training set test set. know scaling? Explain one sentence.5. Now fit simple logistic regression model Purchased function Age.6. saw Logistic Regression chapter previous question, choose argument family binomial use function glm. Explain !7. Write equation model obtained question 5? (Read following note concerning question).\nNote three different ways express equation \nLogistic Regression model.\n8. feature Age significant model? Justify answer.9. value AIC model?\nAIC Akaike Information\nCriterion. use comparing multiple models.\nmodel lower value AIC better. Suppose \nstatistical model data. Let \\({\\hat {L}}\\) maximum value likelihood function \nmodel; let \\(k\\) number \nestimated parameters model. AIC value model \nfollowing.\n\n\\[ \\text{AIC} = 2 k-2\n\\ln(\\hat{L})\\]\n\n\n\n\\(\\hat{L}\\) = maximized value\nlikelihood function model \\(M\\), .e. \\(\\hat{L}=p(x|{\\hat{\\beta }},M)\\), \n\\(\\hat{\\beta}\\) parameter\nvalues maximize likelihood function.\n\n\\(x\\) = observed data.\n\n\\(k\\) = number free\nparameters estimated. model consideration linear\nregression, \\(k\\) number \nregressors, including intercept.\n10. Plot Purchased function Age add curve obtained logistic regression model.(Hints: One way first plot observations, use curve() function option add=TRUE order add curve plot. Pay attention argument “type” function predict() must reponse)must obtain something likeExtra: great \n package visualization ggplot2. Take look link  examples. library obtain following plot model,obtained last figure lines code,11. Now let us take another feature account model. Fit logistic regression model purchasing product function age user salary.12. predictors significant new model?13. obtain better model adding estimated salary?14. Predictions: test set, predict probability purchasing product users using obtained model.15. Take look predicted values variable Purchased. predicted probability user purchase product right? Now order compare results real answers, transform predicted values 0 1 (1 >0.5).Hint: can easily ifelse() function.Now read following evaluation classifier (classification model).\nConfusion matrix: tabular representation \nActual vs Predicted values. helps us find accuracy \nmodel. different results binary classifier true\npositives, true negatives, false positives, false negatives. \nconfusion matrix looks like:\n\n (Image source: Wikipedia)\n\ncan calculate accuracy model\n:\n\nAccuracy key measure performance, \nspecifically rate model able predict \ncorrect value (classification regression) given data point \nobservation. words, accuracy proportion correct\npredictions predictions made.\n\ntwo metrics confusion matrix worth discussing \nPrecision Recall.\n\nPrecision (positive predictive value) ratio true positives\ntotal amount positive predictions made (.e., true false).\nSaid another way, precision measures proportion accurate positive\npredictions positive predictions made.\n\nRecall hand, true positive rate, ratio true\npositives total amount actual positives, whether predicted\ncorrectly . words, recall measures proportion \naccurate positive predictions actual positive\nobservations.\n\nmetric associated precision recall called \nF-score (also called F1 score), combines mathematically, \nsomewhat like weighted average, order produce single measure\nperformance based simultaneous values . values\nrange 0 (worst) 1 (best).\n\nAnother important concept know Receiver\nOperating Characteristic, plotted, results \n’s known ROC curve.\n\nROC Curve: ROC curve two-dimensional plot \nsensitivity (recall, true positive rate) vs 1-\nspecificity (false positive rate). area curve \nreferred AUC, numeric\nmetric used represent quality performance classifier\n(model).\n\nAUC 0.5 essentially random guessing without \nmodel, whereas AUC 1.0 considered perfect classifier.\nGenerally, higher AUC value better, AUC 0.8 \nconsidered quite good.\n\nhigher AUC value, closer curve gets upper left\ncorner plot. One can easily see ROC curves \ngoal find tune model maximizes true positive rate,\nsimultaneously minimizing false positive rate. Said another\nway, goal shown ROC curve correctly predict many\nactual positives possible, also predicting many \nactual negatives possible, therefore minimize errors\n(incorrect classifications) .\n16. Now order evaluate model predictions, compute confusion matrix. obtain ?(Hint: can use table() function).17. Calculate accuracy, specificity, sensitivity precision model.(Note: can create function takes confusion matrix input returns needed metrics)18. Plot ROC curve calculate AUC value.\nHints: plot , install ROCR\npackage. Load use functions:\n\nprediction() calculate elements confusion\nmatrix.\n\nperformance() calculate AUC.\n\nplot() plot ROC curve, can plot \nperformance calculated .\n\nabline() plot line equation\ny=x.\n19. Compare AUC two models fitted (one age one age estimated salary) plot ROC curves figure.\n◼\n","code":"\nlibrary(caTools) # install it first in the console\nset.seed(123)\n# we use the function set.seed() with the same seed number\n# to randomly generate the same values, you already know that right? \n#and you know why we want to generate the same values, am I wrong? \nsplit = sample.split(dataset$Purchased, SplitRatio = 0.75)\n# here we chose the SplitRatio to 75% of the dataset,\n# and 25% for the test set.\ntraining_set = subset(dataset, split == TRUE)\n# we use subset to split the dataset\ntest_set = subset(dataset, split == FALSE)\nlibrary(ggplot2)\nggplot(training_set, aes(x=Age, y=Purchased)) +\n  geom_point() +\n  stat_smooth(method=\"glm\", method.args=list(family=\"binomial\"), se=FALSE)"},{"path":"discriminant-analysis.html","id":"discriminant-analysis","chapter":"4 Discriminant Analysis","heading":"4 Discriminant Analysis","text":"Discriminant analysis popular method multiple-class classiﬁcation. start first Linear Discriminant Analysis (LDA).","code":""},{"path":"discriminant-analysis.html","id":"introduction-2","chapter":"4 Discriminant Analysis","heading":"4.1 Introduction","text":"saw previous chapter, Logistic regression involves directly modeling \\(\\mathbb{P} (Y = k|X = x)\\) using logistic function, case two response classes. logistic regression, model conditional distribution response \\(Y\\), given predictor(s) \\(X\\). now consider alternative less direct approach estimating probabilities. alternative approach, model distribution predictors \\(X\\) separately response classes (.e. given \\(Y\\)), use Bayes’ theorem flip around estimates \\(\\mathbb{P} (Y = k|X = x)\\). distributions assumed Normal, turns model similar form logistic regression.logistic regression?\nneed another method, logistic regression? several reasons:classes well-separated, parameter estimates logistic regression model surprisingly unstable. Linear discriminant analysis suﬀer problem.\\(n\\) small distribution predictors \\(X\\) approximately normal classes, linear discriminant model stable logistic regression model.Linear discriminant analysis popular two response classes.","code":""},{"path":"discriminant-analysis.html","id":"bayes-theorem","chapter":"4 Discriminant Analysis","heading":"4.2 Bayes’ Theorem","text":"Bayes’ theorem stated mathematically following equation:\\[ P(| B) = \\frac{P(\\cap B)}{P(B)} =  \\frac{P(B|) P()}{P(B)}\\]\\(\\) \\(B\\) events \\(P(B) \\neq 0\\).\\(P(| B)\\), conditional probability, probability observing event \\(\\) given \\(B\\) true. called posterior probability.\\(P()\\), called prior, initial degree belief .\\(P(B)\\) likelihood.posterior probability can written memorable form :Posterior probability \\(\\propto\\) Likelihood \\(\\times\\) Prior probability.Extended form:Suppose partition \\(\\{A_i\\}\\) sample space, even space given conceptualized terms \\(P(A_j)\\) \\(P(B | A_j)\\). useful compute \\(P(B)\\) using law total probability:\\[ P(B) = \\sum_j P(B|A_j) P(A_j) \\]\\[ \\Rightarrow P(A_i|B) = \\frac{P(B|A_i) P(A_i)}{\\sum_j P(B|A_j) P(A_j)} \\]Bayes’ Theorem Classification:Suppose wish classify observation one \\(K\\) classes, \\(K \\geq 2\\). words, qualitative response variable \\(Y\\) can take \\(K\\) possible distinct unordered values.Let \\(\\pi_k\\) represent overall prior probability randomly chosen observation comes \\(k\\)-th class; probability given observation associated \\(k\\)-th category response variable \\(Y\\).Let \\(f_k(X) \\equiv P(X = x|Y = k)\\) denote density function \\(X\\) observation comes \\(k\\)-th class. words, \\(f_k(x)\\) relatively large high probability observation \\(k\\)-th class \\(X \\approx x\\), \\(f_k(x)\\) small unlikely observation \\(k\\)-th class \\(X \\approx x\\). Bayes’ theorem states \\[\\begin{equation}\nP(Y=k|X=x) = \\frac{ \\pi_k f_k(x)}{\\sum_{c=1}^K \\pi_c f_c(x)}\n\\tag{4.1}\n\\end{equation}\\]last chapter, use abbreviation \\(p_k(X) =P(Y = k|X)\\).equation stated Bayes’ theorem suggests instead directly computing \\(p_k(X)\\) logistic regression, can simply plug estimates \\(\\pi_k\\) \\(f_k(X)\\) equation. general, estimating \\(\\pi_k\\) easy (fraction training observations belong \\(k\\)-th class). estimating \\(f_k(X)\\) tends challenging.Recall \\(p_k(x)\\) posterior probability observation \\(X=x\\) belongs \\(k\\)-th class.can find way estimate \\(f_k(X)\\), can develop classifier lowest possibe error rate classifiers.","code":""},{"path":"discriminant-analysis.html","id":"lda-for-p1","chapter":"4 Discriminant Analysis","heading":"4.3 LDA for \\(p=1\\)","text":"Assume \\(p=1\\), mean one predictor. like obtain estimate \\(f_k(x)\\) can plug Equation (4.1) order estimate \\(p_k(x)\\). classify observation class \\(p_k(x)\\) greatest.order estimate \\(f_k(x)\\), first make assumptions form.Suppose assume \\(f_k(x)\\) normal (Gaussian). one-dimensional setting, normal density take form\\[\\begin{equation}\nf_k(x)= \\frac{1}{\\sigma_k\\sqrt{2\\pi}} \\exp \\big( - \\frac{1}{2\\sigma_k^2 } (x-\\mu_k)^2\\big)\n\\tag{4.2}\n\\end{equation}\\]\\(\\mu_k\\) \\(\\sigma_k^2\\) mean variance parameters \\(k\\)-th class. Let us assume \\(\\sigma_1^2 = \\ldots = \\sigma_K^2 = \\sigma^2\\) (means shared variance term across \\(K\\) classes). Plugging Eq. (4.2) Bayes formula Eq. (4.1) get,\\[\\begin{equation}\np_k(x) = \\frac{  \\pi_k \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2} \\big(\\frac{x-\\mu_k}{\\sigma}\\big)^2 } }{  \\sum_{c=1}^K  \\pi_c \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2} \\big(\\frac{x-\\mu_c}{\\sigma}\\big)^2 } }\n\\tag{4.3}\n\\end{equation}\\]Note \\(\\pi_k\\) \\(\\pi_c\\) denote prior probabilities. \\(\\pi\\) mathematical constant \\(\\pi \\approx 3.14159\\).classify value \\(X = x\\), need see \\(p_k(x)\\) largest. Taking logs, discarding terms depend \\(k\\), see equivalent assigning \\(x\\) class largest discriminant score:\\[\\begin{equation}\n\\delta_k(x) = x.\\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + \\log (\\pi_k)\n\\tag{4.4}\n\\end{equation}\\]Note \\(\\delta_k(x)\\) linear function \\(x\\).decision surfaces (e.g. decision boundaries) linear discriminant classifiers defined linear equations \\(\\delta_k(x) = \\delta_c(x)\\), classes \\(k\\neq c\\). represents set values \\(x\\) probability belonging classes \\(k\\) \\(c\\) , \\(0.5\\).Example: \\(K=2\\) \\(\\pi_1=\\pi_2\\), desicion boundary \\(x=\\frac{\\mu_1+\\mu2}{2}\\) (Prove !).example \\(\\mu_1=-1.5\\), \\(\\mu_2=1.5\\), \\(\\pi_1=\\pi_2=0.5\\) \\(\\sigma^2=1\\) shown following figureSee  video  understand decision boundary (Applied logistic regression).See  video  understand decision boundary (Applied logistic regression).classify new point according density highest, priors diﬀerent take account well, compare \\(\\pi_k f_k(x)\\). right following figure, favor pink class (remark decision boundary shifted left).classify new point according density highest, priors diﬀerent take account well, compare \\(\\pi_k f_k(x)\\). right following figure, favor pink class (remark decision boundary shifted left).","code":""},{"path":"discriminant-analysis.html","id":"estimating-the-parameters","chapter":"4 Discriminant Analysis","heading":"4.4 Estimating the parameters","text":"Typically don’t know parameters; just training data. case simply estimate parameters plug rule.Let \\(n\\) total number training observations, \\(n_k\\) number training observations \\(k\\)-th class. following estimates used:\\[\\begin{align*}\n\\hat{\\pi}_k &= \\frac{n_k}{n} \\\\\n\\hat{\\mu}_k &= \\frac{1}{n_k} \\sum_{: y_i=k} x_i \\\\\n\\hat{\\sigma}^2 &= \\frac{1}{n - K} \\sum_{k=1}^K \\sum_{: y_i=k} (x_i-\\hat{\\mu}_k)^2 \\\\\n&= \\sum_{k=1}^K \\frac{n_k-1}{n - K} . \\hat{\\sigma}_k^2\n\\end{align*}\\]\\(\\hat{\\sigma}_k^2 = \\frac{1}{n_k-1}\\sum_{: y_i=k}(x_i-\\hat{\\mu}_k)^2\\) usual formula estimated variance -\\(k\\)-th class.linear discriminant analysis (LDA) classifier plugs estimates Eq. (4.4) assigns observation \\(X=x\\) class \\[\\begin{equation}\n\\hat{\\delta}_k(x) = x.\\frac{\\hat{\\mu}_k}{\\hat{\\sigma}^2} - \\frac{\\hat{\\mu}_k^2}{2\\hat{\\sigma}^2} + \\log (\\hat{\\pi}_k)\n\\tag{4.5}\n\\end{equation}\\]largest.discriminant functions Eq. (4.5) linear functions \\(x\\).Recall assumed observations come normal distribution common variance \\(\\sigma^2\\).","code":""},{"path":"discriminant-analysis.html","id":"lda-for-p-1","chapter":"4 Discriminant Analysis","heading":"4.5 LDA for \\(p > 1\\)","text":"Let us now suppose multiple predictors. assume \\(X=(X_1,X_2,\\ldots,X_p)\\) drawn multivariate Gaussian distribution (assuming common covariance matrix, e.g. variances case \\(p=1\\)). multivariate Gaussian distribution assumes individual predictor follows one-dimensional normal distribution Eq. (4.2), correlation pair predictors.indicate \\(p\\)-dimensional random variable \\(X\\) multivariate Gaussian distribution, write \\(X \\sim \\mathcal{N}(\\mu,\\Sigma)\\). \\[ \\mu = E(X) = \\begin{pmatrix}\n    \\mu_1 \\\\\n    \\mu_2 \\\\\n    \\vdots \\\\\n    \\mu_p\n\\end{pmatrix} \\],\n\\[ \\Sigma = Cov(X) = \\begin{pmatrix}\n    \\sigma_1^2 & Cov[X_1, X_2]  & \\dots  & Cov[X_1, X_p] \\\\\n    Cov[X_2, X_1] & \\sigma_2^2  & \\dots  & Cov[X_2, X_p] \\\\\n    \\vdots & \\vdots &  \\ddots & \\vdots \\\\\n    Cov[X_p, X_1] & Cov[X_p, X_2]  & \\dots  & \\sigma_p^2\n\\end{pmatrix}  \\]\\(\\Sigma\\) \\(p\\times p\\) covariance matrix \\(X\\).Formally, multivariate Gaussian density deﬁned \\[f(x) = \\frac{1}{(2\\pi)^{p/2} |\\Sigma|^{1/2}} \\exp \\bigg( - \\frac{1}{2} (x-\\mu)^T \\Sigma^{-1} (x-\\mu) \\bigg)\n\\]Plugging density function \\(k\\)-th class, \\(f_k(X=x)\\), Eq. (4.1) reveals Bayes classifier assigns observation \\(X=x\\) class \\[\\begin{equation}\n\\delta_k(x) = x^T \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\Sigma^{-1}  \\mu_k + \\log \\pi_k\n\\tag{4.6}\n\\end{equation}\\]largest. vector/matrix version (4.4).example shown following figure. Three equally-sized Gaussian classes shown class-specific mean vectors common covariance matrix (\\(\\pi_1=\\pi_2=\\pi_3=1/3\\)). three ellipses represent regions contain \\(95\\%\\) probability three classes. dashed lines Bayes decision boundaries.Recall decision boundaries represent set values \\(x\\) \\(\\delta_k(x)=\\delta_c(x)\\); .e. \\(k\\neq c\\).\\[ x^T \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\Sigma^{-1}  \\mu_k = x^T \\Sigma^{-1} \\mu_c - \\frac{1}{2} \\mu_c^T \\Sigma^{-1}  \\mu_c  \\]Note three lines representing Bayes decision boundaries three pairs classes among three classes. , one Bayes decision boundary separates class 1 class 2, one separates class 1 class 3, one separates class 2 class 3. three Bayes decision boundaries divide predictor space three regions. Bayes classiﬁer classify observation according region located., need estimate unknown parameters \\(\\mu_1,\\ldots,\\mu_k,\\) \\(\\pi_1,\\ldots,\\pi_k,\\) \\(\\Sigma\\); formulas similar used one-dimensional case. assign new observation \\(X = x\\), LDA plugs estimates Eq. (4.6) classiﬁes class \\(\\delta_k(x)\\) largest.Note Eq. (4.6) \\(\\delta_k(x)\\) linear function \\(x\\); , LDA decision rule depends \\(x\\) linear combination elements (e.g. decision boundaries linear). reason word linear LDA.","code":""},{"path":"discriminant-analysis.html","id":"making-predictions","chapter":"4 Discriminant Analysis","heading":"4.6 Making predictions","text":"estimates \\(\\hat{\\delta}_k(x)\\), can turn estimates class probabilities:\\[ \\hat{P}(Y=k|X=x)= \\frac{e^{\\hat{\\delta}_k(x)}}{\\sum_{c=1}^K e^{\\hat{\\delta}_c(x)}} \\]classifying largest \\(\\hat{\\delta}_k(x)\\) amounts classifying class \\(\\hat{P}(Y=k|X=x)\\) largest.\\(K=2\\), classify class 2 \\(\\hat{P}(Y=2|X=x) \\geq 0.5\\), else class \\(1\\).","code":""},{"path":"discriminant-analysis.html","id":"other-forms-of-discriminant-analysis","chapter":"4 Discriminant Analysis","heading":"4.7 Other forms of Discriminant Analysis","text":"\\[P(Y=k|X=x) = \\frac{ \\pi_k f_k(x)}{\\sum_{c=1}^K \\pi_c f_c(x)}\\]saw \\(f_k(x)\\) Gaussian densities, whith covariance matrix \\(\\Sigma\\) class, leads Linear Discriminant Analysis (LDA).altering forms \\(f_k(x)\\), get different classifiers.Gaussians different \\(\\Sigma_k\\) class, get Quadratic Discriminant Analysis (QDA).\\(f_k(x) = \\prod_{j=1}^p f_{jk}(x_j)\\) (conditional independence model) class get Naive Bayes. (Gaussian, mean \\(\\Sigma_k\\) diagonal, e.g. \\(Cov(X_i,X_j)=0 \\,\\, \\forall \\, \\, 1\\leq ,j \\leq p\\)).Many forms proposing specific density models \\(f_k(x)\\), including nonparametric approaches.","code":""},{"path":"discriminant-analysis.html","id":"quadratic-discriminant-analysis-qda","chapter":"4 Discriminant Analysis","heading":"4.7.1 Quadratic Discriminant Analysis (QDA)","text":"Like LDA, QDA classiﬁer results assuming observations class drawn Gaussian distribution, plugging estimates parameters Bayes’ theorem order perform prediction.However, unlike LDA, QDA assumes class covariance matrix. assumption, Bayes classiﬁer assigns observation \\(X = x\\) class \\[\\begin{align*}\n\\delta_k(x) &= - \\frac{1}{2} (x-\\mu)^T \\Sigma_k^{-1} (x-\\mu) - \\frac{1}{2} \\log |\\Sigma_k| + \\log \\pi_k \\\\\n            &= - \\frac{1}{2} x^T \\Sigma_k^{-1} x + \\frac{1}{2} x^T \\Sigma_k^{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\Sigma_k^{-1} \\mu_k - \\frac{1}{2} \\log |\\Sigma_k| + \\log \\pi_k\n\\end{align*}\\]largest.Unlike LDA, quantity \\(x\\) appears quadratic function QDA. QDA gets name.decision boundary QDA non-linear. quadratic (curve).","code":""},{"path":"discriminant-analysis.html","id":"naive-bayes","chapter":"4 Discriminant Analysis","heading":"4.7.2 Naive Bayes","text":"use Naive Bayes classifier features independant class. useful \\(p\\) large (unklike LDA QDA).Naive Bayes assumes \\(\\Sigma_k\\) diagonal, \\[\\begin{align*}\n\\delta_k(x) &\\propto \\log \\bigg[\\pi_k \\prod_{j=1}^p f_{kj}(x_j) \\bigg] \\\\\n            &= -\\frac{1}{2} \\sum_{j=1}^p \\frac{(x_j-\\mu_{kj})^2}{\\sigma_{kj}^2} + \\log \\pi_k\n\\end{align*}\\]can used mixed feature vectors (qualitative quantitative). \\(X_j\\) qualitative, replace \\(f_{kj}(x_j)\\) probability mass function (histogram) discrete categories.","code":""},{"path":"discriminant-analysis.html","id":"lda-vs-logistic-regression","chapter":"4 Discriminant Analysis","heading":"4.8 LDA vs Logistic Regression","text":"logistic regression LDA methods closely connected. Consider two-class setting \\(p =1\\) predictor, let \\(p_1(x)\\) \\(p_2(x)=1−p_1(x)\\) probabilities observation \\(X = x\\) belongs class 1 class 2, respectively. LDA framework, can see Eq. (4.4) (bit simple algebra) log odds given \\[ \\log \\bigg(\\frac{p_1(x)}{1-p_1(x)}\\bigg) = \\log \\bigg(\\frac{p_1(x)}{p_2(x)}\\bigg) = c_0 + c_1 x\\]\\(c_0\\) \\(c_1\\) functions \\(\\mu_1, \\mu_2,\\) \\(\\sigma^2\\).hand, know logistic regression\\[ \\log \\bigg(\\frac{p_1}{1-p_1}\\bigg) = \\beta_0 + \\beta_1 x\\]equations linear functions \\(x\\). Hence logistic regression LDA produce linear decision boundaries. diﬀerence two approaches lies fact \\(\\beta_0\\) \\(\\beta_1\\) estimated using maximum likelihood, whereas \\(c_0\\) \\(c_1\\) computed using estimated mean variance normal distribution. connection LDA logistic regression also holds multidimensional data \\(p> 1\\).Logistic regression uses conditional likelihood based \\(P(Y|X)\\) (known discriminative learning).LDA uses full likelihood based \\(P(X,Y )\\) (known \ngenerative learning).Despite differences, practice results often similar.Remark: Logistic regression can also fit quadratic boundaries like QDA, explicitly including quadratic terms model.\n◼\n","code":""},{"path":"pw-4.html","id":"pw-4","chapter":"PW 4","heading":"PW 4","text":"session going continue analysis Social_Network_Ads dataset . Recall dataset contains informations users social network bought specified product. Last week built Logistic Regression model variable Purchased function Age EstimatedSalary. consider variables week fit different models using methods LDA, QDA, Naive Bayes.","code":""},{"path":"pw-4.html","id":"logistic-regression-2","chapter":"PW 4","heading":"Logistic Regression","text":"1. First, let’s pre-processing steps asked last session fit logistic regression model. Please read understand well following code (read comments!). copy necessary today’s session report (remove comments!).lost dataset, can download .now logistic regression model stored classifier.logreg. model Purchased function Age EstimatedSalary. use model show decision boundary next part PW. compare model models obtained Discriminant Analysis approaches.","code":"\n\n# Loading the dataset.. I have putted it into a folder called \"datasets\"\ndataset <- read.csv('http://www.mghassany.com/MLcourse/datasets/Social_Network_Ads.csv')\n\n# Describing and Exploring the dataset\n\nstr(dataset) # to show the structure of the dataset. \nsummary(dataset) # will show some statistics of every column. \n# Remark what it shows when the column is a numerical or categorical variable.\n# Remark that it has no sense for the variable User.ID\n\nboxplot(Age ~ Purchased, data=dataset, col = \"blue\", main=\"Boxplot Age ~ Purchased\")\n# You know what is a boxplot right? I will let you interpret it.\nboxplot(EstimatedSalary ~ Purchased, data=dataset,col = \"red\",\n main=\"Boxplot EstimatedSalary ~ Purchased\")\n# Another boxplot\n\naov(EstimatedSalary ~Purchased, data=dataset)\n# Anova test, but we need to show the summary of \n# it in order to see the p-value and to interpret.\n\nsummary(aov(EstimatedSalary ~Purchased, data=dataset))\n# What do you conclude ?\n# Now another anova test for the variable Age\nsummary(aov(Age ~Purchased, data=dataset))\n\n# There is a categorical variable in the dataset, which is Gender.\n# Of course we cannot show a boxplot of Gender and Purchased.\n# But we can show a table, or a mosaic plot, both tell the same thing.\ntable(dataset$Gender,dataset$Purchased)\n# Remark for the function table(), that\n# in lines we have the first argument, and in columns we have the second argument.\n# Don't forget this when you use table() to show a confusion matrix!\nmosaicplot(~ Purchased + Gender, data=dataset,\n  main = \"MosaicPlot of two categorical variables: Puchased & Gender\",\n  color = 2:3, las = 1)\n\n# since these 2 variables are categorical, we can apply\n# a Chi-square test. The null hypothesis is the independance between\n# these variables. You will notice that p-value = 0.4562 which is higher than 0.05 (5%)\n# so we cannot reject the null hypothesis. \n# conclusion: there is no dependance between Gender and Purchased (who\n# said that women buy more than men? hah!)\n\nchisq.test(dataset$Purchased, dataset$Gender)\n\n# Let's say we want to remove the first two columns as we are not going to use them.\n# But, we can in fact use a categorical variable as a predictor in logistic regression.\n# It will treat it the same way as in regression. Check Appendix C.\n# Try it by yourself if you would like to.\ndataset = dataset[3:5]\nstr(dataset) # show the new structure of dataset\n\n\n# splitting the dataset into training and testing sets\nlibrary(caTools)\nset.seed(123) # CHANGE THE VALUE OF SEED. PUT YOUR STUDENT'S NUMBER INSTEAD OF 123.\nsplit = sample.split(dataset$Purchased, SplitRatio = 0.75)\ntraining_set = subset(dataset, split == TRUE)\ntest_set = subset(dataset, split == FALSE)\n\n# scaling\n# So here, we have two continuous predictors, Age and EstimatedSalary.\n# There is a very big difference in their scales (units).\n# That's why we scale them. But it is not always necessary.\n\ntraining_set[-3] <- scale(training_set[-3]) #only first two columns\ntest_set[-3] <- scale(test_set[-3])\n\n# Note that, we replace the columns of Age and EstimatedSalary in the training and\n# test sets but their scaled versions. I noticed in a lot of reports that you scaled\n# but you did not do the replacing.\n# Note too that if you do it column by column you will have a problem because \n# it will replace the column by a matrix, you need to retransform it to a vector then.\n# Last note, to call the columns Age and EstimatedSalary we can it like I did or \n# training_set[c(1,2)] or training_set[,c(1,2)] or training_set[,c(\"Age\",\"EstimatedSalary\")]\n\n\n# logistic regression\n\nclassifier.logreg <- glm(Purchased ~ Age + EstimatedSalary , family = binomial, data=training_set)\nclassifier.logreg\nsummary(classifier.logreg)\n\n# prediction\npred.glm = predict(classifier.logreg, newdata = test_set[,-3], type=\"response\")\n# Do not forget to put type response. \n# By the way, you know what you get when you do not put it, right?\n\n# Now let's assign observations to classes with respect to the probabilities\npred.glm_0_1 = ifelse(pred.glm >= 0.5, 1,0)\n# I created a new vector, because we need the probabilities later for the ROC curve.\n\n# show some values of the vectors\nhead(pred.glm)\nhead(pred.glm_0_1)\n\n# confusion matrix\ncm = table(test_set[,3], pred.glm_0_1)\ncm\n# First line to store it into cm, second line to show the matrix! \n\n# You remember my note about table() function and the order of the arguments?\ncm = table(pred.glm_0_1, test_set[,3])\ncm\n\n# You can show the confusion matrix in a mosaic plot by the way\nmosaicplot(cm,col=sample(1:8,2)) # colors are random between 8 colors.\n\n# ROC\nrequire(ROCR)\nscore <- prediction(pred.glm,test_set[,3]) # we use the predicted probabilities not the 0 or 1\nperformance(score,\"auc\") # y.values\nplot(performance(score,\"tpr\",\"fpr\"),col=\"green\")\nabline(0,1,lty=8)"},{"path":"pw-4.html","id":"decision-boundary-of-logistic-regression","chapter":"PW 4","heading":"Decision Boundary of Logistic Regression","text":"Now going visualize decision boundary logistic regression.\nSince decision boundary logistic regression linear\n(know right?) dimension feature space \n2 (Age EstimatedSalary), decision\nboundary 2-dimensional space line separates \npredicted classes “0” “1” (values response\nPurchased).\n\nlogistic regression, predict \\(y=1\\) \\(\\beta^T X \\geq 0\\) (right side line)\n\\(y=0\\) \\(\\beta^T X \\lt 0\\) (left side line).\n\n\n\\[ \\beta = \\begin{pmatrix} \\beta_0 \\\\\n\\beta_1 \\\\ \\beta2 \\end{pmatrix} \\,\\, \\text{} \\,\\, X = \\begin{pmatrix}\n  1 \\\\\n  X_1 \\\\\n  X_2\n  \\end{pmatrix}\\]\n\npredict \\(y=1\\) \\(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 \\geq 0\\) means equation decision boundary (\nline ) \\(X_2 = - \\frac{\\beta_1}{\\beta_2}X_1 - \\frac{\\beta_0}{\\beta_2}\\)\n2. Plot decision boundary obtained logistic regression. order , calculate intercept slope line presenting decision boundary, plot EstimatedSalary function Age (test_set) add line using abline().3. order verify line (decision boundary) well plotted, color points last Figure respect predicted response.Hints:predictions stored y_pred, can using bg = ifelse(y_pred == 1, 'color1', 'color2'), precise argument pch 21 (can choose pch value 21 25, try )., add line using abline(), put line width = 2 make visible. forget title Figure).4. Now make plot color points respect real labels (variable Purchased). figure, count number false positive predictions compare value obtained confusion matrix.","code":""},{"path":"pw-4.html","id":"linear-discriminant-analysis-lda","chapter":"PW 4","heading":"Linear Discriminant Analysis (LDA)","text":"Let us apply linear discriminant analysis (LDA) now. First make use lda() function package MASS. Second, going create model predict classes without using lda() function. visualize decision boundary LDA.5. Fit LDA model Purchased function Age EstimatedSalary. Name model classifier.lda.6. Call classifier.lda understand compute.Plus: enter following returned list summary information concerning computation:7. test set, predict probability purchasing product users using model classifier.lda. Remark predict using LDA, obtain list instead matrix, str() predictions see get.Remark: get predicted class , without obligated round predictions logistic regression.8. Compute confusion matrix compare predictions results obtained LDA ones obtained logistic regression. remark? (Hint: compare accuracy)9. Now let us plot decision boundary obtained LDA. saw course decision boundary LDA represent set values \\(x\\) \\(\\delta_k(x) = \\delta_c(x)\\). Recall \n\\[ \\delta_k(X) = x^T \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\Sigma^{-1}  \\mu_k + \\log \\pi_k \\]case, 2 classes (\\(K=2\\)) 2 predictors (\\(p=2\\)). decision boundary (linear case LDA, line case since \\(p=2\\)) verify equation \\(\\delta_0(x) = \\delta_1(x)\\) Since two classes “0” “1”. case LDA leads linear boundary easy plotted. complicated cases difficult manually simplify equations plot decision boundary. Anyway, smart method plot (little bit costy) decision boundary R using function contour(), corresponding code following (must adapt use plot decision boundary):","code":"\nlibrary(MASS)\nclassifier.lda <- lda(Purchased~Age+EstimatedSalary, data=training_set)\nclassifier.lda$prior\nclassifier.lda$means\n# create a grid corresponding to the scales of Age and EstimatedSalary\n# and fill this grid with lot of points\nX1 = seq(min(training_set[, 1]) - 1, max(training_set[, 1]) + 1, by = 0.01)\nX2 = seq(min(training_set[, 2]) - 1, max(training_set[, 2]) + 1, by = 0.01)\ngrid_set = expand.grid(X1, X2)\n# Adapt the variable names\ncolnames(grid_set) = c('Age', 'EstimatedSalary')\n\n# plot 'Estimated Salary' ~ 'Age'\nplot(test_set[, 1:2],\n     main = 'Decision Boundary LDA',\n     xlab = 'Age', ylab = 'Estimated Salary',\n     xlim = range(X1), ylim = range(X2))\n\n# color the plotted points with their real label (class)\npoints(test_set[1:2], pch = 21, bg = ifelse(test_set[, 3] == 1, 'green4', 'red3'))\n\n# Make predictions on the points of the grid, this will take some time\npred_grid = predict(classifier.lda, newdata = grid_set)$class\n\n# Separate the predictions by a contour\ncontour(X1, X2, matrix(as.numeric(pred_grid), length(X1), length(X2)), add = TRUE)"},{"path":"pw-4.html","id":"lda-from-scratch","chapter":"PW 4","heading":"LDA from scratch","text":"10. Now let us build LDA model data set without using lda() function. free creating function without creating one. Go back question 6 see obtain using lda(). computes prior probability group membership estimated group means two groups. Additional information provided, may important, single covariance matrix used various groupings.\nLDA, compute every observation \\(x\\) discriminant score \\(\\delta_k(x)\\). attribute \\(x\\) class highest \\(\\delta\\). Recall \n\n\\[\\delta_k(x) = x^T \\Sigma^{-1} \\mu_k -\n\\frac{1}{2} \\mu_k^T \\Sigma^{-1}  \\mu_k + \\log \\pi_k\\]\n\ncompute \\(\\delta_k(x)\\) \nneed estimate \\(\\pi_k\\), \\(\\mu_k\\) \\(\\Sigma\\).\n\nNote \\[x=\\begin{pmatrix}\n            X_1 \\\\\n            X_2\n            \\end{pmatrix}\\] \\(X_1\\)=Age \\(X_2\\)=EstimatedSalary.\nlet us step step, first estimates:10.1 Subset training set two sets: class0 Purchased = 0 class1 Purchased = 1).10.2 Compute \\(\\pi_0\\) \\(\\pi_1\\).\\[\\pi_i = N_i / N, \\,\\, \\text{} \\,\\, N_i \\,\\, \\text{number data points group } \\]10.3 Compute \\(\\mu_0\\) \\(\\mu_1\\).\n\\[\\mu_0 = \\begin{pmatrix}\n   \\mu_0(X_1) \\\\\n   \\mu_0(X_2)\n   \\end{pmatrix} \\,\\, \\text{} \\,\\, \\mu_1 = \\begin{pmatrix}\n   \\mu_1(X_1) \\\\\n   \\mu_1(X_2)\n   \\end{pmatrix}\\], example, \\(\\mu_0(X_1)\\) mean variable \\(X_1\\) group \\(0\\) (subset class0).10.4 Compute \\(\\Sigma\\). case two classes like , computed calculating following:\\[\\Sigma = \\frac{(N_0-1)\\Sigma_0 + (N_1-1)\\Sigma_1}{N_0+N_1-2}\\]\\(\\Sigma_i\\) estimated covariance matrix specific group \\(\\).Remark: Recall LDA use \\(\\Sigma\\). QDA .10.5. Now computed needed estimates, can calculate \\(\\delta_0(x)\\) \\(\\delta_1(x)\\) observation \\(x\\). attribute \\(x\\) class highest \\(\\delta\\). First, try \\(x\\) \\(x^T=(1,1.5)\\), class prediction spesific \\(x\\)?10.6. Compute discriminant scores \\(\\delta\\) test set (matrix \\(100\\times 2\\)), predict classes compare results results obtained lda() function.","code":""},{"path":"pw-4.html","id":"quadratic-discriminant-analysis-qda-1","chapter":"PW 4","heading":"Quadratic Discriminant Analysis (QDA)","text":"Training assessing QDA model R similar syntax training assessing LDA model. difference function name qda()11. Fit QDA model Purchased function Age EstimatedSalary. Name model classifier.qda.12. Make predictions test_set using QDA model classifier.qda. Show confusion matrix compare results predictions obtained using LDA model classifier.lda.13. Plot decision boundary obtained QDA. Color points real labels.","code":"\n# qda() is a function of library(MASS)\nclassifier.qda <- qda(Purchased~., data = training_set)"},{"path":"pw-4.html","id":"comparison","chapter":"PW 4","heading":"Comparison","text":"14. order compare methods used, plot Figure ROC curve classifier fitted compare correspondant AUC. best model dataset? Can justify ?Remark:\nuse ROCR package:Logistic regression, use predicted probabilities prediction() (round values “0” “1”).LDA QDA, put pred.lda$posterior[,2] prediction() function (posterior probabilities observations belong class “1”).\n◼\n","code":""},{"path":"decision-trees-random-forests.html","id":"decision-trees-random-forests","chapter":"5 Decision Trees & Random Forests","heading":"5 Decision Trees & Random Forests","text":"chapter, describe tree-based methods regression classification. Tree-based methods simple useful interpretation. However, typically competitive best supervised learning approaches terms prediction accuracy. Hence chapter also introduce bagging, random forests,\nboosting. approaches involves producing multiple trees\ncombined yield single consensus prediction. \nsee combining large number trees can often result dramatic\nimprovements prediction accuracy, expense loss interpretation.invited watch following videos14. can download slides15 used videos clicking .","code":""},{"path":"decision-trees-random-forests.html","id":"the-basics-of-decision-trees","chapter":"5 Decision Trees & Random Forests","heading":"The Basics of Decision Trees","text":"","code":""},{"path":"decision-trees-random-forests.html","id":"classification-trees","chapter":"5 Decision Trees & Random Forests","heading":"Classification Trees","text":"","code":""},{"path":"decision-trees-random-forests.html","id":"bagging-random-forests","chapter":"5 Decision Trees & Random Forests","heading":"Bagging & Random Forests","text":"","code":""},{"path":"decision-trees-random-forests.html","id":"boosting","chapter":"5 Decision Trees & Random Forests","heading":"Boosting","text":"","code":""},{"path":"decision-trees-random-forests.html","id":"trees-in-r","chapter":"5 Decision Trees & Random Forests","heading":"Trees in R","text":"","code":""},{"path":"decision-trees-random-forests.html","id":"random-forests---the-first-choice-method-for-every-data-analysis","chapter":"5 Decision Trees & Random Forests","heading":"Random forests - the first-choice method for every data analysis?","text":"Now know Random Forests method (RF) works. often read claims, like Random Forests “works well without tuning”, “need scale recode predictors”, “works well high dimensional data”, “overfit”, etc..section, find super talk slides discussing common claims Random Forests whether true RF first-choice method every data analysis.can download slides16 clicking .\n◼\n","code":""},{"path":"pw-5.html","id":"pw-5","chapter":"PW 5","heading":"PW 5","text":"practical work, build decision trees regression classification problems. Note many packages  . tree package basic package , rpart17 package seems widely suggested provides better plotting features. use rpart package.recommended correct better using  functions consult documentations. Every  function well documented indeed. can writing ?function_name help(function_name)console.Especially functions multiple use, example, glm() function fits generalizes linear models, one logistic regression type = \"binomial\".Another example, function predict(), generic function predictions results various model fitting functions. first argument  object, model, rest arguments depends nature object. want consult documentation using predict() tree built rpart(), ?predict.rpart help(predict.rpart)want run function certain package without loading package, can write package::function(). example MASS::lda() rpart::rpart(). also helpful remember name package function defined.","code":""},{"path":"pw-5.html","id":"regression-trees","chapter":"PW 5","heading":"Regression Trees","text":"","code":""},{"path":"pw-5.html","id":"single-tree","chapter":"PW 5","heading":"Single tree","text":"demonstrate regression trees, use Boston dataset used first two practical works, MASS package. Recall medv response.1. Load Boston dataset MASS package. Split dataset randomly half.2. Fit regression tree training data using rpart() function rpart package. Name tree Boston_tree.3. Plot obtained tree using following code.4. better plot can obtained using rpart.plot18 package. Re-plot tree using . can use rpart.plot() function default, output continuous, node shows: predicted value, percentage observations node. can also use prp() function.5. Print obtained tree print summary. things can see summary, CP (complexity parameter) table importance variable model. Print CP table using printcp() function see cross validation results. Plot comparison figure using plotcp() function.notice obtained tree pruned. rpart prunes tree default performing 10-fold cross-validation.rpart keeps track something called complexity tree. complexity measure combination size tree ability tree separate classes target variable. next best split growing tree reduce tree’s overall complexity certain amount, rpart terminate growing process. amount specified complexity parameter, cp, call rpart(). Setting cp negative amount (like -1) ensures tree fully grown. can try plot tree.Notice default cp value may prune tree (default one lowest xerror). rule thumb, ’s best prune decision tree using cp smallest tree within one standard deviation tree smallest xerror. example (see CP table figure obtained plotcp()), best xerror 0.30517 standard deviation 0.056180. , want smallest tree xerror less 0.30517 + 0.05618 = 0.361. tree cp = 0.025293, ’ll want prune tree cp slightly greater 0.025293.Next compare regression tree linear model use RMSE metric. RMSE Root Mean Square Error, square root MSE.6. Write  function returns RMSE two vectors.7. Use function predict() predict response test set. calculate RMSE obtained tree model.8. Fit linear regression model training set. predict response test set using linear model. Calculate RMSE compare performance tree linear regression model.obvious linear regression beats tree! ’ll improve tree considering ensembles trees.can visually compare performance models plotting Actual (reality) response values predicted values. model closer points diagonal (y=x) line better one. can try reproduce figure .aggregating many decision trees, using methods like bagging,\nrandom forests, boosting, predictive performance trees can \nsubstantially improved. now use concepts, called ensemble methods.","code":"\nlibrary(MASS)\nlibrary(caTools)\nset.seed(18)\nBoston_idx = sample(1:nrow(Boston), nrow(Boston) / 2) \n# You don't know what we just did?\n# open the documentation of the function sample by \n# writing ?sample in the R console.\n# Note that this is one of the ways to split it randomly and it is not necessary the best.\nBoston_train = Boston[Boston_idx,]\nBoston_test  = Boston[-Boston_idx,]\nplot(Boston_tree)\ntext(Boston_tree, pretty = 0)\ntitle(main = \"Regression Tree\")"},{"path":"pw-5.html","id":"bagging","chapter":"PW 5","heading":"Bagging","text":"Bagging, Bootstrap aggregation, general-purpose procedure reducing variance statistical learning method, particularly useful frequently used context decision trees. idea take many training sets population, build separate prediction model using training set, average resulting predictions. Generally access multiple training sets. Instead, can bootstrap, taking repeated samples (single) training data set.apply bagging regression\ntrees, simply construct \\(B\\) regression trees using B bootstrapped training\nsets, average resulting predictions. trees grown deep,\npruned. Hence individual tree high variance, \nlow bias. Averaging \\(B\\) trees reduces variance.9. Fit bagged model, using randomForest() function randomForest package.Bagging actually special case random forest mtry equal \\(p\\), number predictors.10. Predict response test set using bagging model. Calculate RMSE. performance model better linear regression simple tree?Note “Mean squared residuals” output randomForest() Bag estimate error. plot:","code":""},{"path":"pw-5.html","id":"random-forests","chapter":"PW 5","heading":"Random Forests","text":"Now try random forest. regression, suggestion use mtry equal \\(p/3\\).1911. Fit random forest training set compare performance previous models calculating predictions RMSE.12. Use function importance() randomForest package see important predictors obtained random forest model. three important predictors? find results selected best predictors linear regression model session 2?13. Plot importance predictors model using varImpPlot() function.","code":""},{"path":"pw-5.html","id":"boosting-1","chapter":"PW 5","heading":"Boosting","text":"Last least, let us try boosted model, default produce nice variable importance plot well plots marginal effects predictors. , use gbm package20.14. Using gbm() function like following, fit boosted model training set. compare performance previous models calculating predictions RMSE.15. Show summary boosted model. figure variable importance shown.","code":"\nlibrary(gbm)\nBoston_boost = gbm(medv ~ ., data = Boston_train, distribution = \"gaussian\", \n                    n.trees = 5000, interaction.depth = 4, shrinkage = 0.01)#ans> Using 5000 trees..."},{"path":"pw-5.html","id":"comparison-1","chapter":"PW 5","heading":"Comparison","text":"16. Reproduce following comparison: table show obtained RMSE tested model, can create \\(5 \\times 2\\) data.frame put names models corresponding RMSE. visualize data frame compiled html report can use kable() function knitr package. , compare models plotting Actual (reality) response values predicted values.","code":""},{"path":"pw-5.html","id":"classification-trees-1","chapter":"PW 5","heading":"Classification Trees","text":"classification tree similar regression tree, except classification tree used predict qualitative response rather quantitative one. Recall regression tree, predicted response observation given mean response training observations belong \nterminal node. contrast, classification tree, predict \nobservation belongs commonly occurring class training\nobservations region belongs.construct classification trees, use spam21 dataset, available . description dataset given .rest PW, must:Import spam dataset explore . aware preferable response column type factor.Split dataset training test sets (choose seed using set.seed()).Fit (using rpart gbm packages):\nlogistic regression model.\nsimple classification tree.\nBagging, Random Forests22, Boosting models.\nlogistic regression model.simple classification tree.Bagging, Random Forests22, Boosting models.model, predict response test set evaluate performance model, using prediction accuracy (create function returns accuracy two binary vectors).","code":""},{"path":"pw-5.html","id":"the-spam-dataset","chapter":"PW 5","heading":"The Spam dataset","text":"dataset consists information 4601 email messages, study try predict whether email junk email, \n“spam”. 4601 email messages, true outcome, spam , available,\nalong 57 predictors described :48 quantitative predictors: percentage words email match given word. Examples include business, address, internet; etc.6 quantitative predictors: percentage characters email match given character. characters ; , ( , [ , ! , $ #.average length uninterrupted sequences capital letters: crl.ave.length longest uninterrupted sequence capital letters: crl.long.sum length uninterrupted sequences capital letters: crl.tot.Note spam dataset given already treated ready explored. achieve stage, steps required treat raw data, like Tokenization, Stemming, Lemmatization. dataset important words already selected variables added.\nCurious students can read steps. Two famous  packages text mining tm tidytext.","code":""},{"path":"pw-5.html","id":"extra-tuning","chapter":"PW 5","heading":"Extra: Tuning","text":"far PW, fit bagging, boosting random forest models, tune , simply used certain, somewhat arbitrary, parameters. Actually, make models better parameters tuned. parameters include:Bagging: Actually just subset Random Forest mtry = \\(p\\).Random Forest: mtryBoosting: n.trees, interaction.depth, shrinkage, n.minobsinnodeThe caret package provides excellent functions accomplish . Note tree-based ensemble methods two resampling solutions tuning model:BagCross-ValidationUsing Bag samples advantageous methods compared Cross-Validation since removes need refit model thus much computationally efficient. Unfortunately OOB methods used gbm models. See caret documentation: Short intro, Long intro details.\n◼\n","code":""},{"path":"principal-components-analysis.html","id":"principal-components-analysis","chapter":"6 Principal Components Analysis","heading":"6 Principal Components Analysis","text":"","code":""},{"path":"principal-components-analysis.html","id":"introduction-3","chapter":"6 Principal Components Analysis","heading":"6.1 Introduction","text":"central idea principal component analysis (PCA) reduce dimensionality data set consisting large number interrelated variables, retaining much possible variation present data set. achieved transforming new set variables, principal components (PCs), uncorrelated, ordered first retain variation present original variables.Suppose wish visualize \\(n\\) observations measurements set \\(p\\) features, \\(X_1,X_2,\\ldots,X_p\\), part exploratory data analysis. examining two-dimensional scatterplots data, contains \\(n\\) observations’ measurements two features. However, \\(C_p^2 = p(p−1)/2\\) scatterplots. example, \\(p =10\\) 45 plots! \\(p\\) large, certainly possible look ; moreover, likely none informative since contain just small fraction total information present data set. Clearly, better method required visualize \\(n\\) observations \\(p\\) large. particular, like find low-dimensional representation data captures much information possible. PCA provides tool just .PCA finds low-dimensional representation data set contains much possible variation. idea \\(n\\) observations lives \\(p\\)-dimensional space, dimensions equally interesting. PCA seeks small number dimensions interesting possible, concept interesting measured amount observations vary along dimension. dimensions found PCA linear combination \\(p\\) features. now explain manner dimensions, principal components, found.","code":""},{"path":"principal-components-analysis.html","id":"principal-components","chapter":"6 Principal Components Analysis","heading":"6.2 Principal Components","text":"","code":""},{"path":"principal-components-analysis.html","id":"notations-and-procedure","chapter":"6 Principal Components Analysis","heading":"Notations and Procedure","text":"Suppose random vector features \\(X\\).\\[ \\textbf{X} = \\left(\\begin{array}{c} X_1\\\\ X_2\\\\ \\vdots \\\\X_p\\end{array}\\right) \\]population variance-covariance matrix\\[ \\text{var}(\\textbf{X}) = \\Sigma = \\left(\\begin{array}{cccc}\\sigma^2_1 & \\sigma_{12} & \\dots &\\sigma_{1p}\\\\ \\sigma_{21} & \\sigma^2_2 & \\dots &\\sigma_{2p}\\\\  \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\sigma_{p1} & \\sigma_{p2} & \\dots & \\sigma^2_p\\end{array}\\right) \\]Consider linear combinations\\[ \\begin{array}{lll} Y_1 & = & a_{11}X_1 + a_{12}X_2 + \\dots + a_{1p}X_p \\\\ Y_2 & = & a_{21}X_1 + a_{22}X_2 + \\dots + a_{2p}X_p \\\\ & & \\vdots \\\\ Y_p & = & a_{p1}X_1 + a_{p2}X_2 + \\dots +a_{pp}X_p \\end{array} \\]Note \\(Y_i\\) function random data, also random. Therefore population variance\\[ \\text{var}(Y_i) = \\sum_{k=1}^{p} \\sum_{l=1}^{p} a_{ik} a_{il} \\sigma_{kl} = \\mathbf{}^T_i \\Sigma \\mathbf{}_i \\]Moreover, \\(Y_i\\) \\(Y_j\\) population covariance\\[ \\text{cov}(Y_i, Y_j) = \\sum_{k=1}^{p} \\sum_{l=1}^{p} a_{ik}a_{jl}\\sigma_{kl} = \\mathbf{}^T_i\\Sigma\\mathbf{}_j \\]correlation\\[ \\text{cor}(Y_i, Y_j) = \\frac{\\text{cov}(Y_i, Y_j)}{\\sigma^2_i \\sigma^2_j}\\]coefficients \\(a_{ij}\\) collected vector\\[ \\mathbf{}_i = \\left(\\begin{array}{c} a_{i1}\\\\ a_{i2}\\\\ \\vdots \\\\ a_{ip}\\end{array}\\right) \\]coefficients \\(a_{ij}\\) also called loadings principal component \\(\\) \\(\\mathbf{}_i\\) principal component loading vector.\ntotal variation \\(X\\) \ntrace variance-covariance matrix \\(\\Sigma\\).\n\ntrace \\(\\Sigma\\) sum\nvariances individual variables.\n\n\\(trace(\\Sigma) \\, = \\, \\sigma^2_1 + \\sigma^2_2 + \\dots +\\sigma^2_p\\)\n","code":""},{"path":"principal-components-analysis.html","id":"first-principal-component-textpc_1-y_1","chapter":"6 Principal Components Analysis","heading":"First Principal Component (\\(\\text{PC}_1\\)): \\(Y_1\\)","text":"first principal component normalized linear combination features \\(X_1,X_2,\\ldots,X_p\\) maximum variance (among linear combinations), accounts much variation data possible.Specifically define coefficients \\(a_{11},a_{12},\\ldots,a_{1p}\\) component way variance maximized, subject constraint sum squared coefficients equal one (mean normalized). constraint required unique answer may obtained.formally, select \\(a_{11},a_{12},\\ldots,a_{1p}\\) maximizes\\[ \\text{var}(Y_1) = \\mathbf{}^T_1\\Sigma\\mathbf{}_1  = \\sum_{k=1}^{p}\\sum_{l=1}^{p}a_{1k}a_{1l}\\sigma_{kl} \\]subject constraint \\[ \\sum_{j=1}^{p}^2_{1j} = \\mathbf{}^T_1\\mathbf{}_1   = 1 \\]","code":""},{"path":"principal-components-analysis.html","id":"second-principal-component-textpc_2-y_2","chapter":"6 Principal Components Analysis","heading":"Second Principal Component (\\(\\text{PC}_2\\)): \\(Y_2\\)","text":"second principal component linear combination features \\(X_1,X_2,\\ldots,X_p\\) accounts much remaining variation possible, constraint correlation first second component 0. second principal component maximal variance linear combinations uncorrelated \\(Y_1\\).compute coefficients second principal component, select \\(a_{21},a_{22},\\ldots,a_{2p}\\) maximizes variance new component\\[\\text{var}(Y_2) = \\sum_{k=1}^{p}\\sum_{l=1}^{p}a_{2k}a_{2l}\\sigma_{kl} = \\mathbf{}^T_2\\Sigma\\mathbf{}_2 \\]subject :constraint sums squared coefficients add one, \\(\\sum_{j=1}^{p}^2_{2j} = \\mathbf{}^T_2\\mathbf{}_2 = 1\\).constraint sums squared coefficients add one, \\(\\sum_{j=1}^{p}^2_{2j} = \\mathbf{}^T_2\\mathbf{}_2 = 1\\).Along additional constraint two components uncorrelated one another:Along additional constraint two components uncorrelated one another:\\[ \\text{cov}(Y_1, Y_2) = \\mathbf{}^T_1\\Sigma\\mathbf{}_2  = \\sum_{k=1}^{p}\\sum_{l=1}^{p}a_{1k}a_{2l}\\sigma_{kl} = 0 \\]subsequent principal components property: linear combinations account much remaining variation possible correlated principal components.way additional component. instance:","code":""},{"path":"principal-components-analysis.html","id":"ith-principal-component-textpc_i-y_i","chapter":"6 Principal Components Analysis","heading":"\\(i^{th}\\) Principal Component (\\(\\text{PC}_i\\)): \\(Y_i\\)","text":"select \\(a_{i1},a_{i2},\\ldots,a_{ip}\\) maximizes\\[ \\text{var}(Y_i) = \\sum_{k=1}^{p}\\sum_{l=1}^{p}a_{ik}a_{il}\\sigma_{kl} = \\mathbf{}^T_i\\Sigma\\mathbf{}_i \\]subject constraint sums squared coefficients add one, along additional constraint new component uncorrelated previously defined components:\\[ \\sum_{j=1}^{p}^2_{ij} \\mathbf{}^T_i\\mathbf{}_i = \\mathbf{}^T_i\\mathbf{}_i = 1\\]\\[ \\text{cov}(Y_1, Y_i) = \\sum_{k=1}^{p}\\sum_{l=1}^{p}a_{1k}a_{il}\\sigma_{kl} = \\mathbf{}^T_1\\Sigma\\mathbf{}_i = 0 \\]\\[\\text{cov}(Y_2, Y_i) = \\sum_{k=1}^{p}\\sum_{l=1}^{p}a_{2k}a_{il}\\sigma_{kl} = \\mathbf{}^T_2\\Sigma\\mathbf{}_i = 0\\]\n\\[\\vdots\\]\n\\[\\text{cov}(Y_{-1}, Y_i) = \\sum_{k=1}^{p}\\sum_{l=1}^{p}a_{-1,k}a_{il}\\sigma_{kl} = \\mathbf{}^T_{-1}\\Sigma\\mathbf{}_i = 0\\]Therefore principal components uncorrelated one another.","code":""},{"path":"principal-components-analysis.html","id":"how-do-we-find-the-coefficients","chapter":"6 Principal Components Analysis","heading":"6.3 How do we find the coefficients?","text":"find coefficients \\(a_{ij}\\) principal component? solution involves eigenvalues eigenvectors variance-covariance matrix \\(\\Sigma\\).Let \\(\\lambda_1,\\ldots,\\lambda_p\\) denote eigenvalues variance-covariance matrix \\(\\Sigma\\). ordered \\(\\lambda_1\\) largest eigenvalue \\(\\lambda_p\\) smallest.\\[ \\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_p \\]also going let vectors \\(\\mathbf{}_1, \\ldots,\\mathbf{}_p\\) denote corresponding eigenvectors.turns elements eigenvectors coefficients principal components.\nelements eigenvectors \\(\\Sigma\\) coefficients \nprincipal components.\nvariance \\(\\)th principal component equal \\(\\)th eigenvalue.\\[ \\textbf{var}(Y_i) = \\text{var}(a_{i1}X_1 + a_{i2}X_2 + \\dots a_{ip}X_p) = \\lambda_i \\]Moreover, principal components uncorrelated one another.\\[\\text{cov}(Y_i, Y_j) = 0\\]variance-covariance matrix may written function eigenvalues corresponding eigenvectors. fact, variance-covariance matrix can written sum \\(p\\) eigenvalues, multiplied product corresponding eigenvector times transpose shown following expression\\[ \\Sigma  =  \\sum_{=1}^{p}\\lambda_i \\mathbf{}_i \\mathbf{}_i^T \\]\\(\\lambda_{k+1}, \\lambda_{k+2}, \\dots , \\lambda_{p}\\) small, might approximate \\(\\Sigma\\) \\[ \\Sigma  \\cong  \\sum_{=1}^{k}\\lambda_i \\mathbf{}_i\\mathbf{}_i^T \\]Earlier chapter defined total variation \\(X\\) trace variance-covariance matrix. also equal sum eigenvalues shown :\\[ \\begin{array}{lll}trace(\\Sigma) & = & \\sigma^2_1 + \\sigma^2_2 + \\dots +\\sigma^2_p \\\\ & = & \\lambda_1 + \\lambda_2 + \\dots + \\lambda_p\\end{array} \\]give us interpretation components terms amount full variation explained component. proportion variation explained \\(\\)th principal component going defined eigenvalue component divided sum eigenvalues. words, \\(\\)th principal component explains following proportion total variation:\\[ \\frac{\\lambda_i}{\\lambda_1 + \\lambda_2 + \\dots + \\lambda_p} \\]related quantity proportion variation explained first \\(k\\) principal component. sum first \\(k\\) eigenvalues divided total variation.\\[ \\frac{\\lambda_1 + \\lambda_2 + \\dots + \\lambda_k}{\\lambda_1 + \\lambda_2 + \\dots + \\lambda_p} \\]practice, proportions often expressed percentages.Naturally, proportion variation explained first \\(k\\) principal components large, much information lost considering first \\(k\\) principal components.","code":""},{"path":"principal-components-analysis.html","id":"why-it-may-be-possible-to-reduce-dimensions","chapter":"6 Principal Components Analysis","heading":"Why It May Be Possible to Reduce Dimensions","text":"correlations (multicollinarity) features, data may less fall line plane lower number dimensions. instance, imagine plot two features nearly perfect correlation. data points fall close straight line. line used new (one-dimensional) axis represent variation among data points.\ndefined terms population variance-covariance\nmatrix \\(\\Sigma\\) \nunknown. However, may estimate \\(\\Sigma\\) sample variance-covariance\nmatrix given standard formula :\n\n\\[ \\textbf{S} = \\frac{1}{n-1}\n\\sum_{=1}^{n}(\\mathbf{X}_i-\\bar{\\textbf{x}})(\\mathbf{X}_i-\\bar{\\textbf{x}})^T\n\\]\n","code":""},{"path":"principal-components-analysis.html","id":"procedure","chapter":"6 Principal Components Analysis","heading":"Procedure","text":"Compute eigenvalues \\(\\hat{\\lambda}_1, \\hat{\\lambda}_2, \\dots, \\hat{\\lambda}_p\\) sample variance-covariance matrix \\(\\textbf{S}\\), corresponding eigenvectors \\(\\hat{\\mathbf{}}_1, \\hat{\\mathbf{}}_2, \\dots, \\hat{\\mathbf{}}_p\\).define estimated principal components using eigenvectors coefficients:\\[ \\begin{array}{lll} \\hat{Y}_1 & = & \\hat{}_{11}X_1 + \\hat{}_{12}X_2 + \\dots + \\hat{}_{1p}X_p \\\\ \\hat{Y}_2 & = & \\hat{}_{21}X_1 + \\hat{}_{22}X_2 + \\dots + \\hat{}_{2p}X_p \\\\&&\\vdots\\\\ \\hat{Y}_p & = & \\hat{}_{p1}X_1 + \\hat{}_{p2}X_2 + \\dots + \\hat{}_{pp}X_p \\\\ \\end{array} \\]Generally, retain first \\(k\\) principal component. number criteria may used decide many components retained:obtain simplest possible interpretation, want \\(k\\) small possible. can explain variation just two principal components give us much simpler description data.obtain simplest possible interpretation, want \\(k\\) small possible. can explain variation just two principal components give us much simpler description data.Retain first \\(k\\) components explain “large” proportion total variation, say \\(70-80\\%\\).Retain first \\(k\\) components explain “large” proportion total variation, say \\(70-80\\%\\).Examine scree plot. plot eigenvalues versus component number. idea look “elbow” corresponds point eigenvalues decrease slowly. Adding components point explains relatively little variance. See next figure example scree plot.Examine scree plot. plot eigenvalues versus component number. idea look “elbow” corresponds point eigenvalues decrease slowly. Adding components point explains relatively little variance. See next figure example scree plot.\nScree plot showing eigenvalue number principal component.\n","code":""},{"path":"principal-components-analysis.html","id":"standardization-of-the-features","chapter":"6 Principal Components Analysis","heading":"6.4 Standardization of the features","text":"use raw data, principal component analysis tend give emphasis variables higher variances variables low variances.effect results analysis depend units measurement used measure variable. imply principal component analysis used raw data variables units measure. even case, wish give variables higher variances weight analysis.\nresults principal component analysis depend scales \nvariables measured.\n\nVariables highest sample variances tend \nemphasized first principal components.\n\nPrincipal component analysis using covariance function \nconsidered variables units \nmeasurement.\nvariables either different units measurement, wish variable receive equal weight analysis, variables standardized (scaled) principal components analysis carried . Standardize variables subtracting mean variable dividing standard deviation:\\[Z_{ij} = \\frac{X_{ij}-\\bar{x}_j}{\\sigma_j}\\]\\(X_{ij}\\) = Data variable \\(j\\) sample unit \\(\\)\\(\\bar{x}_j\\) = Sample mean variable \\(j\\)\\(\\sigma_j\\) = Sample standard deviation variable \\(j\\)Note: \\(Z_j\\) mean = 0 variance = 1.\nvariance-covariance matrix standardized data equal \ncorrelation matrix unstandardized data. Therefore, principal\ncomponent analysis using standardized data equivalent \nprincipal component analysis using correlation matrix.\nRemark: going prove practical work\nsession.\n","code":""},{"path":"principal-components-analysis.html","id":"projection-of-the-data","chapter":"6 Principal Components Analysis","heading":"6.5 Projection of the data","text":"","code":""},{"path":"principal-components-analysis.html","id":"scores","chapter":"6 Principal Components Analysis","heading":"Scores","text":"Using coefficients (loadings) every principal component, can project observations axis principal component, projections called scores. example, scores first principal component \\[  \\forall 1 \\le \\le n \\quad \\hat{Y}_1^ =  \\hat{}_{11}X_1^+ \\hat{}_{12}X_2^+ \\dots + \\hat{}_{1p}X_p^\\](\\(X_1^\\) value feature \\(1\\) observation \\(\\))can written observations principal components using matrix formulation\\[ \\mathbf{\\hat{Y}} = \\mathbf{\\hat{}} X\\]\\(\\mathbf{\\hat{}}\\) matrix coefficients \\(\\hat{}_{ij}\\).","code":""},{"path":"principal-components-analysis.html","id":"visualization","chapter":"6 Principal Components Analysis","heading":"Visualization","text":"computed principal components, can plot order produce low-dimensional views data.can plot score vector \\(Y_1\\) \\(Y_2\\), \\(Y_1\\) \\(Y_3\\), \\(Y_2\\) \\(Y_3\\), forth. Geometrically, amounts projecting original data onto subspace spanned \\(\\mathbf{}_1\\), \\(\\mathbf{}_2\\), \\(\\mathbf{}_3\\), plotting projected points.interpret results obtained PCA, plot figure principal component scores \nloading vectors. figure called biplot. example given later chapter.","code":""},{"path":"principal-components-analysis.html","id":"extra","chapter":"6 Principal Components Analysis","heading":"Extra","text":"can read tutorial . document, introduction mathemtical concepts used PCA. Plus detailed example PCA.can read tutorial . document, introduction mathemtical concepts used PCA. Plus detailed example PCA.can watch videos nice explanation PCA  1  2.can watch videos nice explanation PCA  1  2.","code":""},{"path":"principal-components-analysis.html","id":"case-study","chapter":"6 Principal Components Analysis","heading":"6.6 Case study","text":"","code":""},{"path":"principal-components-analysis.html","id":"employement-in-european-countries-in-the-late-70s","chapter":"6 Principal Components Analysis","heading":"Employement in European countries in the late 70s","text":"purpose case study reveal structure job market economy different developed countries. final aim meaningful rigorous plot able show important features countries concise form.dataset eurojob  contains data employed case study. contains percentage workforce employed 1979 9 industries 26 European countries. industries measured :Agriculture (Agr)Mining (Min)Manufacturing (Man)Power supply industries (Pow)Construction (Con)Service industries (Ser)Finance (Fin)Social personal services (Soc)Transport communications (Tra)dataset imported  case names set Country (important order numerical variables), data look like :\nTable 6.1: eurojob dataset.\nNote: set case names Country, doSo far, know compute summaries variable, quantify visualize relations variables correlation matrix scatterplot matrix. even moderate number variables like , results hard process.Takes data variables \\(X_1,\\ldots,X_p\\).Using data, looks new variables \\(\\text{PC}_1,\\ldots \\text{PC}_p\\) :\n\\(\\text{PC}_j\\) linear combination \\(X_1,\\ldots,X_p\\), \\(1\\leq j\\leq p\\). , \\(\\text{PC}_j=a_{j1}X_1+a_{j2}X_2+\\ldots+a_{jp}X_p\\).\n\\(\\text{PC}_1,\\ldots \\text{PC}_p\\) sorted decreasingly terms variance. Hence \\(\\text{PC}_j\\) variance \\(\\text{PC}_{j+1}\\), \\(1\\leq j\\leq p-1\\),\n\\(\\text{PC}_{j_1}\\) \\(\\text{PC}_{j_2}\\) uncorrelated, \\(j_1\\neq j_2\\).\n\\(\\text{PC}_1,\\ldots \\text{PC}_p\\) information, measured terms total variance, \\(X_1,\\ldots,X_p\\).\n\\(\\text{PC}_j\\) linear combination \\(X_1,\\ldots,X_p\\), \\(1\\leq j\\leq p\\). , \\(\\text{PC}_j=a_{j1}X_1+a_{j2}X_2+\\ldots+a_{jp}X_p\\).\\(\\text{PC}_1,\\ldots \\text{PC}_p\\) sorted decreasingly terms variance. Hence \\(\\text{PC}_j\\) variance \\(\\text{PC}_{j+1}\\), \\(1\\leq j\\leq p-1\\),\\(\\text{PC}_{j_1}\\) \\(\\text{PC}_{j_2}\\) uncorrelated, \\(j_1\\neq j_2\\).\\(\\text{PC}_1,\\ldots \\text{PC}_p\\) information, measured terms total variance, \\(X_1,\\ldots,X_p\\).Produces three key objects:\nVariances PCs. sorted decreasingly give idea PCs contain information data (ones variance).\nWeights variables PCs. give interpretation PCs terms original variables, coefficients linear combination. weights variables \\(X_1,\\ldots,X_p\\) \\(PC_j\\), \\(a_{j1},\\ldots,a_{jp}\\), normalized: \\(a_{1j}^2+\\ldots+a_{pj}^2=1\\), \\(j=1,\\ldots,p\\). , called loadings.\nScores data PCs: data \\(\\text{PC}_1,\\ldots \\text{PC}_p\\) variables instead \\(X_1,\\ldots,X_p\\). scores uncorrelated. Useful knowing PCs effect certain observation.\nVariances PCs. sorted decreasingly give idea PCs contain information data (ones variance).Weights variables PCs. give interpretation PCs terms original variables, coefficients linear combination. weights variables \\(X_1,\\ldots,X_p\\) \\(PC_j\\), \\(a_{j1},\\ldots,a_{jp}\\), normalized: \\(a_{1j}^2+\\ldots+a_{pj}^2=1\\), \\(j=1,\\ldots,p\\). , called loadings.Scores data PCs: data \\(\\text{PC}_1,\\ldots \\text{PC}_p\\) variables instead \\(X_1,\\ldots,X_p\\). scores uncorrelated. Useful knowing PCs effect certain observation.Hence, PCA rearranges variables information-equivalent, convenient, layout variables sorted according ammount information able explain. position, next step clear: stick limited number PCs explain information (e.g., 70% total variance) dimension reduction. effectiveness PCA practice varies structure present dataset. example, case highly dependent data, explain 90% variability dataset tens variables just two PCs.Let’s see compute full PCA .\nPCA produces uncorrelated variables \noriginal set \\(X_1,\\ldots,X_p\\). \nimplies :\n\nPCs uncorrelated, independent\n(uncorrelated imply independent).\n\nuncorrelated independent variable \\(X_1,\\ldots,X_p\\) get PC \nassociated . extreme case \\(X_1,\\ldots,X_p\\) uncorrelated, \ncoincide PCs (sign flips).\nBased weights variables PCs shown , can extract following interpretation:PC1 roughly linear combination Agr, negative weight, (Man, Pow, Con, Ser, Soc, Tra), positive weights. can interpreted indicator kind economy country: agricultural (negative values) industrial (positive values).PC2 negative weights (Min, Man, Pow, Tra) positive weights (Ser, Fin, Soc). can interpreted contrast relatively large small service sectors. tends negative communist countries positive capitalist countries.\ninterpretation PCs involves inspecting weights \ninterpreting linear combination original variables, \nmight separating two clear characteristics data\nconclude, let’s see can represent original data plot called biplot summarizes analysis two PCs.biplot confirms interpretation.Recall biplot superposition two figures: first one scatterplot principal component scores (projections observations new low dimensional space - also know graph individuals) second one figure loading vectors (also known graph variables). fact, graph variables correlation circle, variables whose unit vectors close said positively correlated, meaning influence positioning individuals similar (, proximities reflected projections variables graph individuals). However, variables far away defined negatively correlated. Variables perpendicular unit vector uncorrelated. see graphs can use factoextra package like follows:\n◼\n","code":"\nrow.names(eurojob) <- eurojob$Country\neurojob$Country <- NULL\n# Summary of the data - marginal\nsummary(eurojob)\n#ans>       Agr            Min             Man            Pow             Con       \n#ans>  Min.   : 2.7   Min.   :0.100   Min.   : 7.9   Min.   :0.100   Min.   : 2.80  \n#ans>  1st Qu.: 7.7   1st Qu.:0.525   1st Qu.:23.0   1st Qu.:0.600   1st Qu.: 7.53  \n#ans>  Median :14.4   Median :0.950   Median :27.6   Median :0.850   Median : 8.35  \n#ans>  Mean   :19.1   Mean   :1.254   Mean   :27.0   Mean   :0.908   Mean   : 8.17  \n#ans>  3rd Qu.:23.7   3rd Qu.:1.800   3rd Qu.:30.2   3rd Qu.:1.175   3rd Qu.: 8.97  \n#ans>  Max.   :66.8   Max.   :3.100   Max.   :41.2   Max.   :1.900   Max.   :11.50  \n#ans>       Ser             Fin             Soc            Tra      \n#ans>  Min.   : 5.20   Min.   : 0.50   Min.   : 5.3   Min.   :3.20  \n#ans>  1st Qu.: 9.25   1st Qu.: 1.23   1st Qu.:16.2   1st Qu.:5.70  \n#ans>  Median :14.40   Median : 4.65   Median :19.6   Median :6.70  \n#ans>  Mean   :12.96   Mean   : 4.00   Mean   :20.0   Mean   :6.55  \n#ans>  3rd Qu.:16.88   3rd Qu.: 5.92   3rd Qu.:24.1   3rd Qu.:7.08  \n#ans>  Max.   :19.10   Max.   :11.30   Max.   :32.4   Max.   :9.40\n\n# Correlation matrix\ncor(eurojob)\n#ans>         Agr     Min    Man     Pow     Con    Ser     Fin    Soc    Tra\n#ans> Agr  1.0000  0.0358 -0.671 -0.4001 -0.5383 -0.737 -0.2198 -0.747 -0.565\n#ans> Min  0.0358  1.0000  0.445  0.4055 -0.0256 -0.397 -0.4427 -0.281  0.157\n#ans> Man -0.6711  0.4452  1.000  0.3853  0.4945  0.204 -0.1558  0.154  0.351\n#ans> Pow -0.4001  0.4055  0.385  1.0000  0.0599  0.202  0.1099  0.132  0.375\n#ans> Con -0.5383 -0.0256  0.494  0.0599  1.0000  0.356  0.0163  0.158  0.388\n#ans> Ser -0.7370 -0.3966  0.204  0.2019  0.3560  1.000  0.3656  0.572  0.188\n#ans> Fin -0.2198 -0.4427 -0.156  0.1099  0.0163  0.366  1.0000  0.108 -0.246\n#ans> Soc -0.7468 -0.2810  0.154  0.1324  0.1582  0.572  0.1076  1.000  0.568\n#ans> Tra -0.5649  0.1566  0.351  0.3752  0.3877  0.188 -0.2459  0.568  1.000\n\n# Scatterplot matrix\nrequire(car)\nscatterplotMatrix(eurojob, regLine = list(method=lm, lty=1, lwd=2, col=\"green\"), smooth = FALSE,\n                  ellipse = FALSE, plot.points = T, col=\"black\",\n                  diagonal = TRUE)\n# The main function - use cor = TRUE to avoid scale distortions\npca <- princomp(eurojob, cor = TRUE)\n\n# What is inside?\nstr(pca)\n#ans> List of 7\n#ans>  $ sdev    : Named num [1:9] 1.867 1.46 1.048 0.997 0.737 ...\n#ans>   ..- attr(*, \"names\")= chr [1:9] \"Comp.1\" \"Comp.2\" \"Comp.3\" \"Comp.4\" ...\n#ans>  $ loadings: 'loadings' num [1:9, 1:9] 0.52379 0.00132 -0.3475 -0.25572 -0.32518 ...\n#ans>   ..- attr(*, \"dimnames\")=List of 2\n#ans>   .. ..$ : chr [1:9] \"Agr\" \"Min\" \"Man\" \"Pow\" ...\n#ans>   .. ..$ : chr [1:9] \"Comp.1\" \"Comp.2\" \"Comp.3\" \"Comp.4\" ...\n#ans>  $ center  : Named num [1:9] 19.131 1.254 27.008 0.908 8.165 ...\n#ans>   ..- attr(*, \"names\")= chr [1:9] \"Agr\" \"Min\" \"Man\" \"Pow\" ...\n#ans>  $ scale   : Named num [1:9] 15.245 0.951 6.872 0.369 1.614 ...\n#ans>   ..- attr(*, \"names\")= chr [1:9] \"Agr\" \"Min\" \"Man\" \"Pow\" ...\n#ans>  $ n.obs   : int 26\n#ans>  $ scores  : num [1:26, 1:9] -1.71 -0.953 -0.755 -0.853 0.104 ...\n#ans>   ..- attr(*, \"dimnames\")=List of 2\n#ans>   .. ..$ : chr [1:26] \"Belgium\" \"Denmark\" \"France\" \"WGerm\" ...\n#ans>   .. ..$ : chr [1:9] \"Comp.1\" \"Comp.2\" \"Comp.3\" \"Comp.4\" ...\n#ans>  $ call    : language princomp(x = eurojob, cor = TRUE)\n#ans>  - attr(*, \"class\")= chr \"princomp\"\n\n# The standard deviation of each PC\npca$sdev\n#ans>  Comp.1  Comp.2  Comp.3  Comp.4  Comp.5  Comp.6  Comp.7  Comp.8  Comp.9 \n#ans> 1.86739 1.45951 1.04831 0.99724 0.73703 0.61922 0.47514 0.36985 0.00675\n\n# Weights: the expression of the original variables in the PCs\n# E.g. Agr = -0.524 * PC1 + 0.213 * PC5 - 0.152 * PC6 + 0.806 * PC9\n# And also: PC1 = -0.524 * Agr + 0.347 * Man + 0256 * Pow + 0.325 * Con + ...\n# (Because the matrix is orthogonal, so the transpose is the inverse)\npca$loadings\n#ans> \n#ans> Loadings:\n#ans>     Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 Comp.9\n#ans> Agr  0.524                       0.213  0.153                0.806\n#ans> Min         0.618 -0.201        -0.164 -0.101 -0.726              \n#ans> Man -0.347  0.355 -0.150 -0.346 -0.385 -0.288  0.479  0.126  0.366\n#ans> Pow -0.256  0.261 -0.561  0.393  0.295  0.357  0.256 -0.341       \n#ans> Con -0.325         0.153 -0.668  0.472  0.130 -0.221 -0.356       \n#ans> Ser -0.379 -0.350 -0.115        -0.284  0.615 -0.229  0.388  0.238\n#ans> Fin        -0.454 -0.587         0.280 -0.526 -0.187  0.174  0.145\n#ans> Soc -0.387 -0.222  0.312  0.412 -0.220 -0.263 -0.191 -0.506  0.351\n#ans> Tra -0.367  0.203  0.375  0.314  0.513 -0.124         0.545       \n#ans> \n#ans>                Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 Comp.9\n#ans> SS loadings     1.000  1.000  1.000  1.000  1.000  1.000  1.000  1.000  1.000\n#ans> Proportion Var  0.111  0.111  0.111  0.111  0.111  0.111  0.111  0.111  0.111\n#ans> Cumulative Var  0.111  0.222  0.333  0.444  0.556  0.667  0.778  0.889  1.000\n\n# Scores of the data on the PCs: how is the data reexpressed into PCs\nhead(pca$scores, 10)\n#ans>         Comp.1  Comp.2  Comp.3  Comp.4  Comp.5  Comp.6  Comp.7 Comp.8    Comp.9\n#ans> Belgium -1.710 -1.2218 -0.1148  0.3395 -0.3245 -0.0473 -0.3401  0.403 -0.001090\n#ans> Denmark -0.953 -2.1278  0.9507  0.5939  0.1027 -0.8273 -0.3029 -0.352  0.015619\n#ans> France  -0.755 -1.1212 -0.4980 -0.5003 -0.2997  0.1158 -0.1855 -0.266 -0.000507\n#ans> WGerm   -0.853 -0.0114 -0.5795 -0.1105 -1.1652 -0.6181  0.4446  0.194 -0.006539\n#ans> Ireland  0.104 -0.4140 -0.3840  0.9267  0.0152  1.4242 -0.0370 -0.334  0.010879\n#ans> Italy   -0.375 -0.7695  1.0606 -1.4772 -0.6452  1.0021 -0.1418 -0.130  0.005602\n#ans> Luxem   -1.059  0.7558 -0.6515 -0.8352 -0.8659  0.2188 -1.6942  0.547  0.003453\n#ans> Nether  -1.688 -2.0048  0.0637 -0.0235  0.6352  0.2120 -0.3034 -0.591 -0.010931\n#ans> UK      -1.630 -0.3731 -1.1409  1.2669 -0.8129 -0.0361  0.0413 -0.349 -0.005478\n#ans> Austria -1.176  0.1431 -1.0434 -0.1577  0.5210  0.8019  0.4150  0.215 -0.002816\n\n# Scatterplot matrix of the scores - See how they are uncorrelated!\nscatterplotMatrix(pca$scores, regLine = list(method=lm, lty=1, lwd=2, col=\"green\"), smooth = FALSE,\n                  ellipse = FALSE, plot.points = T, col=\"black\",\n                  diagonal = TRUE)\n\n# Means of the variables - before PCA the variables are centered\npca$center\n#ans>    Agr    Min    Man    Pow    Con    Ser    Fin    Soc    Tra \n#ans> 19.131  1.254 27.008  0.908  8.165 12.958  4.000 20.023  6.546\n\n# Rescalation done to each variable\n# - if cor = FALSE (default), a vector of ones\n# - if cor = TRUE, a vector with the standard deviations of the variables\npca$scale\n#ans>    Agr    Min    Man    Pow    Con    Ser    Fin    Soc    Tra \n#ans> 15.245  0.951  6.872  0.369  1.614  4.486  2.752  6.697  1.364\n\n# Summary of the importance of components - the third row is key\nsummary(pca)\n#ans> Importance of components:\n#ans>                        Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8\n#ans> Standard deviation      1.867  1.460  1.048  0.997 0.7370 0.6192 0.4751 0.3699\n#ans> Proportion of Variance  0.387  0.237  0.122  0.110 0.0604 0.0426 0.0251 0.0152\n#ans> Cumulative Proportion   0.387  0.624  0.746  0.857 0.9171 0.9597 0.9848 1.0000\n#ans>                          Comp.9\n#ans> Standard deviation     6.75e-03\n#ans> Proportion of Variance 5.07e-06\n#ans> Cumulative Proportion  1.00e+00\n\n# Scree plot - the variance of each component\nplot(pca)\n\n# With connected lines - useful for looking for the \"elbow\"\nplot(pca, type = \"l\")\n\n# PC1 and PC2 -- These are the weights of the variables on the first two Principal Components\npca$loadings[, 1:2]\n#ans>       Comp.1  Comp.2\n#ans> Agr  0.52379  0.0536\n#ans> Min  0.00132  0.6178\n#ans> Man -0.34750  0.3551\n#ans> Pow -0.25572  0.2611\n#ans> Con -0.32518  0.0513\n#ans> Ser -0.37892 -0.3502\n#ans> Fin -0.07437 -0.4537\n#ans> Soc -0.38741 -0.2215\n#ans> Tra -0.36682  0.2026\n# Biplot - plot together the scores for PC1 and PC2 and the\n# variables expressed in terms of PC1 and PC2\nbiplot(pca)\n# loading factoextra library\nrequire(factoextra)\n# Calculating pca using prcomp(), which is a built-in function in R \nres.pca <- prcomp(eurojob, scale = TRUE)\n\n# Graph of individuals\nfviz_pca_ind(res.pca,\n             col.ind = \"contrib\", # Color by their contribution to axes\n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE     \n             )\n# Graph of variables\nfviz_pca_var(res.pca,\n             col.var = \"contrib\", \n             gradient.cols = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n             repel = TRUE     \n             )\n# Biplot of individuals and variables\nfviz_pca_biplot(res.pca, repel = TRUE,\n                col.var = \"#2E9FDF\", \n                col.ind = \"#696969\"  \n                )"},{"path":"pw-6.html","id":"pw-6","chapter":"PW 6","heading":"PW 6","text":"","code":""},{"path":"pw-6.html","id":"the-iris-dataset","chapter":"PW 6","heading":"The Iris Dataset","text":"Iris flower dataset Fisher’s Iris dataset multivariate data set introduced British statistician biologist Ronald Fisher 1936 paper (FISHER 1936).data set consists 50 samples three species Iris. Four features measured sample.three species Iris dataset :Iris-setosa (\\(n_1=50\\))Iris-versicolor (\\(n_2=50\\))Iris-virginica (\\(n_3=50\\))four features Iris dataset :sepal length cmsepal width cmpetal length cmpetal width cm","code":""},{"path":"pw-6.html","id":"loading-data-1","chapter":"PW 6","heading":"Loading Data","text":"1. Download iris dataset   import .","code":""},{"path":"pw-6.html","id":"exploratory-analysis","chapter":"PW 6","heading":"Exploratory analysis","text":"2. Compare means quartiles 3 different flower classes 4 different features (Plot 4 boxplots figure).Hint: can use par(mfrow=c(2,2)) show multiple plots figure (\\(2 \\times 2\\) plots ). Like follows:3. explore 3 different flower classes distributed along 4 different features, visualize via histograms using following code.","code":"\n# Let's use the ggplot2 library\n# ggplot2 is the most advanced package for data visualization\n# gg corresponds to The Grammar of Graphics.\nlibrary(ggplot2) #of course you must install it first if you don't have it already\n\n# histogram of sepal_length\nggplot(iris, aes(x=sepal_length, fill=class)) +\n  geom_histogram(binwidth=.2, alpha=.5)\n# histogram of sepal_width\nggplot(iris, aes(x=sepal_width, fill=class)) +\n  geom_histogram(binwidth=.2, alpha=.5)\n# histogram of petal_length\nggplot(iris, aes(x=petal_length, fill=class)) +\n  geom_histogram(binwidth=.2, alpha=.5)\n# histogram of petal_width\nggplot(iris, aes(x=petal_width, fill=class)) +\n  geom_histogram(binwidth=.2, alpha=.5)"},{"path":"pw-6.html","id":"pca-using-princomp","chapter":"PW 6","heading":"PCA using princomp()","text":"princomp() prcomp() built- functions. perform PCA.4. Apply PCA Iris dataset using princomp() function interpret results.","code":"\npcairis=princomp(iris[,-5], cor=T) \n# Note that we take only the numerical columns to apply PCA.\n# now pcairis is a R object of type princomp\n\n# To display the internal structure of pcairis\nstr(pcairis)\n#ans> List of 7\n#ans>  $ sdev    : Named num [1:4] 1.706 0.96 0.384 0.144\n#ans>   ..- attr(*, \"names\")= chr [1:4] \"Comp.1\" \"Comp.2\" \"Comp.3\" \"Comp.4\"\n#ans>  $ loadings: 'loadings' num [1:4, 1:4] 0.522 -0.263 0.581 0.566 0.372 ...\n#ans>   ..- attr(*, \"dimnames\")=List of 2\n#ans>   .. ..$ : chr [1:4] \"sepal_length\" \"sepal_width\" \"petal_length\" \"petal_width\"\n#ans>   .. ..$ : chr [1:4] \"Comp.1\" \"Comp.2\" \"Comp.3\" \"Comp.4\"\n#ans>  $ center  : Named num [1:4] 5.84 3.05 3.76 1.2\n#ans>   ..- attr(*, \"names\")= chr [1:4] \"sepal_length\" \"sepal_width\" \"petal_length\" \"petal_width\"\n#ans>  $ scale   : Named num [1:4] 0.825 0.432 1.759 0.761\n#ans>   ..- attr(*, \"names\")= chr [1:4] \"sepal_length\" \"sepal_width\" \"petal_length\" \"petal_width\"\n#ans>  $ n.obs   : int 150\n#ans>  $ scores  : num [1:150, 1:4] -2.26 -2.09 -2.37 -2.3 -2.39 ...\n#ans>   ..- attr(*, \"dimnames\")=List of 2\n#ans>   .. ..$ : NULL\n#ans>   .. ..$ : chr [1:4] \"Comp.1\" \"Comp.2\" \"Comp.3\" \"Comp.4\"\n#ans>  $ call    : language princomp(x = iris[, -5], cor = T)\n#ans>  - attr(*, \"class\")= chr \"princomp\"\n\n# To see the variance explained by the the pcs\nsummary(pcairis) \n#ans> Importance of components:\n#ans>                        Comp.1 Comp.2 Comp.3  Comp.4\n#ans> Standard deviation      1.706  0.960 0.3839 0.14355\n#ans> Proportion of Variance  0.728  0.230 0.0368 0.00515\n#ans> Cumulative Proportion   0.728  0.958 0.9948 1.00000\n\n# To plot the variance explained by each pc\nplot(pcairis) \n\n# To plot together the scores for PC1 and PC2 and the \n# variables expressed in terms of PC1 and PC2.\nbiplot(pcairis) "},{"path":"pw-6.html","id":"deeper-pca-using-factoextra-package","chapter":"PW 6","heading":"Deeper PCA using factoextra package","text":"help interpretation visualization PCA going use package named factoextra.matter function package decide use computing principal component methods, factoextra  package can help extract easily, human readable data format, analysis results different functions mentioned . factoextra provides also convenient solutions create ggplot2 based beautiful graphs.can take look link  detailed example.5. Using factoextra package plot following:scree plot.graph individuals.graph variables.biplot graph.contributions variables first 2 principal components.","code":""},{"path":"pw-6.html","id":"step-by-step-pca","chapter":"PW 6","heading":"Step-by-step PCA","text":"order understand PCA works, let’s implement step--step.\nSummary PCA Approach:\n\nStandardize data.\n\nObtain Eigenvectors Eigenvalues covariance matrix\ncorrelation matrix.\n\nSort eigenvalues descending order choose \\(k\\) eigenvectors correspond \n\\(k\\) largest eigenvalues, \\(k\\) number dimensions new\nfeature subspace (\\(k \\le p\\)).\n\nConstruct projection matrix \\(\\mathbf{}\\) selected \\(k\\) eigenvectors.\n\nTransform original dataset \\(X\\) via \\(\\mathbf{}\\) obtain \\(k\\)-dimensional feature subspace \\(\\mathbf{Y}\\).\n6. First step, split iris dataset data \\(X\\) class labels \\(y\\).\niris dataset now stored form \\(150 \\times 4\\) matrix columns \ndifferent features, every row represents separate flower\nsample. sample row \\(X^\\) can \npictured 4-dimensional vector\n\n\\[ (X^)^T = \\begin{pmatrix} X_1^\\\\\nX_2^\\\\ X_3^\\\\ X_4^\\end{pmatrix}\n= \\begin{pmatrix} \\text{sepal length} \\\\ \\text{sepal width}\n\\\\\\text{petal length} \\\\ \\text{petal width} \\end{pmatrix}\\]\n\nEigendecomposition - Computing Eigenvectors \nEigenvalues\n\neigenvectors eigenvalues covariance (correlation)\nmatrix represent “core” PCA: eigenvectors (principal\ncomponents) determine directions new feature space, \neigenvalues determine magnitude. words, eigenvalues\nexplain variance data along new feature axes.\nStandardizing7. Scale 4 features. Store scaled matrix new one (example, name X_scaled).Covariance Matrix8. classic approach PCA perform eigendecomposition covariance matrix \\(\\Sigma\\), \\(p\\times p\\) matrix element represents covariance two features. Compute Covariance Matrix scaled features (Print results).\ncan summarize calculation covariance matrix via \nfollowing matrix equation: \\[ \\Sigma =\n\\frac{1}{n-1} \\left( (\\mathbf{X} - \\mathbf{\\bar{X}})^T\\;(\\mathbf{X} -\n\\mathbf{\\bar{X}}) \\right) \\] \\(\\mathbf{\\bar{X}}\\) mean vector \\(\\mathbf{\\bar{X}} = \\frac{1}{n} \\sum\\limits_{k=1}^n x_{k}\\).\n\nmean vector \\(p\\)-dimensional vector value \nvector represents sample mean feature column \ndataset.\n9. Perform eigendecomposition covariance matrix. Compute Eigenvectors Eigenvalues (can use eigen() function). obtain?Correlation Matrix\nEspecially, field “Finance”, correlation matrix\ntypically used instead covariance matrix. However, \neigendecomposition covariance matrix (input data \nstandardized) yields results eigendecomposition \ncorrelation matrix, since correlation matrix can understood \nnormalized covariance matrix.\n10. Perform eigendecomposition standardized data based correlation matrix.11. Perform eigendecomposition raw data based correlation matrix. Compare obtained results previous question.\nsee three approaches yield eigenvectors\neigenvalue pairs:\n\nEigendecomposition covariance matrix standardizing \ndata.\n\nEigendecomposition correlation matrix.\n\nEigendecomposition correlation matrix standardizing \ndata.\nSelecting Principal Components\neigen() function , default, sort \neigenvalues decreasing order.\nExplained Variance12. Calculate individual explained variation cumulative explained variation principal component. Show results.13. Plot individual explained variation. (scree plot)Projection Matrix14. Construct projection matrix used transform Iris data onto new feature subspace.\n“projection matrix” basically just matrix \nconcatenated top \\(k\\) eigenvectors.\n, projection matrix \\(\\mathbf{}\\) \\(4 \\times 2\\)-dimensional matrix.\nProjection Onto New Feature SpaceIn last step use \\(4 \\times 2\\)-dimensional projection matrix \\(\\mathbf{}\\) transform samples (observations) onto new subspace via equation \\(\\mathbf{Y}=X \\times \\mathbf{}\\) \\(\\mathbf{Y}\\) \\(150 \\times 2\\) matrix transformed samples.15. Compute \\(\\mathbf{Y}\\) (Recall \\(\\mathbf{Y}\\) matrix scores, \\(\\mathbf{}\\) matrix loadings).Visualization16. Plot observations new feature space. Name axis PC1 PC2.17. plot, color observations (flowers) respect flower classes.\n◼\n","code":"\nX <- iris[,-5]\ny <- iris[,5]"},{"path":"kmeans-hierarchical-clustering.html","id":"kmeans-hierarchical-clustering","chapter":"7 Kmeans & Hierarchical Clustering","heading":"7 Kmeans & Hierarchical Clustering","text":"","code":""},{"path":"kmeans-hierarchical-clustering.html","id":"unsupervised-learning-1","chapter":"7 Kmeans & Hierarchical Clustering","heading":"7.1 Unsupervised Learning","text":"Previously considered supervised learning methods regression classification, typically access set \\(p\\) features \\(X_1,X_2,\\ldots,X_p\\), measured \\(n\\) observations, response \\(Y\\) also measured \\(n\\) observations (call labels). goal predict \\(Y\\) using \\(X_1,X_2,\\ldots,X_p\\). now instead focus unsupervised learning, set statistical tools set features \\(X_1,X_2,\\ldots,X_p\\) measured \\(n\\) observations. interested prediction, associated response variable \\(Y\\). Rather, goal discover interesting things measurements \\(X_1,X_2,\\ldots,X_p\\). informative way visualize data? Can discover subgroups among variables among observations? Unsupervised learning refers diverse set techniques answering questions . chapter, focus particular type unsupervised learning: Principal Components Analysis (PCA), tool used data visualization data pre-processing supervised techniques applied. next chapters, talk clustering, another particular type unsupervised learning. Clustering broad class methods discovering unknown subgroups data.Unsupervised learning often much challenging supervised learning. exercise tends subjective, simple goal analysis, prediction response. Unsupervised learning often performed part exploratory data analysis. hard assess results obtained unsupervised learning methods. fit predictive model using supervised learning technique, possible check work seeing well model predicts response \\(Y\\) observations used fitting model. unsupervised learning, way check work don’t know true answer: problem unsupervised.","code":""},{"path":"kmeans-hierarchical-clustering.html","id":"clustering","chapter":"7 Kmeans & Hierarchical Clustering","heading":"7.2 Clustering","text":"Clustering (Cluster analysis) collection techniques designed find subgroups clusters dataset variables \\(X_1,\\ldots,X_p\\). Depending similarities observations, partitioned homogeneous groups separated possible . Clustering methods can classified main categories:Partition methods: Given fixed number cluster \\(k\\), methods aim assign observation \\(X_1,\\ldots,X_p\\) unique cluster, way within-cluster variation small possible (clusters homogeneous possible) cluster variation large possible (clusters separated possible).Distribution models: clustering models based notion probable data points cluster belong distribution (example: Normal, Poisson, etc..). popular example models Expectation-maximization algorithm using multivariate Normal distributions.Hierarchical methods: methods construct hierarchy observations terms similitudes. results tree-based representation data terms dendrogram, depicts observations clustered different levels – smallest groups one element largest representing whole dataset.Density Models: models search data space areas varied density data points data space. isolates various different density regions assign data points within regions cluster. Popular examples density models DBSCAN OPTICS.\nFigure 7.1: Performance comparison different clustering methods different datasets\nchapter see basics partition methods, one well-known clustering techniques, namely \\(k\\)-means clustering.","code":""},{"path":"kmeans-hierarchical-clustering.html","id":"introduction-4","chapter":"7 Kmeans & Hierarchical Clustering","heading":"7.3 Introduction","text":"Clustering (Cluster analysis) process partitioning \nset data objects (observations) subsets. subset \ncluster, objects cluster similar one another,\nyet dissimilar objects clusters.set clusters resulting cluster analysis\ncan referred clustering. \ncontext, different clustering methods may generate different clusterings data set. partitioning performed humans, clustering algorithm. Hence, clustering useful can lead discovery previously unknown groups within data.\nDifferent clustering methods may generate different clusterings \ndata set.\nExample: Imagine Director Customer Relationships Electronics magazine, five managers working . like organize company’s customers five groups group can assigned different manager. Strategically, like customers group similar possible. Moreover, two given customers different business patterns placed group. intention behind business strategy develop customer relationship campaigns specifically target group, based common features shared customers per group. Unlike classification, class label customer unknown. needs discover groupings. Given large number customers many attributes describing customer profiles, can costly even infeasible human study data manually come way partition customers strategic groups. needs clustering tool help.Clustering widely used many applications business\nintelligence, image pattern recognition, Web search, biology, \nsecurity. business intelligence, clustering can used organize \nlarge number customers groups, customers within group\nshare strong similar characteristics. image recognition, clustering\ncan used discover clusters “subclasses” handwritten\ncharacter recognition systems, example.\nClustering also found many applications Web search. example,\nkeyword search may often return large number hits (.e.,\npages relevant search) due extremely large number web\npages. Clustering can used organize search results groups\npresent results concise easily accessible way.\nMoreover, clustering techniques developed cluster documents\ntopics (remember google news example?), commonly used information retrieval practice.Clustering also called data segmentation applications\nclustering partitions large data sets groups according \nsimilarity.branch statistics, clustering extensively studied, \nmain focus distance-based cluster analysis. Clustering tools\nproposed like \\(k\\)-means, fuzzy \\(c\\)-means, several \nmethods.Many clustering algorithms introduced literature. Since\nclusters can formally seen subsets data set, one possible\nclassification clustering methods can according whether \nsubsets fuzzy crisp (hard).","code":""},{"path":"kmeans-hierarchical-clustering.html","id":"hard-clustering","chapter":"7 Kmeans & Hierarchical Clustering","heading":"Hard clustering","text":"Hard clustering methods based classical set theory, \nrequire object either belong cluster. Hard\nclustering means partitioning data specified number \nmutually exclusive subsets. common hard clustering method \n\\(k\\)-means.","code":""},{"path":"kmeans-hierarchical-clustering.html","id":"fuzzy-clustering","chapter":"7 Kmeans & Hierarchical Clustering","heading":"Fuzzy clustering","text":"Fuzzy clustering methods, however, allow objects belong \nseveral clusters simultaneously, different degrees membership.\nmany situations, fuzzy clustering natural hard\nclustering. known technique fuzzy clustering fuzzy\n\\(c\\)-means.","code":""},{"path":"kmeans-hierarchical-clustering.html","id":"k-means","chapter":"7 Kmeans & Hierarchical Clustering","heading":"7.4 \\(k\\)-Means","text":"ever watched group tourists couple tour\nguides hold umbrellas everybody can see follow\n, seen dynamic version \\(k\\)-means algorithm.\n\\(k\\)-means even simpler, data (playing part \ntourists) move, tour guides move.Suppose want divide input data \\(K\\) categories, \nknow value \\(K\\). allocate \\(K\\) cluster centres (also called\nprototypes centroids) input space, like position \ncentres one cluster centre middle \ncluster. However, don’t know clusters , let alone \n‘middle’ , need algorithm find . Learning\nalgorithms generally try minimize sort error, need \nthink error criterion describes aim. two\nthings need define:distance measure: order talk distances points, need way measure distances. often normal Euclidean distance, alternatives like Manhattan distance, Correlation distance, Chessboard distance .Euclidean distance: Let \\(x=(x_1,x_2)\\) \\(y=(y_1,y_2)\\) two observations two-dimensional space. Euclidean distance \\(d_{x,y}\\) \\(x\\) \\(y\\) \\[\\begin{align*}  \nd_{x,y}^2 &= (x_1-y_1)^2+(x_2 - y_2)^2  \\\\\nd_{x,y} &= \\sqrt{(x_1-y_1)^2+(x_2 - y_2)^2}\n\\end{align*}\\]mean average: distance measure, can compute central point set data points, mean average. Actually, true Euclidean space, one used , everything nice flat.can now think suitable way positioning cluster\ncentres: compute mean point cluster, \\(\\textbf{v}_k\\),\n\\(=1,\\ldots,K\\), put cluster centre . equivalent \nminimizing Euclidean distance (sum--squares error)\ndata point cluster centre. decide points\nbelong clusters associating point cluster\ncentre closest . changes algorithm iterates. \nstart positioning cluster centres randomly though input\nspace, since don’t know put , update \npositions according data. decide cluster data point\nbelongs computing distance data point \ncluster centres, assigning cluster \nclosest. point assigned cluster, \ncompute mean , move cluster centre place. \niterate algorithm cluster centres stop moving.convenient point define notation describe \nassignment data points clusters. data point \\(x_i\\), \nintroduce corresponding set binary indicator variables\n\\(u_{ki} \\{0,1}\\), \\(k=1,\\ldots,K\\) describing \\(K\\) clusters data point \\(x_i\\) \nassigned , data point \\(x_i\\) assigned cluster \\(k\\) \n\\(u_{ki}= 1\\), \\(u_{ji}= 0\\) \\(j \\neq k\\). known \n\\(1\\)--\\(K\\) coding scheme. can define objective function (\nsometimes called distortion measure), given \\[J= \\sum_{=1}^{N} \\sum_{k=1}^{K} u_{ki} \\| x_{}- \\mathbf{v}_{k} \\|^2\\]represents sum squares distances data\npoint assigned vector \\(\\mathbf{v}_{k}\\). goal find\nvalues \\(\\{u_{ki}\\}\\) \\(\\{\\mathbf{v}_{k}\\}\\) \nminimize \\(J\\). can iterative procedure \niteration involves two successive steps corresponding successive\noptimizations respect \\(u_{ki}\\) \\(\\mathbf{v}_{k}\\). \nalgorithm \\(k\\)-means described following algorithm:Choose value \\(K\\).Choose \\(K\\) random positions input space.Assign prototypes \\(\\mathbf{v}_{k}\\) positionsfor data point \\(x_i\\) docompute distance prototype:\n\\[d_{ki}= \\text{min}_k d(x_i,\\mathbf{v}_k)\\]assign data point nearest prototype distance\n\\[u_{ki}= \\left\\lbrace \\begin{array}{ll}  1   & \\mbox{} \\quad k = argmin_j d(x_i,\\mathbf{v}_j) \\\\  0 & \\mbox{otherwise} \\end{array} \\right.\\]prototype domove position prototype mean points cluster:\n\\[\\mathbf{v}_k = \\frac{\\sum_i u_{ki} x_i}{\\sum_i u_{ki}}\\]prototypes stop moving.\n\\(k\\)-means algorithm\nproduces\n\nfinal estimate cluster centroids (.e. coordinates).\n\nassignment point respective cluster.\ndenominator expression \\(\\mathbf{v}_k = \\frac{\\sum_i u_{ki} x_i}{\\sum_i u_{ki}}\\) equal number \npoints assigned cluster \\(k\\), result simple\ninterpretation, namely set \\(\\mathbf{v}_k\\) equal mean \ndata points \\(x_i\\) assigned cluster \\(k\\). reason, \nprocedure known \\(k\\)-means algorithm.two phases re-assigning data points clusters re-computing\ncluster means repeated turn change\nassignments (maximum number iterations \nexceeded). phase reduces value objective\nfunction \\(J\\), convergence algorithm assured. However, may\nconverge local rather global minimum \\(J\\).\\(k\\)-means algorithm illustrated using Old Faithful data set 23\nfollowing figure.\nFigure 7.2: Illustration \\(k\\)-means algorithm using re-scaled Old Faithful data set, \\(k=2\\). can see \\(k\\)-means algorithm works. () first thing \\(k\\)-means assign initial set centroids. (b) next stage algorithm assigns every point dataset closest centroid. (c) next stage re-calculate centroids based new cluster assignments data points. (d) Now completed one full cycle algorithm can continue re-assign points (new) closest cluster centroid. (e) can update centroid positions one time based re-assigned points. (g)(h)(f) algorithm stops obtain results consecutive iterations.\n\\(k\\)-means algorithm illustrated using Iris data set following interactive figure24. (Try modify X Y variables numbers chosen clusters see result)","code":""},{"path":"kmeans-hierarchical-clustering.html","id":"k-means-in","chapter":"7 Kmeans & Hierarchical Clustering","heading":"7.4.1 \\(k\\)-means in ","text":"use example simulated data demonstrate \\(k\\)-means algorithm works. simulate data three clusters plot dataset .\nFigure 7.3: Simulated dataset\nkmeans() function R implements \\(k\\)-means algorithm can found stats package, comes R usually already loaded start R. Two key parameters specify x, matrix data frame data, centers either integer indicating number clusters matrix indicating locations initial cluster centroids. data organized row observation column variable feature observation.can see cluster data point got assigned looking cluster element list returned kmeans() function.plot \\(k\\)-means clustering solution.\nFigure 7.4: \\(k\\)-means clustering solution\n","code":"\nset.seed(1234)\nx <- rnorm(12,mean=rep(1:3,each=4),sd=0.2)\ny <- rnorm(12,mean=rep(c(1,2,1),each=4),sd=0.2)\nplot(x,y,col=\"blue\",pch=19,cex=2)\ntext(x+0.05,y+0.05,labels=as.character(1:12))\ndataFrame <- data.frame(x,y)\nkmeansObj <- kmeans(dataFrame,centers=3)\nnames(kmeansObj)\n#ans> [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n#ans> [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"\nkmeansObj$cluster\n#ans>  [1] 3 1 1 3 2 2 2 2 2 2 2 2\nplot(x,y,col=kmeansObj$cluster,pch=19,cex=2)\npoints(kmeansObj$centers,col=1:3,pch=3,cex=3,lwd=3)"},{"path":"kmeans-hierarchical-clustering.html","id":"cluster-validity-choosing-the-number-of-clusters","chapter":"7 Kmeans & Hierarchical Clustering","heading":"7.4.2 Cluster Validity, Choosing the Number of Clusters","text":"result clustering algorithm can different \ndata set input parameters algorithm\ncan extremely modify behavior execution algorithm. \naim cluster validity find partitioning best fits\nunderlying data. Usually 2D data sets used evaluating\nclustering algorithms reader easily can verify result. \ncase high dimensional data visualization visual validation \ntrivial tasks therefore formal methods needed.process evaluating results clustering algorithm \ncalled cluster validity assessment. Two measurement criteria \nproposed evaluating selecting optimal clustering scheme:Compactness: member cluster close \npossible. common measure compactness variance.Compactness: member cluster close \npossible. common measure compactness variance.Separation: clusters widely separated.\nthree common approaches measuring distance two\ndifferent clusters: distance closest member \nclusters, distance distant members distance\ncentres clusters.Separation: clusters widely separated.\nthree common approaches measuring distance two\ndifferent clusters: distance closest member \nclusters, distance distant members distance\ncentres clusters.three different techniques evaluating result \nclustering algorithms, several Validity\nmeasures proposed: Validity measures scalar indices assess\ngoodness obtained partition. Clustering algorithms generally\naim locating well separated compact clusters. number \nclusters chosen equal number groups actually exist \ndata, can expected clustering algorithm identify\ncorrectly. case, misclassifications appear,\nclusters likely well separated compact. Hence,\ncluster validity measures designed quantify separation\ncompactness clusters.Check answer stackoverflow containing  code several methods computing optimal value \\(k\\) \\(k\\)-means cluster analysis: .","code":""},{"path":"kmeans-hierarchical-clustering.html","id":"hierarchical-clustering","chapter":"7 Kmeans & Hierarchical Clustering","heading":"7.5 Hierarchical Clustering","text":"previous part introduced \\(k\\)-means. One potential disadvantage requires us pre-specify number clusters \\(k\\). Hierarchical clustering alternative approach require commit particular choice \\(k\\). Hierarchical clustering added advantage \\(k\\)-means clustering results attractive tree-based representation observations, called dendrogram.common type hierarchical clustering agglomerative clustering (bottom-clustering). refers fact dendrogram (generally depicted upside-tree) built starting leaves combining clusters trunk.","code":""},{"path":"kmeans-hierarchical-clustering.html","id":"dendrogram","chapter":"7 Kmeans & Hierarchical Clustering","heading":"7.5.1 Dendrogram","text":"Suppose simulated data following figure:\nFigure 7.5: Simulated data 45 observations generated three-class model.\ndata figure consists 45 observations two-dimensional space. data generated three-class model; true class labels observation shown distinct colors. However, suppose data observed without class labels, wanted perform hierarchical clustering data. Hierarchical clustering (complete linkage, discussed later) yields result shown Figure 7.6. can interpret dendrogram?\nFigure 7.6: Dendrogram\ndendrogram Figure 7.6, leaf dendrogram represents one 45 observations. However, move tree, leaves begin fuse branches. correspond observations similar . move higher tree, branches fuse, either leaves branches. earlier (lower tree) fusions occur, similar groups observations . hand, observations fuse later (near top tree) can quite different. fact, statement can made precise: two observations, can look point tree branches containing two observations first fused. height fusion, measured vertical axis, indicates different two observations . Thus, observations fuse bottom tree quite similar , whereas observations fuse close top tree tend quite different.example interpreting dendrogram presented Figure 7.7\nFigure 7.7: illustration properly interpret dendrogram nine observations two-dimensional space. Left: dendrogram generated using Euclidean distance complete linkage. Observations 5 7 quite similar , observations 1 6. However, observation 9 similar observation 2 observations 8, 5, 7, even though observations 9 2 close together terms horizontal distance. observations 2, 8, 5, 7 fuse observation 9 height, approximately 1.8. Right: raw data used generate dendrogram can used confirm indeed, observation 9 similar observation 2 observations 8, 5, 7.\nNow understand interpret dendrogram Figure 7.6, can move issue identifying clusters basis dendrogram. order , make horizontal cut across dendrogram, shown following Figure 7.8 cut dendrogram height nine results two clusters.\nFigure 7.8: dendrogram simulated dataset, cut height nine (indicated dashed line). cut results two distinct clusters, shown different colors.\ndistinct sets observations beneath cut can interpreted clusters. Figure 7.9, cutting dendrogram height five results three clusters.\nFigure 7.9: dendrogram simulated dataset, cut height five (indicated dashed line). cut results three distinct clusters, shown different colors.\nterm hierarchical refers fact clusters obtained cutting dendrogram given height necessarily nested within clusters obtained cutting dendrogram greater height.","code":""},{"path":"kmeans-hierarchical-clustering.html","id":"the-hierarchical-clustering-algorithm","chapter":"7 Kmeans & Hierarchical Clustering","heading":"7.5.2 The Hierarchical Clustering Algorithm","text":"hierarchical clustering dendrogram obtained via extremely simple algorithm. begin defining sort dissimilarity measure pair observations. often, Euclidean distance used. algorithm proceeds iteratively. Starting bottom dendrogram, n observations treated cluster. two clusters similar fused now \\(n−1\\) clusters. Next two clusters similar fused , now \\(n − 2\\) clusters. algorithm proceeds fashion observations belong one single cluster, dendrogram complete.Figure 7.10 depicts first steps algorithm.\nFigure 7.10: illustration first steps hierarchical clustering algorithm, complete linkage Euclidean distance. Top Left: initially, nine distinct clusters {1}, {2}, …, {9}. Top Right: two clusters closest together, {5} {7}, fused single cluster. Bottom Left: two clusters closest together, {6} {1},fused single cluster. Bottom Right: two clusters closest together using complete linkage, {8} cluster {5, 7}, fused single cluster.\nsummarize, hierarchical clustering algorithm given following Algorithm:algorithm seems simple enough, one issue addressed. Consider bottom right panel Figure 7.10. determine cluster {5, 7} fused cluster {8}? concept dissimilarity pairs observations, define dissimilarity two clusters one clusters contains multiple observations? concept dissimilarity pair observations needs extended pair groups observations. extension achieved developing notion linkage, defines dissimilarity two groups observations. four common types linkage: complete, average, single, centroid briefly described like follows:Complete: Maximal intercluster dissimilarity. Compute pairwise dissimilarities observations cluster observations cluster B, record largest dissimilarities.Complete: Maximal intercluster dissimilarity. Compute pairwise dissimilarities observations cluster observations cluster B, record largest dissimilarities.Single: Minimal intercluster dissimilarity. Compute pairwise dissimilarities observations cluster observations cluster B, record smallest dissimilarities. Single linkage can result extended, trailing clusters single observations fused one---timeSingle: Minimal intercluster dissimilarity. Compute pairwise dissimilarities observations cluster observations cluster B, record smallest dissimilarities. Single linkage can result extended, trailing clusters single observations fused one---timeAverage: Mean intercluster dissimilarity. Compute pairwise dissimilarities observations cluster observations cluster B, record average dissimilarities.Average: Mean intercluster dissimilarity. Compute pairwise dissimilarities observations cluster observations cluster B, record average dissimilarities.Centroid: Dissimilarity centroid cluster (mean vector length p) centroid cluster B. Centroid linkage can result undesirable inversions.Centroid: Dissimilarity centroid cluster (mean vector length p) centroid cluster B. Centroid linkage can result undesirable inversions.Average complete linkage generally preferred single linkage, tend yield balanced dendrograms. Centroid linkage often used genomics.dissimilarities computed Step 2(b) hierarchical clustering algorithm depend type linkage used, well choice dissimilarity measure. Hence, resulting dendrogram typically depends quite strongly type linkage used, shown Figure 7.11.\nFigure 7.11: Average, complete, single linkage applied example data set. Average complete linkage tend yield balanced clusters.\n","code":""},{"path":"kmeans-hierarchical-clustering.html","id":"hierarchical-clustering-in","chapter":"7 Kmeans & Hierarchical Clustering","heading":"7.5.3 Hierarchical clustering in ","text":"Let’s illustrate perform hierarchical clustering dataset  Ligue1 2017-2018 .\n◼\n","code":"\n# Load the dataset\nligue1 <- read.csv(\"datasets/ligue1_17_18.csv\", row.names=1,sep=\";\")\n\n# Work with standardized data\nligue1_scaled <- data.frame(scale(ligue1))\n\n# Compute dissimilary matrix - in this case Euclidean distance\nd <- dist(ligue1_scaled)\n\n# Hierarchical clustering with complete linkage\ntreeComp <- hclust(d, method = \"complete\")\nplot(treeComp)\n\n# With average linkage\ntreeAve <- hclust(d, method = \"average\")\nplot(treeAve)\n\n# With single linkage\ntreeSingle <- hclust(d, method = \"single\")\nplot(treeSingle) # Chaining\n\n# Set the number of clusters after inspecting visually\n# the dendrogram for \"long\" groups of hanging leaves\n# These are the cluster assignments\ncutree(treeComp, k = 2) \n#ans>      Paris-SG        Monaco          Lyon     Marseille        Rennes \n#ans>             1             1             1             1             2 \n#ans>      Bordeaux Saint-Etienne          Nice        Nantes   Montpellier \n#ans>             2             2             2             2             2 \n#ans>         Dijon      Guingamp        Amiens        Angers    Strasbourg \n#ans>             2             2             2             2             2 \n#ans>          Caen         Lille      Toulouse        Troyes          Metz \n#ans>             2             2             2             2             2\ncutree(treeComp, k = 3) \n#ans>      Paris-SG        Monaco          Lyon     Marseille        Rennes \n#ans>             1             1             1             1             2 \n#ans>      Bordeaux Saint-Etienne          Nice        Nantes   Montpellier \n#ans>             2             2             2             2             2 \n#ans>         Dijon      Guingamp        Amiens        Angers    Strasbourg \n#ans>             3             3             3             3             3 \n#ans>          Caen         Lille      Toulouse        Troyes          Metz \n#ans>             3             3             3             3             3\ncutree(treeComp, k = 4) \n#ans>      Paris-SG        Monaco          Lyon     Marseille        Rennes \n#ans>             1             2             2             2             3 \n#ans>      Bordeaux Saint-Etienne          Nice        Nantes   Montpellier \n#ans>             3             3             3             3             3 \n#ans>         Dijon      Guingamp        Amiens        Angers    Strasbourg \n#ans>             4             4             4             4             4 \n#ans>          Caen         Lille      Toulouse        Troyes          Metz \n#ans>             4             4             4             4             4"},{"path":"pw-7.html","id":"pw-7","chapter":"PW 7","heading":"PW 7","text":"practical work learn create report Rstudio using Rmarkdown files. apply \\(k\\)-means clustering algorithm using standard function kmeans() , use dataset Ligue1 2017/2018.","code":""},{"path":"pw-7.html","id":"reporting-1","chapter":"PW 7","heading":"Reporting","text":"","code":""},{"path":"pw-7.html","id":"markdown","chapter":"PW 7","heading":"Markdown","text":"Markdown lightweight markup language plain text formatting syntax designed can converted HTML many formats (pdf, docx, etc..).Click  see example markdown (.md) syntaxes result HTML. markdown syntaxes right HTML result left. can modify source text see result.\nExtra: markdown online editors can\nuse, like dillinger.io/. See \nMarkdown source file HTML preview. Play source text \nsee result preview.\n","code":""},{"path":"pw-7.html","id":"r-markdown","chapter":"PW 7","heading":"R Markdown","text":"R Markdown variant Markdown embedded  code chunks, used knitr package make easy create reproducible web-based reports.First, Rstudio create new R Markdown file. default template opened.  code R chunks. Click knit, save file see produced output. output html report containing results  codes.\nfile named report.Rmd, report named report.html.\nMake sure latest version Rstudio.\n\nproblems creating R Markdown file\n(problem installing packages, etc..) close Rstudio\nreopen administrative tools retry.\n\n\nready submit report (.html file) \nend class.\n\n\nready submit report (.html file) \nend class.\n\n\nreport must named:\n\n\nreport must named:\n\nYouLastName_YourFirstName_WeekNumber.html\ncan find informations R Markdown site: rmarkdown.rstudio.com.may also find following resources helpful:R Markdown Reference GuideThe R Markdown Cheatsheet","code":""},{"path":"pw-7.html","id":"the-report-to-be-submitted","chapter":"PW 7","heading":"The report to be submitted","text":"Rstudio, start creating R Markdown file. create default template opened following first lines:lines YAML header choose settings report (title, author, date, appearance, etc..)submitted report, use following YAML header:\nImportant Remark: Click settings button\nRstudio’s text editor choose Chunk Output Console.\ncore report:Put every exercise section, name section Exercise (exercise’s number).Paste exercise content.Write code exercise R chunks.Run chunk make sure works.need, explain results.Click knit","code":"---\ntitle: \"Untitled\"\noutput: html_document\n------\ntitle: \"Week 7\"\nsubtitle: \"Clustering\"\nauthor: LastName FirstName\ndate: \"`#r format(Sys.time())`\" # remove the # to show the date\noutput:\n  html_document:\n    toc: true\n    toc_depth: 2\n    theme: flatly\n---"},{"path":"pw-7.html","id":"k-means-clustering","chapter":"PW 7","heading":"\\(k\\)-means clustering","text":"1. Download dataset:  Ligue1 2017-2018  import . Put argument row.names 1.2. Print first two rows dataset total number features dataset.\ncan create awesome HTML table using function\nkable knitr library. example, \nwant show first 5 lines 5 columns dataset, \ncan use knitr::kable(ligue1[1:5,1:5]). Give try \nsee result html report!\n","code":"\n# You can import directly from my website (instead of downloading it..)\nligue1 <- read.csv(\"http://mghassany.com/MLcourse/datasets/ligue1_17_18.csv\", row.names=1, sep=\";\")"},{"path":"pw-7.html","id":"pointscards","chapter":"PW 7","heading":"pointsCards","text":"3. first consider smaller dataset easily understand results \\(k\\)-means. Create new dataset consider Points Yellow.cards original dataset. Name pointsCards4. Apply \\(k\\)-means pointsCards. Chose \\(k=2\\) clusters put number iterations 20. Store results km. (Remark: kmeans() uses random initialization clusters, results may vary one call another. Use set.seed() reproducible outputs).5. Print describe inside km.6. coordinates centers clusters (called also prototypes centroids) ?7. Plot data (Yellow.cards vs Points). Color points corresponding cluster.8. Add previous plot clusters centroids add names observations.9. Re-run \\(k\\)-means pointsCards using 3 4 clusters store results km3 km4 respectively. Visualize results like question 7 8.many clusters \\(k\\) need practice? single answer: advice try several compare. Inspecting ‘between_SS / total_SS’ good trade-number clusters percentage total variation explained usually gives good starting point deciding \\(k\\) (criterion select \\(k\\) similar PCA).several methods computing optimal value \\(k\\)  code following stackoverflow answer: .10. Visualize “within groups sum squares” \\(k\\)-means clustering results (use code link ).11. Modify code previous question order visualize ‘between_SS / total_SS’. Interpret results.","code":""},{"path":"pw-7.html","id":"ligue-1","chapter":"PW 7","heading":"Ligue 1","text":"far, taken information two variables performing clustering. Now apply kmeans() original dataset ligue1. Using PCA, can visualize clustering performed available variables dataset.default, kmeans() standardize variables, affect clustering result. consequence, clustering dataset different one variable expressed millions tenths. want avoid distortion, use scale automatically center standardize dataset (result matrix, need transform data frame ).12. Scale dataset transform data frame . Store scaled dataset ligue1_scaled.13. Apply kmeans() ligue1 ligue1_scaled using 3 clusters 20 iterations. Store results km.ligue1 km.ligue1.scaled respectively (forget set seed)14. many observations cluster km.ligue1 km.ligue1.scaled ? (can use table()). obtain results perform kmeans() scaled unscaled data?","code":""},{"path":"pw-7.html","id":"pca","chapter":"PW 7","heading":"PCA","text":"15. Apply PCA ligue1 dataset store results pcaligue1. need apply PCA scaled dataset? Justify answer.16. Plot observations variables first two principal components (biplot). Interpret results.17. Visualize teams first two principal components color respect cluster.18. Recall figure question 17 visualization PC1 PC2 clustering done variables, PC1 PC2. Now apply kmeans() clustering taking first two PCs instead variables original dataset. Visualize results compare question 17.\napplying \\(k\\)-means \nPCs obtain different less accurate result, still \ninsightful way.\n","code":"\n# You can use the following code, based on `factoextra` library.\nfviz_cluster(km.ligue1, data = ligue1, # km.ligue1 is where you stored your kmeans results\n              palette = c(\"red\", \"blue\", \"green\"), # 3 colors since 3 clusters\n              ggtheme = theme_minimal(),\n              main = \"Clustering Plot\"\n)"},{"path":"pw-7.html","id":"implementing-k-means","chapter":"PW 7","heading":"Implementing k-means","text":"part, perform \\(k\\)-means clustering manually, \n\\(k=2\\), small example \\(n=6\\) observations \\(p=2\\)\nfeatures. observations follows.19. Plot observations.20. Randomly assign cluster label observation. can\nuse sample() command  . Report cluster\nlabels observation.21. Compute centroid cluster.22. Create function calculates Euclidean distance two observations.23. Assign observation centroid closest, \nterms Euclidean distance. Report cluster labels \nobservation.24. Repeat 21 23 answers obtained stop changing.25. plot 19, color observations according \ncluster labels obtained.","code":""},{"path":"pw-7.html","id":"hierarchical-clustering-1","chapter":"PW 7","heading":"Hierarchical clustering","text":"","code":""},{"path":"pw-7.html","id":"distances-dist","chapter":"PW 7","heading":"Distances dist()","text":"calculate distance  use dist() function. tutorial use .","code":"\n# Generate a matrix M of values from 1 to 15 with 5 rows and 3 columns\nM <- matrix(1:15,5,3)\nM\n#ans>      [,1] [,2] [,3]\n#ans> [1,]    1    6   11\n#ans> [2,]    2    7   12\n#ans> [3,]    3    8   13\n#ans> [4,]    4    9   14\n#ans> [5,]    5   10   15\n# - Compute the distance between rows of M.\n# - The default distance is the euclidian distance.\n# - Since there are 3 columns, it is the euclidian\n#        distance between tri-dimensional points.\ndist(M)\n#ans>      1    2    3    4\n#ans> 2 1.73               \n#ans> 3 3.46 1.73          \n#ans> 4 5.20 3.46 1.73     \n#ans> 5 6.93 5.20 3.46 1.73\n# To compute the Manhattan distance \ndist(M, method= \"manhattan\")\n#ans>    1  2  3  4\n#ans> 2  3         \n#ans> 3  6  3      \n#ans> 4  9  6  3   \n#ans> 5 12  9  6  3"},{"path":"pw-7.html","id":"dendrogram-hclust","chapter":"PW 7","heading":"Dendrogram hclust()","text":"","code":"\n# First we construct the dendrogram \ndendro <- hclust(dist(M))\n\n# Then we plot it\nplot(dendro)"},{"path":"pw-7.html","id":"hierarchical-clustering-on-iris-dataset","chapter":"PW 7","heading":"Hierarchical clustering on Iris dataset","text":"1. Download iris dataset   import R.2. Choose randomly 40 observations iris dataset store sample dataset sampleiris.3. Calculate euclidean distances flowers. Store results matrix called D. (Remark: last column dataset class labels flowers)4. Construct dendrogram iris dataset using method average. Store result dendro.avg.5. Plot dendrogram.6. Plot dendrogram using following command:7. cut dendrogram obtain clustering use cutree. can choose number clusters wish obtain, can cut choosing height dendrogram figure. Cut dendrogram order obtain 3 clusters. Store results vector groups.avg.8. Visualize cut tree using function rect.hclust(). can choose colors rectangles !9. Compare obtained results obtained Hierarchical clustering real class labels flowers (function table()). Interpret results.Bonus: can cut tree manually (demand!). , plot dendrogram first use function identify().\nfigure, click clusters wish obtain. hit Escape finish.10. Now apply Hierarchical clustering iris dataset (150 observations). Choose 3 clusters compare results real class labels. Compare different methods Hierarchical clustering (average, complete single linkages).\n◼\n","code":"\nplot(dendro.avg, hang=-1, label=sampleiris$class)"},{"path":"gaussian-mixture-models-em.html","id":"gaussian-mixture-models-em","chapter":"8 Gaussian Mixture Models & EM","heading":"8 Gaussian Mixture Models & EM","text":"previous chapter saw \\(k\\)-means algorithm considered hard clustering technique, point allocated one cluster. \\(k\\)-means, cluster described centroid. flexible, may problems clusters overlapping, ones circular shape.chapter, introduce model-based clustering technique, Expectation Maximization (EM). apply using Gaussian Mixture Models (GMM).EM Clustering, can go step describe cluster centroid (mean), covariance (can elliptical clusters), weight (size cluster). probability point belongs cluster now given multivariate Gaussian probability distribution (multivariate - depending multiple variables). also means can calculate probability point Gaussian ‘bell’, .e. probability point belonging cluster. comparison performances \\(k\\)-means EM clustering artificial dataset shown Figure 8.1.\nFigure 8.1: Comparison \\(k\\)-means EM artificial data called Mouse dataset. Using Variances, EM algorithm can describe normal distributions exact, \\(k\\)-means splits data Voronoi-Cells.\nstart chapter reminding Gaussian distribution, introduce Mixture Gaussians finish explaining Expectation-Maximization algorithm.","code":""},{"path":"gaussian-mixture-models-em.html","id":"the-gaussian-distribution","chapter":"8 Gaussian Mixture Models & EM","heading":"8.1 The Gaussian distribution","text":"Gaussian, also known normal distribution, widely used\nmodel distribution continuous variables. case \nsingle variable \\(x\\), Gaussian distribution can written \nform\\[\\begin{equation}\n\\mathcal{N}(x|m,\\sigma^2)=\\frac{1}{(2 \\pi \\sigma^2 )^{1/2}} \\exp \\left\\lbrace - \\frac{1}{2 \\sigma^2} (x-m)^2\\right\\rbrace\n\\tag{8.1}\n\\end{equation}\\]\\(m\\) mean \\(\\sigma^2\\) variance.\\(D\\)-dimensional vector \\(X\\), multivariate Gaussian distribution\ntake form\\[\\begin{equation}\n\\mathcal{N}(X|\\mu,\\Sigma)=\\frac{1}{(2 \\pi)^{D/2}} \\frac{1}{|\\Sigma|^{1/2}} \\exp \\left\\lbrace - \\frac{1}{2} (X-\\mu)^T \\Sigma^{-1} (X-\\mu) \\right\\rbrace\n\\tag{8.2}\n\\end{equation}\\]\\(\\mu\\) \\(D\\)-dimensional mean vector, \\(\\Sigma\\) \\(D\\times D\\)\ncovariance matrix, \\(|\\Sigma|\\) denotes determinant \\(\\Sigma\\).Gaussian distribution arises many different contexts can motivated variety different perspectives. example, consider sum multiple random variables. central limit theorem (due Laplace) tells us , subject certain mild conditions, sum set random variables, course random variable, distribution becomes increasingly Gaussian number terms sum increases.","code":""},{"path":"gaussian-mixture-models-em.html","id":"mixture-of-gaussians","chapter":"8 Gaussian Mixture Models & EM","heading":"8.2 Mixture of Gaussians","text":"Gaussian distribution important analytical properties, suffers \nsignificant limitations comes modeling real data sets.\nConsider example shown Figure 8.2 applied \n’Old Faithful’ data set, data set comprises 272 measurements eruption Old Faithful geyser Yellowstone National Park USA. measurement comprises duration eruption minutes (horizontal axis) time minutes next eruption (vertical axis). see data set forms two dominant clumps, simple Gaussian distribution unable capture structure, whereas linear superposition two Gaussians gives better characterization \ndata set. superpositions, formed taking linear combinations \nbasic distributions Gaussians, can formulated \nprobabilistic models known mixture distributions.\nFigure 8.2: Plots ’old faithful’ data blue curves show contours constant probability density. left single Gaussian ditribution fitted data using maximum likelihood. right distribution given linear combination two Gaussians fitted data maximum likelihood using EM technique, gives better representation data\nFigure 8.3 see linear combination Gaussians can give rise complex densities. using sufficient number Gaussians, adjusting means covariances well coefficients linear combination, almost continuous density can approximated arbitrary accuracy.\nFigure 8.3: Example Gaussian mixture distribution one dimension showing three Gaussians (scaled coefficient) blue sum red.\ntherefore consider superposition \\(K\\) Gaussian densities \nform\\[\\label{eq:gaussian}\np(x)= \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x| \\mu_k, \\Sigma_k)\\]called mixture Gaussians. Gaussian density\n\\(\\mathcal{N}(x| \\mu_k, \\Sigma_k)\\) called component mixture\nmean \\(\\mu_k\\) covariance \\(\\Sigma_k\\).parameters \\(\\pi_k\\) called mixing coefficients. verify \nconditions\n\\[\\sum_{k=1}^{K} \\pi_k = 1 \\quad \\text{} \\quad 0 \\leq \\pi_k \\leq 1\\]order find equivalent formulation Gaussian mixture\ninvolving explicit latent variable, introduce \n\\(K\\)-dimensional binary random variable \\(z\\) 1--\\(K\\)\nrepresentation particular element \\(z_k\\) equal 1 \nelements equal 0. values \\(z_k\\) therefore satisfy\n\\(z_k \\\\{0,1\\}\\) \\(\\sum_k z_k =1\\), see \\(K\\)\npossible states vector \\(z\\) according element \nnonzero. marginal distribution \\(z\\) specified terms \nmixing coefficients \\(\\pi_k\\) , \\[p(z_k=1)=\\pi_k\\]\nlatent\nvariable variable directly measurable, \nvalue can inferred taking measurements.\n\nhappens lot machine learning, robotics, statistics \nfields. example, may able directly quantify\nintelligence (’s countable thing like number brain cells\n), think exists can run experiments may\ntell us intelligence. intelligence latent variable\naffects performance multiple tasks even though can \ndirectly measured (link).\nconditional distribution \\(x\\) given particular value \\(z\\) \nGaussian\\[p(x|z_k=1)= \\mathcal{N}(x| \\mu_k, \\Sigma_k)\\]joint distribution given \\(p(z)p(x|z)\\), marginal\ndistribution \\(x\\) obtained summing joint distribution\npossible states \\(z\\) give\\[p(x)= \\sum_z p(z)p(x|z) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x| \\mu_k, \\Sigma_k)\\]Now, able work joint distribution \\(p(x|z)\\) instead \nmarginal distribution \\(p(x)\\). leads significant\nsimplification, notably introduction \nExpectation-Maximization (EM) algorithm.Another quantity play important role conditional\nprobability \\(z\\) given \\(x\\). shall use \\(r(z_k)\\) denote\n\\(p(z_k = 1|x)\\), whose value can found using Bayes’ theorem\\[\\begin{align}\nr(z_k)= p(z_k = 1|x) &= \\frac{ p(z_k = 1) p(x|(z_k=1)}{\\displaystyle \\sum_{j=1}^{K} p(z_j = 1) p(x|(z_j=1)} \\nonumber \\\\\n&= \\frac{\\pi_k \\mathcal{N}(x| \\mu_k, \\Sigma_k)}{\\displaystyle \\sum_{j=1}^{K} \\pi_j \\mathcal{N}(x| \\mu_j, \\Sigma_j)}\n\\tag{8.3}\n\\end{align}\\]shall view \\(\\pi_k\\) prior probability \\(z_k = 1\\), \nquantity \\(r(z_k)\\) corresponding posterior probability \nobserved \\(x\\). shall see next section, \\(r(z_k)\\) can also \nviewed responsibility component \\(k\\) takes ’explaining’\nobservation \\(x\\).\nDoesn’t reminds Equation 4.1\nused Bayes’ theorm Classification? \\(p_k(x)\\) posterior\nprobability observation \\(X=x\\)\nbelongs \\(k\\)-th class. \ndifference data unlabled (class), \ncreate latent (hidden, unobserved) variable \\(z\\) play similar role.\nFigure 8.4 role responsibilities illustrated sample 500 points drawn mixture three Gaussians.\nFigure 8.4: Example 500 points drawn mixture 3 Gaussians. () Samples joint distribution \\(p(z)p(x|z)\\) three states \\(z\\), corresponding three components mixture, depicted red, green, blue, (b) corresponding samples marginal distribution \\(p(x)\\), obtained simply ignoring values \\(z\\) just plotting \\(x\\) values. data set () said complete, whereas (b) incomplete. (c) samples colours represent value responsibilities \\(r(z_{nk})\\) associated data point \\(x_n\\), obtained plotting corresponding point using proportions red, blue, green ink given \\(r(z_{nk})\\) \\(k = 1,2,3\\), respectively.\nform Gaussian mixture distribution governed parameters \\(\\pi\\), \\(\\mu\\) \\(\\Sigma\\), used notation \\(\\pi=\\{\\pi_1,\\ldots,\\pi_K\\}\\), \\(\\mu=\\{\\mu_1,\\ldots,\\mu_K\\}\\) \\(\\Sigma=\\{\\Sigma_1,\\ldots,\\Sigma_K\\}\\). One way set values parameters use maximum likelihood. log likelihood function given \\[\\ln p(X|\\pi,\\mu,\\Sigma)=\\sum_{n=1}^{N} \\ln \\left\\lbrace \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x_n | \\mu_k, \\Sigma_k) \\right\\rbrace\\]\nimmediately see situation now much complex single Gaussian, due presence summation \\(k\\) inside logarithm. result, maximum likelihood solution \nparameters longer closed-form analytical solution. One approach maximizing likelihood function use iterative numerical optimization techniques. Alternatively can employ powerful framework called Expectation Maximization, \ndiscussed chapter.","code":""},{"path":"gaussian-mixture-models-em.html","id":"em-for-gaussian-mixtures","chapter":"8 Gaussian Mixture Models & EM","heading":"8.3 EM for Gaussian Mixtures","text":"Suppose data set observations \\(\\{x_1, \\ldots, x_N\\}\\), \ngives data set \\(X\\) size \\(N \\times D\\), wish model data using mixture \nGaussians. Similarly, corresponding latent variable denoted \n\\(N \\times K\\) matrix \\(Z\\) rows \\(z_n^K\\).\nRecall objective estimate parameters \\(\\pi\\), \\(\\mu\\) \\(\\Sigma\\) order estimate posterior\nprobabilities (named also responsibilities, called \\(\\, r(z_k)\\) chapter). , \nfind estimators maximize log likelihood\nfunction.\nassume data points ..d. (independent \nidentically distributed), can calculate log \nlikelihood function, given \\[\\begin{equation}\n\\tag{8.4}\n\\ln p(X|\\pi,\\mu,\\Sigma)=\\sum_{n=1}^{N} \\ln \\left\\lbrace \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x_n | \\mu_k, \\Sigma_k) \\right\\rbrace\n\\end{equation}\\]elegant powerful method finding maximum likelihood solutions\nmodels latent variables called \nExpectation Maximization algorithm, EM algorithm.Setting derivatives \\(\\ln p(X|\\pi,\\mu,\\Sigma)\\) \n(8.4) respectively respect \n\\(\\mu_k,\\Sigma_k\\) \\(\\pi_k\\) zero, obtain\\[\\begin{equation}\n\\tag{8.5}\n\\mu_k = \\frac{1}{N_k} \\sum_{n=1}^{N} r(z_{nk}) x_n\n\\end{equation}\\]define \\[N_k= \\sum_{n=1}^{N}r(z_{nk})\\]can interpret \\(N_k\\) effective number points assigned cluster \\(k\\). Note\ncarefully form solution. see mean \\(\\mu_k\\) \\(k\\)-th Gaussian\ncomponent obtained taking weighted mean points data set,\nweighting factor data point \\(x_n\\) given posterior probability\n\\(r(z_{nk})\\) component \\(k\\) responsible generating \\(x_n\\).\\(\\sigma_k\\) obtain\\[\\begin{equation}\n\\tag{8.6}\n\\Sigma_k= \\frac{1}{N_k} \\sum_{n=1}^{N} r(z_{nk}) (x_n - \\mu_k)(x_n - \\mu_k)^T\n\\end{equation}\\]form corresponding result single Gaussian fitted \ndata set, data point weighted corresponding posterior probability denominator given effective number points\nassociated corresponding component.Finally, mixing coefficients \\(\\pi_k\\) obtain\\[\\begin{equation}\n\\tag{8.7}\n\\pi_k=\\frac{N_k}{N}\n\\end{equation}\\]mixing coefficient \\(k\\)-th component given average responsibility component takes explaining data points.first choose initial values means, covariances, \nmixing coefficients. alternate following two updates\nshall call E step M step. expectation step,\nE step, use current values parameters evaluate \nposterior probabilities, responsibilities, given Eq.\n(8.3). use probabilities \nmaximization step, M step, re-estimate means, covariances,\nmixing coefficients using results Equations (8.5),\n(8.6) (8.7). algorithm EM mixtures \nGaussians shown following Algorithm:Choose value \\(K\\), \\(1 < K < N\\).Initialize means \\(\\mu_k\\), covariances \\(\\Sigma_k\\) mixing coefficients \\(\\pi_k\\) randomly.Evaluate initial value log likelihood.E step:Evaluate responsibilities using current parameter values:\n\\[r(z_{nk})= \\frac{\\pi_k \\mathcal{N}(x| \\mu_k, \\Sigma_k)}{\\displaystyle \\sum_{j=1}^{K} \\pi_j \\mathcal{N}(x| \\mu_j, \\Sigma_j)}\\]M step:Re-estimate parameters using current responsibilities:\n\\[\\mu_k = \\frac{1}{N_k} \\sum_{n=1}^{N} r(z_{nk}) x_n\\]\n\\[\\Sigma_k= \\frac{1}{N_k} \\sum_{n=1}^{N} r(z_{nk}) (x_n - \\mu_k)(x_n - \\mu_k)^T\\]\n\\[\\pi_k=\\frac{N_k}{N}\\]\n\\[\\text{} \\quad N_k= \\sum_{n=1}^{N}r(z_{nk})\\]Re-estimate parameters using current responsibilities:\n\\[\\mu_k = \\frac{1}{N_k} \\sum_{n=1}^{N} r(z_{nk}) x_n\\]\n\\[\\Sigma_k= \\frac{1}{N_k} \\sum_{n=1}^{N} r(z_{nk}) (x_n - \\mu_k)(x_n - \\mu_k)^T\\]\n\\[\\pi_k=\\frac{N_k}{N}\\]\n\\[\\text{} \\quad N_k= \\sum_{n=1}^{N}r(z_{nk})\\]Evaluate log likelihood:\n\\[\\ln p(X|\\pi,\\mu,\\Sigma)=\\sum_{n=1}^{N} \\ln \\left\\lbrace \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x_n | \\mu_k, \\Sigma_k) \\right\\rbrace\\]\nconvergence either parameters log likelihood. convergence criterion satisfied return E step.Evaluate log likelihood:\n\\[\\ln p(X|\\pi,\\mu,\\Sigma)=\\sum_{n=1}^{N} \\ln \\left\\lbrace \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x_n | \\mu_k, \\Sigma_k) \\right\\rbrace\\]\nconvergence either parameters log likelihood. convergence criterion satisfied return E step.EM algorithm mixture two Gaussians applied rescaled\nOld Faithful data set illustrated Figure 8.5. \nplot () see initial configuration, Gaussian component \nshown blue red circles. Plot (b) shows result initial\nE step update responsibilities. Plot (c) shows M step\nupdate parameters. Plots (d), (e), (f) show results\n2, 5, 20 complete cycles EM, respectively. plot (f) \nalgorithm close convergence.\nFigure 8.5: Illustration EM algorithm using Old Faithful dataset. mixture two Gaussians used.\n\nSummary chapter:\n\nGaussian Mixture Models (GMM) take Gaussian add another\nGaussian(s).\n\nallows model complex data.\n\nfit GMM Expectation-Maximization (EM) algorithm.\n\nExpectation-Maximization (EM) algorithm series steps find\ngood parameter estimates latent variables.\n\nEM steps:\n\nInitialize t parameter estimates.\n\n\nGiven current parameter estimates, find minimum log\nlikelihood \\(Z\\) (data + latent\nvariables).\n\n\nGivent current data, find better parameter estimates.\n\n\nRepeat steps 2 & 3.\n\n\nInitialize t parameter estimates.\n\nGiven current parameter estimates, find minimum log\nlikelihood \\(Z\\) (data + latent\nvariables).\n\nGivent current data, find better parameter estimates.\n\nRepeat steps 2 & 3.\n\n◼\n","code":""},{"path":"pw-8.html","id":"pw-8","chapter":"PW 8","heading":"PW 8","text":"first part PW :Learn fit Gaussian Mixture Model (GMM) using mclust  package.Compare \\(k\\)-means GMM artificial data (2-D data).Fit GMM univariate (1-D) simulated data.second part PW build algorithm scratch. algorithm must fit GMM model using Expectation-Maximization (EM) technique multi dimensional dataset.","code":""},{"path":"pw-8.html","id":"report-template","chapter":"PW 8","heading":"Report template","text":"session, write report RMarkdown script, must use following YAML header settings (replace default YAML header one, edit author’s name show date):","code":"---\ntitle: \"Week 8\"\nsubtitle: \"Gaussian Mixture Models & EM\"\nauthor: LastName FirstName\ndate: \"`#r format(Sys.time())`\" # remove the # to show the date\noutput:\n  html_document:\n    toc: true\n    toc_depth: 2\n    toc_float: true\n    theme: cerulean\n    highlight: espresso\n---"},{"path":"pw-8.html","id":"em-using-mclust","chapter":"PW 8","heading":"8.4 EM using mclust","text":"","code":""},{"path":"pw-8.html","id":"gmm-vs-k-means","chapter":"PW 8","heading":"GMM vs \\(k\\)-means","text":"section, use two artificial (simulated) datasets know ground truth (true labels) order compare performances \\(k\\)-means GMM. fit GMM using EM technique need install use package mclust.1. Download import  Data1   Data2 . Plot datasets window. Color observations respect ground truth, like Figure 8.6.\nFigure 8.6: Data1 plotted left. Data2 right. Colors shown respect ground truth.\n2. Apply \\(k\\)-means datasets 4 clusters. Plot dataset window color observations respect \\(k\\)-means results. Interpret results.\nOne way think \\(k\\)-means model places circle\n(, higher dimensions, hyper-sphere) center \ncluster, radius defined distant point cluster.\nradius acts hard cutoff cluster assignment within \ntraining set: point outside circle considered member\ncluster. can try visualize circles plots.\n3. Now fit GMM model datasets. , load mclust library. can use function Mclust() data (function choose automatically number mixtures, basing BIC criterion). Use clustering results GMM model visualize results datasets, color observations respect clusters obtained GMM model. Interpret results.following questions section, explore mclust library offers. Apply functions Data2.mclust contributed  package model-based clustering, classification, density estimation based finite normal mixture modelling. provides functions parameter estimation via EM algorithm normal mixture models variety covariance structures, functions simulation models. Also included functions combine model-based hierarchical clustering, EM mixture estimation Bayesian Information Criterion (BIC) comprehensive strategies clustering, density estimation discriminant analysis. Additional functionalities available displaying visualizing fitted models along clustering, classification, density estimation results.4. Show summary GMM model fitted Data2. Explain shows.5. mclust package offers visualization. plot two-dimensional data, use standard plot function applied model. Apply following code, given model named gmm_model, interpret shows.6. mclust package uses Bayesian Information Criterion (BIC) choose best number mixtures. see values BIC different number mixtures use following code.Information criteria based penalised forms log-likelihood. likelihood increases addition components, penalty term number estimated parameters subtracted log-likelihood. BIC popular choice context GMMs, takes form\\[ \\text{BIC} \\approx 2 \\ell (X|\\hat{\\theta}) - \\nu \\log (n)\\]\\(\\theta\\) set parameters (GMM \\(\\theta=\\{\\mu,\\Sigma,\\pi\\})\\), \\(\\ell (X|\\hat{\\theta})\\) log-likelihood Maximum Likelihood Estimators \\(\\hat{\\theta}\\) model, \\(n\\) sample size, \\(\\nu\\) number estimated parameters. select model maximises BIC.\nsee figure showing BIC values different\nparameterisations within-group covariance matrix \\(\\Sigma_k\\). GMM, clusters \nellipsoidal, centered mean vector \\(\\mu_k\\), geometric features,\nvolume, shape orientation, determined covariance\nmatrix \\(\\Sigma_k\\).\n7. Though GMM often categorized clustering algorithm, fundamentally algorithm density estimation. say, result GMM fit data technically clustering model, generative probabilistic model describing distribution data. Density estimation plays important role applied statistical data analysis theoretical research. density estimate based GMM can obtained using function densityMclust(). Apply Data2 visualize estimated densities (show “image” “perspective” plot bivariate density estimate).","code":"\nplot(gmm_model, what = \"classification\")\nplot(gmm_model, what = \"uncertainty\")\nplot(gmm_model, what = \"BIC\")"},{"path":"pw-8.html","id":"em-on-1d","chapter":"PW 8","heading":"EM on 1D","text":"part must fit GMM model one dimensional simulated data.8. Create data table 300 observations two columns:first column contains generated data. data generated three Gaussian distributions different parameters.second column corresponds groud truth (every observation generated Gaussian).Hint: functions may need rnorm(), rep(), rbind() cbind().must course set seed (sutdent_pk).\nexample 9 generated values three Gaussians shown following table:9. Show generated data one axe (kind figures called stripchart), color respect ground truth, must obtain something like:10. Plot histogram corresponding generated data. Interpret .11. Fit GMM model generated data. Print summary visualize results. Explain results.12. Apply density estimate generated data visualize . Interpret obtained figure.","code":""},{"path":"pw-8.html","id":"em-from-scratch","chapter":"PW 8","heading":"8.5 EM from scratch","text":"second part PW build GMM model scratch, must develop EM technique fit model.2.1 Generate two-dimensional dataset \\(k\\)-component Gaussian mixture density different means different covariance matrices. choose mixing proportions \\(\\{\\pi_1,\\ldots,\\pi_k\\}\\).2.2 Implement EM algorithm fit GMM generated data:Initialize mixing proportions covariance matrices (e.g., can initialize equal mixing proportions Identity covariance matrices).Initialize means “randomly” (choice \\(k\\)).EM training loop, store value observed-data log-likelihood iteration.convergence, plot log-likelihood curve.2.3 Create function selects number mixture components computing values BIC criterion \\(k\\) varying 1 10.2.4 generated data, compare results obtained algorithm developed ground truth (terms chosen number mixture components; terms error rate).2.5 Apply algorithm developed  Iris  dataset.\nvisualize results Iris dataset, can use PCA projection\ncoloring respect clustering results.\n\npackage mclust provides also dimensionality\nreduction technique function MclustDR().\n\n◼\n","code":""},{"path":"hackathon.html","id":"hackathon","chapter":"Hackathon","heading":"Hackathon","text":"session special.–>–>–>\n –>\n◼\n","code":""},{"path":"final-grades.html","id":"final-grades","chapter":"A Final Grades","heading":"A Final Grades","text":"can get grade details entering badge number (example 702891).","code":""},{"path":"app-introRStudio.html","id":"app-introRStudio","chapter":"B Introduction to RStudio","heading":"B Introduction to RStudio","text":"RStudio employed Integrated Development Environment (IDE)  nowadays. start RStudio see window similar Figure B.1. lot items GUI, described RStudio IDE Cheat Sheet. important things keep mind :code written scripts source panel (upper-right panel Figure B.1);running line code selection script console (first tab lower-right panel Figure B.1), keyboard shortcut 'Ctrl+Enter' (Windows Linux) 'Cmd+Enter' (Mac OS X).\nFigure B.1: Main window RStudio. red shows code panel yellow shows console output. Extracted .\n","code":""},{"path":"app-ht.html","id":"app-ht","chapter":"C Review on hypothesis testing","heading":"C Review on hypothesis testing","text":"process hypothesis testing interesting analogy trial helps understanding elements present formal hypothesis test intuitive way.formally, \\(p\\)-value hypothesis test \\(H0\\) defined :\\(p\\)-value probability obtaining statistic unfavourable \\(H_0\\) observed, assuming \\(H_0\\) true.Therefore, \\(p\\)-value small (smaller chosen level \\(\\alpha\\)), unlikely evidence \\(H_0\\) due randomness. consequence, \\(H_0\\) rejected. \\(p\\)-value large (larger \\(\\alpha\\)), possible evidences \\(H_0\\) merely due randomness data. case, reject \\(H_0\\).\n\\(H_0\\) holds, \\(p\\)-value (random variable) \ndistributed uniformly \\((0,1)\\). \n\\(H_0\\) hold, \ndistribution \\(p\\)-value \nuniform concentrated \\(0\\)\n(rejections \\(H_0\\) take\nplace).\n","code":""},{"path":"use-qual.html","id":"use-qual","chapter":"D Use of qualitative predictors","heading":"D Use of qualitative predictors","text":"important situation deal qualitative, quantitative, predictors fit regression model. Qualitative predictors, also known categorical variables , ’s terminology, factors, common, example social sciences. Dealing requires care proper understanding variables represented.simplest case situation two levels. binary variable \\(C\\) two levels (example, b) can represented \n\\[\nD=\\left\\{\\begin{array}{ll}\n1,&\\text{}C=b,\\\\\n0,&\\text{}C=.\n\\end{array}\\right.\n\\]\n\\(D\\) now dummy variable: codifies zeros ones two possible levels categorical variable. example \\(C\\) gender, levels male female. dummy variable associated \\(D=0\\) gender male \\(D=1\\) gender female.advantage dummification interpretability regression models. Since level corresponds \\(0\\), can seen reference level level b compared. key point dummification: set one level reference codify rest departures ones. dummification automatically (translates categorical variable \\(C\\) dummy version \\(D\\)) detects factor variable present regression model.Let’s see now case two levels, example, categorical variable \\(C\\) levels , b, c. take reference level, variable can represented two dummy variables:\n\\[\nD_1=\\left\\{\\begin{array}{ll}1,&\\text{}C=b,\\\\0,& \\text{}C\\neq b\\end{array}\\right.\n\\]\n\n\\[\nD_2=\\left\\{\\begin{array}{ll}1,&\\text{}C=c,\\\\0,& \\text{}C\\neq c.\\end{array}\\right.\n\\]\n\\(C=\\) represented \\(D_1=D_2=0\\), \\(C=b\\) represented \\(D_1=1,D_2=0\\) \\(C=c\\) represented \\(D_1=0,D_2=1\\).general, categorical variable \\(J\\) levels, number dummy variables required \\(J-1\\). ,  dummification automatically detects factor variable present regression model.\nmay happen one dummy variable, say \\(D_1\\) significant, dummy\nvariables, say \\(D_2\\), \nsignificant.\n\ncodify categorical variable discrete\nvariable. constitutes major methodological fail \nflaw subsequent statistical analysis.\n\nexample categorical variable party\nlevels partyA, partyB, \npartyC, encode discrete variable taking \nvalues 1, 2, 3, respectively.\n:\n\nassume implicitly order levels party,\nsince partyA closer partyB \npartyC.\n\nassume implicitly partyC three times larger\npartyA.\n\ncodification completely arbitrary – considering\n1, 1.5, 1.75 instead ?\n\nright way dealing categorical variables regression \nset variable factor let \n internally dummification.\n","code":""},{"path":"model-selection.html","id":"model-selection","chapter":"E Model Selection","heading":"E Model Selection","text":"","code":""},{"path":"model-selection.html","id":"linear-model-selection-and-best-subset-selection","chapter":"E Model Selection","heading":"Linear Model Selection and Best Subset Selection","text":"","code":""},{"path":"model-selection.html","id":"forward-stepwise-selection","chapter":"E Model Selection","heading":"Forward Stepwise Selection","text":"","code":""},{"path":"model-selection.html","id":"backward-stepwise-selection","chapter":"E Model Selection","heading":"Backward Stepwise Selection","text":"","code":""},{"path":"model-selection.html","id":"estimating-test-error-using-mallows-cp-aic-bic-adjusted-r-squared","chapter":"E Model Selection","heading":"Estimating Test Error Using Mallow’s Cp, AIC, BIC, Adjusted R-squared","text":"","code":""},{"path":"model-selection.html","id":"estimating-test-error-using-cross-validation","chapter":"E Model Selection","heading":"Estimating Test Error Using Cross-Validation","text":"","code":""},{"path":"model-selection.html","id":"examples","chapter":"E Model Selection","heading":"Examples","text":"","code":""},{"path":"model-selection.html","id":"best-subset-selection","chapter":"E Model Selection","heading":"Best Subset Selection","text":"","code":""},{"path":"model-selection.html","id":"forward-stepwise-selection-and-model-selection-using-validation-set","chapter":"E Model Selection","heading":"Forward Stepwise Selection and Model Selection Using Validation Set","text":"","code":""},{"path":"model-selection.html","id":"model-selection-using-cross-validation","chapter":"E Model Selection","heading":"Model Selection Using Cross-Validation","text":"","code":""},{"path":"references-and-credits.html","id":"references-and-credits","chapter":"F References and Credits","heading":"F References and Credits","text":"references used course:Elements Statistical Learning Trevor Hastie, Robert Tibshirani Jerome H. Friedman, link .Pattern Recognition Machine Learning Christopher Bishop, link .Andrew Ng’s Machine Learning Mooc, link .Lab notes Statistics Social Sciences II: Multivariate Techniques, link .Credits:website written Rmarkdown compiled using bookdown.","code":""},{"path":"other-references.html","id":"other-references","chapter":"G Other References","heading":"G Other References","text":"","code":""},{"path":"main-references-credits.html","id":"main-references-credits","chapter":"Main References & Credits","heading":"Main References & Credits","text":"Readings:TibshiraniAndrew Ng courserahttp://egarpor.github.io/index.htmlhttps://onlinecourses.science.psu.edu/stat505/node/49BishopSite using Rstudio, Rmarkdown, bookdown. Pdf compiled using latex.icons used notes designed madebyoliver, freepik roundicons Flaticon.","code":""}]
