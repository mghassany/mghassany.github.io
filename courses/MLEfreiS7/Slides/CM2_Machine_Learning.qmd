---
title: "Machine Learning"
subtitle: "Lecture 2 : Introduction to Classification, Logistic Regression"
author: "Mohamad GHASSANY"
institute: "EFREI PARIS"
format:
  beamer:
    pdf-engine: pdflatex
    slide-level: 2
    aspectratio: 169
    keep-tex: true
    include-in-header: preface_beamer.tex
classoption: "t, dvipsnames"
fontsize: 9pt
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE,
                      fig.width = 6,
                      fig.asp = 0.8,
                      out.width = "50%",
                      fig.align='center',
                      dev = "tikz",
                      cache = TRUE)
```

```{r, message=FALSE, warning=FALSE}
# libraries 
pkgs <- c("tidyverse",
          # "tidylog",
          "data.table",
          "magrittr",
          "RColorBrewer",
          "ggplot2", 
          "patchwork",
          "cowplot",
          "here",
          "knitr",
          "kableExtra",
          "plot3D",
          "scatterplot3d")

xfun::pkg_attach(pkgs, message = F, install = T)
```


```{r theme_ggplot2, echo = FALSE}
# Creating a ggplot2 theme
theme_mooc <- function(base_size = 14) {
  theme_bw(base_size = base_size) %+replace%
  
    theme(
      # changed theme options
        plot.background = element_rect(fill = "white", colour=NA), 
        # panel.border = element_rect(fill = NA),
        panel.background = element_rect(fill = "white"),
        plot.title = element_text(hjust = 0.5),
        legend.background = element_rect(fill = "white"),
        legend.key =  element_rect(fill = "white"),
        panel.grid = element_line(colour="#ECECED",linetype = "dashed"))
}
# Changing the default theme
theme_set(theme_mooc())
```

# Classification

## Classification examples

- Email: Spam / Not Spam? 
- Online Transactions: Fraudulent (Yes/No)?
- Tumor: Malignant / Benign?
- Loan Demand (Credit Risk): Safe / Risky

\vspace{1cm}

\pause 

:::{.exampleblock data-latex="{Classification: categorical output}"}

- $y \in \{0,1\}$
- 0: "Negative class"
- 1: "Positive Class"
:::

\vspace{1cm}

\pause 

.. and also multiclass classification


## Evaluating Classifiers 

$$\text{Accuracy} = \frac{\text{Number of data points classified correctly}}{\text{all data points}}  $$

:::{.alertblock data-latex="{Confusion Matrix}"}

\vspace{3cm}

:::

.. while in Regression (continuous output): Mean Squared Error (MSE).


# Logistic Regression


## The logistic function (sigmoid)


$$g(z) = \frac{e^z}{1+e^{z}} = \frac{1}{1+e^{-z}}$$

\pause

\vspace{0.5cm}

```{r, out.width = "50%"}
# curve(1/(1+exp(-x)), -8,8 , xlab = "z", ylab= "",
#       lwd = 2 , col = "#CD0050", main = "La fonction logistique",
#       yaxt="n")
# grid()
# axis(2,at=c(0,0.5,1),labels=c(0,0.5,1))

ggplot(data.frame(x=c(-8, 8)), aes(x)) + 
  stat_function(fun = function(x) 1/(1+exp(-x)), geom = "line",lwd=1, color =  "#CD0050") + 
  ylim(-0.1,1.1) +
  scale_y_continuous(breaks=c(0,0.5,1)) +
  xlab("$z$") + 
  ylab("$g(z)$") + 
  geom_abline(slope=0, linetype="dashed") + 
  geom_abline(slope=0, intercept = 1, linetype="dashed") +
  theme_mooc()
```
    


## Logistic Regression: why not linear regression

\begincols
\begincol{.48\textwidth}


```{r, out.width = "80%"}

dd = data.frame(
  taux = seq(27,40),
  defaut = c(0,0,0,0,0,1,0,0,1,1,1,1,1,1)
)

ggplot(dd, aes(taux,defaut, shape=factor(defaut), color=factor(defaut))) +
  scale_color_brewer(palette="Set1") +
  scale_shape_manual(values=c(19, 15)) +
  geom_point(size=3) +
  ylim(-0.25,1.25) +
  theme_mooc() +
  theme(legend.position="none") +
  xlab("Debt ratio") + 
  ylab("Default") 
```

\endcol

\begincol{.48\textwidth}

- $y \in \{0,1\}$:
  + $"0"$: Negative class (here \textcolor{barcolor}{no default})
  + $"1"$: Positive class (here \textcolor{classificationblue}{default})
  
\vspace{0.5cm}

- $f_{\omega} (x) = \omega'x$ can be $>1$ ou $<0$ \alert{!}

\vspace{0.5cm}

- Ideally $0 \leq f_{\omega} (x) \leq 1$ s.t.:
  + If $f_{\omega} (x) \geq 0.5$, predict "$y=1$" 
  + If $f_{\omega} (x) < 0.5$, predict "$y=0$" 

\endcol
\endcols




## Logistic Regression: intuition (1)
\begincols
\begincol{.48\textwidth}

\vspace{1cm}

\begin{itemize}
  \item Let $f_{\omega}(x) =  \textcolor{ForestGreen}{\omega' x}$
\end{itemize}
\endcol

\begincol{.48\textwidth}

```{r, out.width = "80%"}

dd = data.frame(
  taux = seq(27,40),
  defaut = c(0,0,0,0,0,1,0,0,1,1,1,1,1,1)
)

ggplot(dd, aes(taux,defaut)) +
  scale_color_brewer(palette="Set1") +
  scale_shape_manual(values=c(19, 15)) +
  geom_point(size=3, aes(shape=factor(defaut), color=factor(defaut))) +
  ylim(-0.25,1.25) +
  theme_mooc() +
  theme(legend.position="none") +
  xlab("Debt ratio") + 
  ylab("Default") + 
  stat_smooth(method="lm", se=FALSE, lwd = 2 , color = RColorBrewer::brewer.pal(3, "Set1")[3])

# +  stat_function(fun = function(x) 1/(1+exp(-29+0.85*x)), geom = "line",lwd=1, color =  "#CD0050") 
```

\endcol
\endcols

## Logistic Regression: intuition (2)
\begincols
\begincol{.48\textwidth}

\vspace{1cm}

\begin{itemize}
  \item Let $f_{\omega}(x) = \xcancel{\textcolor{ForestGreen}{\omega' x}} =\alert{g(} \textcolor{ForestGreen}{\omega' x} \alert{)} = {\displaystyle \frac{1}{1+e^{- \textcolor{ForestGreen}{\omega' x}}}}$
\end{itemize}
\endcol

\begincol{.48\textwidth}

```{r, out.width = "80%"}

dd = data.frame(
  taux = seq(27,40),
  defaut = c(0,0,0,0,0,1,0,0,1,1,1,1,1,1)
)

# coef = glm(defaut~taux, family="binomial", data=dd)$coefficients

ggplot(dd, aes(taux,defaut)) +
  scale_color_brewer(palette="Set1") +
  scale_shape_manual(values=c(19, 15)) +
  geom_point(size=3, aes(shape=factor(defaut), color=factor(defaut))) +
  ylim(-0.25,1.25) +
  theme_mooc() +
  theme(legend.position="none") +
  xlab("Debt ratio") + 
  ylab("Default") + 
  stat_smooth(method="glm", method.args=list(family="binomial"), se=FALSE, lwd = 2 , color = "#CD0050") + 
  stat_smooth(method="lm", se=FALSE, lwd = 2 , color = RColorBrewer::brewer.pal(3, "Set1")[3])
  # geom_abline(intercept = coef[1], slope=coef[2], lwd = 2 , color = RColorBrewer::brewer.pal(9, "Set1")[9])

# +  stat_function(fun = function(x) 1/(1+exp(-29+0.85*x)), geom = "line",lwd=1, color =  "#CD0050") 
```

\endcol
\endcols

## Logistic Regression: intuition (3)
\begincols
\begincol{.48\textwidth}

\begin{itemize}
  \item $0 \leq \alert{g(\omega' x)} \leq 1$
  
  \item $f_{\omega}(x) = g(\omega' x)$ = estimated probability that $y=1$ on input $x$
  
  \item Probability that $y=1$, given $x$, parameterized by $\omega$
  
  \item $\alert{g(\omega' x)} = \textcolor{classificationblue}{p(y=1\mid x)} = \textcolor{classificationblue}{p(x)}$ 
  
  \item $y \in \{0,1\}$ so $p(y=1\mid x) + p(y=0\mid x)=1$
  
\end{itemize}

\endcol

\begincol{.48\textwidth}

```{r, out.width = "80%"}

dd = data.frame(
  taux = seq(27,40),
  defaut = c(0,0,0,0,0,1,0,0,1,1,1,1,1,1)
)

# coef = glm(defaut~taux, family="binomial", data=dd)$coefficients

ggplot(dd, aes(taux,defaut)) +
  scale_color_brewer(palette="Set1") +
  scale_shape_manual(values=c(19, 15)) +
  geom_point(size=3, aes(shape=factor(defaut), color=factor(defaut))) +
  ylim(-0.25,1.25) +
  theme_mooc() +
  theme(legend.position="none") +
  xlab("Debt ratio") + 
  ylab("Default") + 
  stat_smooth(method="glm", method.args=list(family="binomial"), se=FALSE, lwd = 2 , color = "#CD0050")
  # stat_smooth(method="lm", se=FALSE, lwd = 2 , color = RColorBrewer::brewer.pal(3, "Set1")[3])
  # geom_abline(intercept = coef[1], slope=coef[2], lwd = 2 , color = RColorBrewer::brewer.pal(9, "Set1")[9])

# +  stat_function(fun = function(x) 1/(1+exp(-29+0.85*x)), geom = "line",lwd=1, color =  "#CD0050") 
```

\endcol

\endcols


## Logistic Regression: odds \& log-odds (logit)
<!-- \begincols -->
<!-- \begincol{.3\textwidth} -->

<!-- - un odds càd un rapport de chances Par exemple si un individu présente un odds de 2, cela veut dire qu'il a 2 fois plus de chances d'être positif que d'être négatif -->
<!-- - score logist. entre 0 et 1 , odds entre 0 et inf, logit entre -inf et inf -->
<!-- - si p(x) > 0.5, y = 1  -->
<!-- - si cote > 1 , y = 1 -->
<!-- - si logit > 0, y = 1  -->
<!-- - d'autre fonctions de transformation existent -->

<!-- \endcol -->
<!-- \begincol{.7\textwidth} -->

:::{.block data-latex="{logistic score}"}
$$p(x) = p(y=1\mid x) = {\displaystyle \frac{e^{\omega' x}}{1+e^{\omega' x}}} = {\displaystyle \frac{1}{1+e^{-\omega' x}}}$$
:::

:::{.alertblock data-latex="{odds (côtes)}"}
$${\displaystyle \frac{p(x)}{1-p(x)}} = e^{\omega' x}$$
:::

:::{.exampleblock data-latex="{log-odds or logit (logarithme des côtes)}"}
$$\log \big({\displaystyle \frac{p(x)}{1-p(x)}}\big) = \omega' x$$
:::


<!-- \endcol -->
<!-- \endcols -->


# Logistic Regression: decision boundary

## Logistic Regression: decision boundary
\begincols
\begincol{.48\textwidth}

```{r, out.width = "80%"}

dd = data.frame(
  taux = seq(27,40),
  defaut = c(0,0,0,0,0,1,0,0,1,1,1,1,1,1)
)

coef = glm(defaut~taux, family="binomial", data=dd)$coefficients

ggplot(dd, aes(taux,defaut)) +
  scale_color_brewer(palette="Set1") +
  scale_shape_manual(values=c(19, 15)) +
  geom_point(size=3, aes(shape=factor(defaut), color=factor(defaut))) +
  ylim(-0.25,1.25) +
  theme_mooc() +
  theme(legend.position="none") +
  xlab("Debt ratio") + 
  ylab("Default") + 
  stat_smooth(method="glm", method.args=list(family="binomial"), se=FALSE, lwd = 2 , color = "#CD0050") +
  geom_abline(intercept = coef[1], slope=coef[2], lwd = 2 , color = RColorBrewer::brewer.pal(9, "Set1")[9]) +
  geom_hline(yintercept=0.5, linetype="dashed") + 
  geom_vline(xintercept = -coef[1]/coef[2], linetype="dashed")

# +  stat_function(fun = function(x) 1/(1+exp(-29+0.85*x)), geom = "line",lwd=1, color =  "#CD0050") 
```

\endcol
\begincol{.48\textwidth}

\begin{itemize}
  \item We predict "$\textcolor{classificationblue}{y=1}$" if $p(x) \geq 0.5$ which means $\omega' x \geq 0$
  
  \vspace{1cm}

  \item $\displaystyle \omega_0 + \omega_1 x \geq 0 \Rightarrow  x \geq - \frac{\omega_0}{\omega_1}$
\end{itemize}
\endcol
\endcols

## Logistic Regression: decision boundary (2 features)
\begincols
\begincol{.48\textwidth}

```{r, out.width = "80%"}
set.seed(190414)
dd = caret::twoClassSim(20)

glm.mod = glm(Class~TwoFactor1+TwoFactor2,family="binomial",data=dd[-c(9,20),])
beta0 = glm.mod$coefficients[1] 
beta1 = glm.mod$coefficients[2]
beta2 = glm.mod$coefficients[3]

ggplot(dd[-c(9,20),], aes(TwoFactor1,TwoFactor2, shape=factor(Class), color=factor(Class))) +
  scale_color_brewer(palette="Set1") +
  scale_shape_manual(values=c(19, 15)) +
  geom_point(size=3) +
  theme_mooc() +
  theme(legend.position="none") +
  xlab("$x_1$") + 
  ylab("$x_2$") + 
  geom_abline(slope=-beta1/beta2, intercept=-beta0/beta2, lwd = 2 , color = RColorBrewer::brewer.pal(3, "Set1")[3]) +
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())
```

\endcol
\begincol{.48\textwidth}

- $p(x) = p(y=1\mid x) = f_{\omega}(x) =  g(\omega' x)$
- Predict "$\textcolor{classificationblue}{y=1}$" if $p(x) \geq 0.5$ which means $\omega' x \geq 0$

\vspace{0.5cm}

- $\omega_0 + \omega_1 x_1 + \omega_2 x_2 \geq 0$ So 

\vspace{0.5cm}

$$\textcolor{ForestGreen}{x_2 \geq - \frac{\omega_1}{\omega_2}x_1 - \frac{\omega_0}{\omega_2}}$$


\endcol
\endcols
  

## Non linear decision boundaries

\begincols
\begincol{.48\textwidth}

```{r, out.width = "80%"}
set.seed(190414)
dd = caret::twoClassSim(20)

# glm.mod = glm(Class~TwoFactor1+TwoFactor2,family="binomial",data=dd[-c(9,20),])
# beta0 = glm.mod$coefficients[1] 
# beta1 = glm.mod$coefficients[2]
# beta2 = glm.mod$coefficients[3]

ggplot(dd[-c(9,20),], aes(TwoFactor1,TwoFactor2, shape=factor(Class), color=factor(Class))) +
  # scale_color_brewer(palette="Set1") +
  # scale_shape_manual(values=c(19, 15)) +
  # geom_point(size=3) +
  theme_mooc() +
  theme(legend.position="none") +
  xlab("$x_1$") + 
  ylab("$x_2$") +
  # geom_abline(slope=-beta1/beta2, intercept=-beta0/beta2, lwd = 2 , color = RColorBrewer::brewer.pal(3, "Set1")[3]) +
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())
```

\endcol
\begincol{.48\textwidth}

- Let $f_{\omega}(x) = g(\omega_0 + \omega_1 x_1 + \omega_2 x_2 + \omega_3 x_1^2 + \omega_4 x_2^2)$

\vspace{0.5cm}

- For example, predict "$\textcolor{classificationblue}{y=1}$" if $-1 + x_1^2 + x_2^2 \geq 0$

\vspace{0.5cm}

- Or, $f_{\omega}(x) = g(\omega_0 + \omega_1 x_1 + \omega_2 x_2 + \omega_3 x_1^2 + \omega_4 x_1^2 x_2 + \omega_5 x_1^2 x_2^2 + \ldots)$


\endcol
\endcols

# Logistic Regression: model estimation

## Logistic Regression: model estimation^[check: https://shinyserv.es/shiny/log-maximum-likelihood/, by Eduardo García Portugués]

\begincols
\begincol{.48\textwidth}

- Parameters to estimate: $\omega = \{\omega_0, \omega_1\}$ if univariate

\vspace{0.5cm}

- $\omega = \{\omega_0, \omega_1, \ldots, \omega_p\}$ if multivariate with $p$ features

\vspace{0.5cm}

- How to choose parameters $\omega$?

\endcol

\begincol{.48\textwidth}


```{r, out.width = "80%"}

dd = data.frame(
  taux = seq(27,40),
  defaut = c(0,0,0,0,0,1,0,0,1,1,1,1,1,1)
)

ggplot(dd, aes(taux,defaut)) +
  scale_color_brewer(palette="Set1") +
  scale_shape_manual(values=c(19, 15)) +
  geom_point(size=3, aes(shape=factor(defaut), color=factor(defaut))) +
  ylim(-0.25,1.25) +
  theme_mooc() +
  theme(legend.position="none") +
  xlab("Debt ratio") + 
  ylab("Default") + 
  stat_smooth(method="glm", method.args=list(family="binomial"), se=FALSE, lwd = 2 , color = "#CD0050") + 
  stat_function(fun = function(x) 1/(1+exp(28-0.9096*x)), geom = "line",lwd=1, color =   RColorBrewer::brewer.pal(7, "Set1")[7]) + 
  stat_function(fun = function(x) 1/(1+exp(29-0.9096*x)), geom = "line",lwd=1, color =  RColorBrewer::brewer.pal(7, "Set1")[7]) + 
  stat_function(fun = function(x) 1/(1+exp(31-0.85*x)), geom = "line",lwd=1, color =  RColorBrewer::brewer.pal(7, "Set1")[7]) 

# +  stat_function(fun = function(x) 1/(1+exp(-29+0.85*x)), geom = "line",lwd=1, color =  "#CD0050") 
```


\endcol
\endcols


## Recall the cost function of linear regression

:::{.block data-latex="{Cost function of simple linear regression}"}
- Model: $f_{\omega}(x) = \omega_0 + \omega_1 x = \omega'x$
- Parameters: $\omega_0$ and $\omega_1$
- Cost function: $J(\omega_0,\omega_1) = \frac{1}{n} \sum_{i=1}^{n} \frac{1}{2} \left(f_{\omega}\left(x^{(i)}\right)-y^{(i)}\right)^{2}$
- Goal: $\min_{\omega_0,\omega_1} J(\omega_0,\omega_1)$
:::

\vspace{1cm}

Non-convex in case of logistic regression \alert{!}

## Logistic Regression: how to estimate the parameters


- How to choose parameters $\omega$?
- $y \in \{0,1\}$, Let's assume:
\begin{align*}
  p(y=1 \mid  x,\omega) &= f_{\omega}(x) \\
  p(y=0 \mid  x,\omega) &= 1- f_{\omega}(x)
\end{align*}

\pause

- We represent $y\mid x,\omega \thicksim \mathcal{B}(f_{\omega}(x))$
- We can write: 
$$p(y \mid  x , \omega)=\left(f_{\omega}(x)\right)^{y}\left(1-f_{\omega}(x)\right)^{1-y} \quad \quad y \in \{0,1\}$$

\pause

- Given the $n$ observations and assuming independance, we estimate $\omega$ by maximizing the \alert{likelihood}:

$$\mathcal{L}(\omega) = \prod_{i=1}^{n} p\left(y^{(i)} \mid x^{(i)} , \omega\right)$$




## Logistic Regression: model estimation

\vspace{-0.5cm}



- The \alert{likelihood}:
\begin{align*}
  \mathcal{L}(\omega) &= \prod_{i=1}^{n} p\left(y^{(i)} \mid x^{(i)} , \omega\right) \\
                     &= \prod_{i=1}^{n}\left(f_{\omega}\left(x^{(i)}\right)\right)^{y^{(i)}}\left(1-f_{\omega}\left(x^{(i)}\right)\right)^{1-y^{(i)}}
\end{align*}

\pause

- Maximizing the likelihood is same as maximizing its log:

\begin{align*}
  \ell(\omega) &= \log\left(\mathcal{L}(\omega)\right) \\
                     &= \sum_{i=1}^{n} y^{(i)} \log f_{\omega}\left(x^{(i)}\right)+\left(1-y^{(i)}\right) \log \left(1-f_{\omega}\left(x^{(i)}\right)\right)
\end{align*}

- Maximizing $\ell(\omega)$ is same as minimizing:
$- \frac{1}{n} \ell(\omega)$

\pause

- Let $J(\omega) = - \frac{1}{n} \ell(\omega)$, a \alert{convex cost function} for the logistic regression model (known as *binary cross entropy*).



## Logistic Regression: optimization of the cost function


- **Goal**: Find $\omega$ s.t. $\omega=\argmin_{\omega} J(\omega)$
- $J(\omega) = \displaystyle - \frac{1}{n} \sum_{i=1}^{n} y^{(i)} \log f_{\omega}\left(x^{(i)}\right)+\left(1-y^{(i)}\right) \log \left(1-f_{\omega}\left(x^{(i)}\right)\right)$
- Contrary to the linear regression, this cost function **does not** have an \alert{analytical} solution. We need an optimization technique.

\pause

:::{.alertblock data-latex="{GD for logistic regression}"}
  + `initialize` $\omega$ `randomly``
  + `repeat until convergence`{ 
$$\omega_i^{new} = \omega_i^{old} - \alpha \frac{\partial J(\omega)}{\partial \omega_i} $$
$\quad \quad  \quad \quad \,\,$ `simultaneously for ` $i=0,\ldots,p$
}
:::


## Logistic Regression: optimization of the cost function

- Recall that $\displaystyle g(z) = \frac{e^z}{1+e^{z}} = \frac{1}{1+e^{-z}}$
- Notice that $g'(z)=g(z)(1-g(z))$

\pause

- $\displaystyle \frac{\partial J(\omega)}{\partial \omega_{i}} = (y-f_{\omega}(x))x_{i}$


\pause

:::{.alertblock data-latex="{GD for logistic regression}"}
  + `initialize` $\omega$ `randomly`
  + `repeat until convergence`{ 
$$\omega_i^{new} = \omega_i^{old} - \alpha \frac{1}{n} \sum_{i=1}^{n}\left(f_{\omega}\left(x^{(i)}\right)-y^{(i)}\right).x_i^{(i)}$$
$\quad \quad  \quad \quad \,\,$ `simultaneously for ` $i=0,\ldots,p$
}
:::


## Multi‐class classification: One‐vs-all

\begincols
\begincol{.48\textwidth}

- Weather: Sunny, Cloudy, Rain, Snow
- Medical diagrams: Not ill, Cold, Flu
- News articles: Sport, Education, Technology, Politics

\vspace{0.5cm}



```{r, out.width = "70%"}
# set.seed(190414)
# dd = caret::twoClassSim(20)

x = c(8.86749806734,10.27715188003,0,
8.61369981523,12.03295526581,0,
10.22508427786,12.5488619389,0,
8.76282254632,2.705145352439,1,
10.0161824002,3.64314700879,1,
10.40185355966,1.726518684672,1,
9.5119227667,11.91342902027,0,
9.39967960253,3.012588810034,1,
3.01563665351,4.6777247493,2,
11.88825095823,3.002413919017,1,
11.80002142984,10.64323520941,0,
10.95311372027,10.46897664453,0,
9.10846066345,10.33343995443,0,
11.33975513806,10.98814957185,0,
11.88361084046,11.11589874009,0,
1.33156302345,7.74292832322,2,
2.86095571924,5.9516880527,2,
1.08379634717,4.7704036782,2,
10.03443240516,11.79736738235,0,
2.05332936113,4.9193499125,2,
9.10596354893,4.23514572077,1,
3.39979130387,6.6101016262,2)
dd = as.data.frame(matrix(x, ncol=3, byrow = T))

ggplot(dd, aes(V1,V2, shape=factor(V3), color=factor(V3))) +
  theme_mooc() +
  theme(legend.position="none") +
  scale_color_brewer(palette="Set1") +
  scale_shape_manual(values=c(19, 15, 18)) +
  geom_point(size=4) +
  # ylim(-0.25,1.25) +
  xlab("$x_1$") + 
  ylab("$x_2$")  +
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())
```

\endcol

\begincol{.48\textwidth}

- $f_{\omega}^{(i)}(x) = P(y=1 | x,\omega) \quad \text{ for } i=1,2,3$
- Train a logistic regression classifier for each class $i$ to predict the probability that $y=i$
- On a new input $x$, to make a prediction, pick the class $i$ that maximizes $f_{\omega}^{(i)}(x)$

```{r}
#| layout-nrow: 2
#| out-width: 1in

ggplot(dd, aes(V1,V2, shape=factor(V3), color=factor(V3))) +
  theme_mooc() +
  theme(legend.position="none") +
  scale_color_brewer(palette="Set1") +
  scale_shape_manual(values=c(19, 15, 18)) +
  geom_point(size=4) +
  # ylim(-0.25,1.25) +
  xlab("$x_1$") + 
  ylab("$x_2$")  +
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())

ggplot(dd, aes(V1,V2, shape=factor(V3), color=factor(V3))) +
  theme_mooc() +
  theme(legend.position="none") +
  scale_color_brewer(palette="Set1") +
  scale_shape_manual(values=c(19, 15, 18)) +
  geom_point(size=4) +
  # ylim(-0.25,1.25) +
  xlab("$x_1$") + 
  ylab("$x_2$")  +
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())

ggplot(dd, aes(V1,V2, shape=factor(V3), color=factor(V3))) +
  theme_mooc() +
  theme(legend.position="none") +
  scale_color_brewer(palette="Set1") +
  scale_shape_manual(values=c(19, 15, 18)) +
  geom_point(size=4) +
  # ylim(-0.25,1.25) +
  xlab("$x_1$") + 
  ylab("$x_2$")  +
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())
```

\endcol
\endcols

## Logistic Regression: Remarks \& summary



- Very famous method and maybe the most used
- Adapted for a binary $y$ 
- Relation with linear regression
- Linear decision boundary, but can be non linear using other hypothesis
- Direct calculation of $p(y=1 \mid x)$




