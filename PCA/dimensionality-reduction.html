<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title></title>
  <meta name="description" content="">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  


<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="css\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">ESILV Intoduction to Machine Learning</a></li>

<li class="divider"></li>
<li class="part"><span><b>I Unsupervised Learning</b></span></li>
<li class="chapter" data-level="1" data-path="dimensionalityreduction.html"><a href="#dimensionality-reduction"><i class="fa fa-check"></i><b>1</b> Dimensionality Reduction</a><ul>
<li class="chapter" data-level="1.1" data-path="dimensionalityreduction.html"><a href="#unsupervised-learning"><i class="fa fa-check"></i><b>1.1</b> Unsupervised Learning</a></li>
<li class="chapter" data-level="1.2" data-path="dimensionalityreduction.html"><a href="#principal-components-analysis"><i class="fa fa-check"></i><b>1.2</b> Principal Components Analysis</a></li>
<li class="chapter" data-level="1.3" data-path="dimensionalityreduction.html"><a href="#principal-components"><i class="fa fa-check"></i><b>1.3</b> Principal Components</a><ul>
<li class="chapter" data-level="" data-path=""><a href="#notations-and-procedure"><i class="fa fa-check"></i>Notations and Procedure</a></li>
<li><a href="#first-principal-component-textpc_1-y_1">First Principal Component (<span class="math inline">\(\text{PC}_1\)</span>): <span class="math inline">\(Y_1\)</span></a></li>
<li><a href="#second-principal-component-textpc_2-y_2">Second Principal Component (<span class="math inline">\(\text{PC}_2\)</span>): <span class="math inline">\(Y_2\)</span></a></li>
<li><a href="#ith-principal-component-textpc_i-y_i"><span class="math inline">\(i^{th}\)</span> Principal Component (<span class="math inline">\(\text{PC}_i\)</span>): <span class="math inline">\(Y_i\)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="dimensionalityreduction.html"><a href="#how-do-we-find-the-coefficients"><i class="fa fa-check"></i><b>1.4</b> How do we find the coefficients?</a><ul>
<li class="chapter" data-level="" data-path=""><a href="#why-it-may-be-possible-to-reduce-dimensions"><i class="fa fa-check"></i>Why It May Be Possible to Reduce Dimensions</a></li>
<li class="chapter" data-level="" data-path=""><a href="#procedure"><i class="fa fa-check"></i>Procedure</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="dimensionalityreduction.html"><a href="#standardization-of-the-features"><i class="fa fa-check"></i><b>1.5</b> Standardization of the features</a></li>
<li class="chapter" data-level="1.6" data-path="dimensionalityreduction.html"><a href="#projection-of-the-data"><i class="fa fa-check"></i><b>1.6</b> Projection of the data</a><ul>
<li class="chapter" data-level="" data-path=""><a href="#scores"><i class="fa fa-check"></i>Scores</a></li>
<li class="chapter" data-level="" data-path=""><a href="#visualization"><i class="fa fa-check"></i>Visualization</a></li>
<li class="chapter" data-level="" data-path=""><a href="#extra"><i class="fa fa-check"></i>Extra</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="dimensionalityreduction.html"><a href="#case-study"><i class="fa fa-check"></i><b>1.7</b> Case study</a><ul>
<li class="chapter" data-level="" data-path=""><a href="#employement-in-european-countries-in-the-late-70s"><i class="fa fa-check"></i>Employement in European countries in the late 70s</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<!--bookdown:title:end-->
<!--bookdown:title:start-->
<div id="dimensionality-reduction" class="section level1">
<h1><span class="header-section-number">1</span> Dimensionality Reduction</h1>
<div id="unsupervised-learning" class="section level2">
<h2><span class="header-section-number">1.1</span> Unsupervised Learning</h2>
<p>Previously we considered <em>supervised</em> learning methods such as regression and classification, where we typically have access to a set of <span class="math inline">\(p\)</span> features <span class="math inline">\(X_1,X_2,\ldots,X_p\)</span>, measured on <span class="math inline">\(n\)</span> observations, and a response <span class="math inline">\(Y\)</span> also measured on those same <span class="math inline">\(n\)</span> observations (what we call <strong>labels</strong>). The goal was then to predict <span class="math inline">\(Y\)</span> using <span class="math inline">\(X_1,X_2,\ldots,X_p\)</span>. From now on we will instead focus on <strong>unsupervised</strong> learning, a set of statistical tools where we have only a set of features <span class="math inline">\(X_1,X_2,\ldots,X_p\)</span> measured on <span class="math inline">\(n\)</span> observations. We are not interested in prediction, because we do not have an associated response variable <span class="math inline">\(Y\)</span>. Rather, the goal is to discover interesting things about the measurements on <span class="math inline">\(X_1,X_2,\ldots,X_p\)</span>. Is there an informative way to visualize the data? Can we discover subgroups among the variables or among the observations? Unsupervised learning refers to a diverse set of techniques for answering questions such as these. In this chapter, we will focus on a particular type of unsupervised learning: Principal Components Analysis (PCA), a tool used for <em>data visualization</em> or <em>data pre-processing</em> before supervised techniques are applied. In the next chapters, we will talk about clustering, another particular type of unsupervised learning. Clustering is a broad class of methods for discovering unknown subgroups in data.</p>
<p>Unsupervised learning is often much more challenging than supervised learning. The exercise tends to be more subjective, and there is no simple goal for the analysis, such as prediction of a response. Unsupervised learning is often performed as part of an <em>exploratory data analysis</em>. It is hard to assess the results obtained from unsupervised learning methods. If we fit a predictive model using a supervised learning technique, then it is possible to check our work by seeing how well our model predicts the response <span class="math inline">\(Y\)</span> on observations not used in fitting the model. But in unsupervised learning, there is no way to check our work because we don’t know the true answer: the problem is <em>unsupervised</em>.</p>
</div>
<div id="principal-components-analysis" class="section level2">
<h2><span class="header-section-number">1.2</span> Principal Components Analysis</h2>
<p>The central idea of principal component analysis (PCA) is to reduce the dimensionality of a data set consisting of a large number of interrelated variables, while retaining as much as possible of the <em>variation</em> present in the data set. This is achieved by <em>transforming</em> to a new set of variables, the principal components (PCs), which are uncorrelated, and which are ordered so that the first few retain most of the variation present in all of the original variables.</p>
<p>Suppose that we wish to visualize <span class="math inline">\(n\)</span> observations with measurements on a set of <span class="math inline">\(p\)</span> features, <span class="math inline">\(X_1,X_2,\ldots,X_p\)</span>, as part of an exploratory data analysis. We could do this by examining two-dimensional scatterplots of the data, each of which contains the <span class="math inline">\(n\)</span> observations’ measurements on two of the features. However, there are <span class="math inline">\(C_p^2 = p(p−1)/2\)</span> such scatterplots. For example, with <span class="math inline">\(p =10\)</span> there are 45 plots! If <span class="math inline">\(p\)</span> is large, then it will certainly not be possible to look at all of them; moreover, most likely none of them will be informative since they each contain just a small fraction of the total information present in the data set. Clearly, a better method is required to visualize the <span class="math inline">\(n\)</span> observations when <span class="math inline">\(p\)</span> is large. In particular, we would like to find a low-dimensional representation of the data that captures as much of the information as possible. PCA provides a tool to do just this.</p>
<p>PCA finds a low-dimensional representation of a data set that contains as much as possible of the variation. The idea is that each of the <span class="math inline">\(n\)</span> observations lives in <span class="math inline">\(p\)</span>-dimensional space, but not all of these dimensions are equally interesting. PCA seeks a small number of dimensions that are as interesting as possible, where the concept of interesting is measured by the amount that the observations vary along each dimension. Each of the dimensions found by PCA is a <strong>linear combination of the <span class="math inline">\(p\)</span> features</strong>. We now explain the manner in which these dimensions, or principal components, are found.</p>
</div>
<div id="principal-components" class="section level2">
<h2><span class="header-section-number">1.3</span> Principal Components</h2>
<div id="notations-and-procedure" class="section level3 unnumbered">
<h3>Notations and Procedure</h3>
<p>Suppose that we have a random vector of the features <span class="math inline">\(X\)</span>.</p>
<p><span class="math display">\[ \textbf{X} = \left(\begin{array}{c} X_1\\ X_2\\ \vdots \\X_p\end{array}\right) \]</span></p>
<p>with population variance-covariance matrix</p>
<p><span class="math display">\[ \text{var}(\textbf{X}) = \Sigma = \left(\begin{array}{cccc}\sigma^2_1 &amp; \sigma_{12} &amp; \dots &amp;\sigma_{1p}\\ \sigma_{21} &amp; \sigma^2_2 &amp; \dots &amp;\sigma_{2p}\\  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \sigma_{p1} &amp; \sigma_{p2} &amp; \dots &amp; \sigma^2_p\end{array}\right) \]</span></p>
<p>Consider the linear combinations</p>
<p><span class="math display">\[ \begin{array}{lll} Y_1 &amp; = &amp; a_{11}X_1 + a_{12}X_2 + \dots + a_{1p}X_p \\ Y_2 &amp; = &amp; a_{21}X_1 + a_{22}X_2 + \dots + a_{2p}X_p \\ &amp; &amp; \vdots \\ Y_p &amp; = &amp; a_{p1}X_1 + a_{p2}X_2 + \dots +a_{pp}X_p \end{array} \]</span></p>
<p>Note that <span class="math inline">\(Y_i\)</span> is a function of our random data, and so is also random. Therefore it has a population variance</p>
<p><span class="math display">\[ \text{var}(Y_i) = \sum_{k=1}^{p} \sum_{l=1}^{p} a_{ik} a_{il} \sigma_{kl} = \mathbf{a}^T_i \Sigma \mathbf{a}_i \]</span></p>
<p>Moreover, <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(Y_j\)</span> will have a population covariance</p>
<p><span class="math display">\[ \text{cov}(Y_i, Y_j) = \sum_{k=1}^{p} \sum_{l=1}^{p} a_{ik}a_{jl}\sigma_{kl} = \mathbf{a}^T_i\Sigma\mathbf{a}_j \]</span></p>
<p>and a correlation</p>
<p><span class="math display">\[ \text{cor}(Y_i, Y_j) = \frac{\text{cov}(Y_i, Y_j)}{\sigma^2_i \sigma^2_j}\]</span></p>
<p>Here the coefficients <span class="math inline">\(a_{ij}\)</span> are collected into the vector</p>
<p><span class="math display">\[ \mathbf{a}_i = \left(\begin{array}{c} a_{i1}\\ a_{i2}\\ \vdots \\ a_{ip}\end{array}\right) \]</span></p>
<p>The coefficients <span class="math inline">\(a_{ij}\)</span> are also called <em>loadings</em> of the principal component <span class="math inline">\(i\)</span> and <span class="math inline">\(\mathbf{a}_i\)</span> is a principal component loading vector.</p>
<div class="rmdtip">
<ul>
<li>
The total variation of <span class="math inline"><em>X</em></span> is the <em>trace</em> of the variance-covariance matrix <span class="math inline"><em>Σ</em></span>.
</li>
<li>
The trace of <span class="math inline"><em>Σ</em></span> is the sum of the variances of the individual variables.
</li>
<li>
<span class="math inline"><em>t</em><em>r</em><em>a</em><em>c</em><em>e</em>(<em>Σ</em>)  =  <em>σ</em><sub>1</sub><sup>2</sup> + <em>σ</em><sub>2</sub><sup>2</sup> + … + <em>σ</em><sub><em>p</em></sub><sup>2</sup></span>
</li>
</ul>
</div>
</div>
<div id="first-principal-component-textpc_1-y_1" class="section level3 unnumbered">
<h3>First Principal Component (<span class="math inline">\(\text{PC}_1\)</span>): <span class="math inline">\(Y_1\)</span></h3>
<p>The <em>first principal component</em> is the <em>normalized</em> linear combination of the features <span class="math inline">\(X_1,X_2,\ldots,X_p\)</span> that has maximum variance (among all linear combinations), so it accounts for as much variation in the data as possible.</p>
<p>Specifically we will define coefficients <span class="math inline">\(a_{11},a_{12},\ldots,a_{1p}\)</span> for that component in such a way that its variance is maximized, subject to the constraint that the sum of the squared coefficients is equal to one (that is what we mean by <em>normalized</em>). This constraint is required so that a unique answer may be obtained.</p>
<p>More formally, select <span class="math inline">\(a_{11},a_{12},\ldots,a_{1p}\)</span> that maximizes</p>
<p><span class="math display">\[ \text{var}(Y_1) = \mathbf{a}^T_1\Sigma\mathbf{a}_1  = \sum_{k=1}^{p}\sum_{l=1}^{p}a_{1k}a_{1l}\sigma_{kl} \]</span></p>
<p>subject to the constraint that</p>
<p><span class="math display">\[ \sum_{j=1}^{p}a^2_{1j} = \mathbf{a}^T_1\mathbf{a}_1   = 1 \]</span></p>
</div>
<div id="second-principal-component-textpc_2-y_2" class="section level3 unnumbered">
<h3>Second Principal Component (<span class="math inline">\(\text{PC}_2\)</span>): <span class="math inline">\(Y_2\)</span></h3>
<p>The <em>second principal component</em> is the linear combination of the features <span class="math inline">\(X_1,X_2,\ldots,X_p\)</span> that accounts for as much of the remaining variation as possible, with the constraint that the correlation between the first and second component is 0. So the second principal component has maximal variance out of all linear combinations that are uncorrelated with <span class="math inline">\(Y_1\)</span>.</p>
<p>To compute the coefficients of the second principal component, we select <span class="math inline">\(a_{21},a_{22},\ldots,a_{2p}\)</span> that maximizes the variance of this new component</p>
<p><span class="math display">\[\text{var}(Y_2) = \sum_{k=1}^{p}\sum_{l=1}^{p}a_{2k}a_{2l}\sigma_{kl} = \mathbf{a}^T_2\Sigma\mathbf{a}_2 \]</span></p>
<p>subject to:</p>
<ul>
<li><p>The constraint that the sums of squared coefficients add up to one, <span class="math inline">\(\sum_{j=1}^{p}a^2_{2j} = \mathbf{a}^T_2\mathbf{a}_2 = 1\)</span>.</p></li>
<li><p>Along with the additional constraint that these two components will be uncorrelated with one another:</p></li>
</ul>
<p><span class="math display">\[ \text{cov}(Y_1, Y_2) = \mathbf{a}^T_1\Sigma\mathbf{a}_2  = \sum_{k=1}^{p}\sum_{l=1}^{p}a_{1k}a_{2l}\sigma_{kl} = 0 \]</span></p>
<p>All subsequent principal components have this same property: they are linear combinations that account for as much of the remaining variation as possible and they are not correlated with the other principal components.</p>
<p>We will do this in the same way with each additional component. For instance:</p>
</div>
<div id="ith-principal-component-textpc_i-y_i" class="section level3 unnumbered">
<h3><span class="math inline">\(i^{th}\)</span> Principal Component (<span class="math inline">\(\text{PC}_i\)</span>): <span class="math inline">\(Y_i\)</span></h3>
<p>We select <span class="math inline">\(a_{i1},a_{i2},\ldots,a_{ip}\)</span> that maximizes</p>
<p><span class="math display">\[ \text{var}(Y_i) = \sum_{k=1}^{p}\sum_{l=1}^{p}a_{ik}a_{il}\sigma_{kl} = \mathbf{a}^T_i\Sigma\mathbf{a}_i \]</span></p>
<p>subject to the constraint that the sums of squared coefficients add up to one, along with the additional constraint that this new component will be uncorrelated with all the previously defined components:</p>
<p><span class="math display">\[ \sum_{j=1}^{p}a^2_{ij} \mathbf{a}^T_i\mathbf{a}_i = \mathbf{a}^T_i\mathbf{a}_i = 1\]</span></p>
<p><span class="math display">\[ \text{cov}(Y_1, Y_i) = \sum_{k=1}^{p}\sum_{l=1}^{p}a_{1k}a_{il}\sigma_{kl} = \mathbf{a}^T_1\Sigma\mathbf{a}_i = 0 \]</span></p>
<p><span class="math display">\[\text{cov}(Y_2, Y_i) = \sum_{k=1}^{p}\sum_{l=1}^{p}a_{2k}a_{il}\sigma_{kl} = \mathbf{a}^T_2\Sigma\mathbf{a}_i = 0\]</span> <span class="math display">\[\vdots\]</span> <span class="math display">\[\text{cov}(Y_{i-1}, Y_i) = \sum_{k=1}^{p}\sum_{l=1}^{p}a_{i-1,k}a_{il}\sigma_{kl} = \mathbf{a}^T_{i-1}\Sigma\mathbf{a}_i = 0\]</span></p>
<p>Therefore all principal components are uncorrelated with one another.</p>
</div>
</div>
<div id="how-do-we-find-the-coefficients" class="section level2">
<h2><span class="header-section-number">1.4</span> How do we find the coefficients?</h2>
<p>How do we find the coefficients <span class="math inline">\(a_{ij}\)</span> for a principal component? The solution involves the <strong>eigenvalues</strong> and <strong>eigenvectors</strong> of the variance-covariance matrix <span class="math inline">\(\Sigma\)</span>.</p>
<p>Let <span class="math inline">\(\lambda_1,\ldots,\lambda_p\)</span> denote the eigenvalues of the variance-covariance matrix <span class="math inline">\(\Sigma\)</span>. These are ordered so that <span class="math inline">\(\lambda_1\)</span> has the largest eigenvalue and <span class="math inline">\(\lambda_p\)</span> is the smallest.</p>
<p><span class="math display">\[ \lambda_1 \ge \lambda_2 \ge \dots \ge \lambda_p \]</span></p>
<p>We are also going to let the vectors <span class="math inline">\(\mathbf{a}_1, \ldots,\mathbf{a}_p\)</span> denote the corresponding eigenvectors.</p>
<p>It turns out that the elements for these eigenvectors will be the coefficients of the principal components.</p>
<div class="rmdinsight">
<p>
The elements for the eigenvectors of <span class="math inline"><em>Σ</em></span> are the coefficients of the principal components.
</p>
</div>
<p>The variance for the <span class="math inline">\(i\)</span>th principal component is equal to the <span class="math inline">\(i\)</span>th eigenvalue.</p>
<p><span class="math display">\[ \textbf{var}(Y_i) = \text{var}(a_{i1}X_1 + a_{i2}X_2 + \dots a_{ip}X_p) = \lambda_i \]</span></p>
<p>Moreover, the principal components are uncorrelated with one another.</p>
<p><span class="math display">\[\text{cov}(Y_i, Y_j) = 0\]</span></p>
<p>The variance-covariance matrix may be written as a function of the eigenvalues and their corresponding eigenvectors. In fact, the variance-covariance matrix can be written as the sum over the <span class="math inline">\(p\)</span> eigenvalues, multiplied by the product of the corresponding eigenvector times its transpose as shown in the following expression</p>
<p><span class="math display">\[ \Sigma  =  \sum_{i=1}^{p}\lambda_i \mathbf{a}_i \mathbf{a}_i^T \]</span></p>
<p>If <span class="math inline">\(\lambda_{k+1}, \lambda_{k+2}, \dots , \lambda_{p}\)</span> are small, we might approximate <span class="math inline">\(\Sigma\)</span> by</p>
<p><span class="math display">\[ \Sigma  \cong  \sum_{i=1}^{k}\lambda_i \mathbf{a}_i\mathbf{a}_i^T \]</span></p>
<p>Earlier in the chapter we defined the total variation of <span class="math inline">\(X\)</span> as the trace of the variance-covariance matrix. This is also equal to the sum of the eigenvalues as shown below:</p>
<p><span class="math display">\[ \begin{array}{lll}trace(\Sigma) &amp; = &amp; \sigma^2_1 + \sigma^2_2 + \dots +\sigma^2_p \\ &amp; = &amp; \lambda_1 + \lambda_2 + \dots + \lambda_p\end{array} \]</span></p>
<p>This will give us an interpretation of the components in terms of the amount of the full variation explained by each component. The proportion of variation explained by the <span class="math inline">\(i\)</span>th principal component is then going to be defined to be the eigenvalue for that component divided by the sum of the eigenvalues. In other words, the <span class="math inline">\(i\)</span>th principal component explains the following proportion of the total variation:</p>
<p><span class="math display">\[ \frac{\lambda_i}{\lambda_1 + \lambda_2 + \dots + \lambda_p} \]</span></p>
<p>A related quantity is the proportion of variation explained by the first <span class="math inline">\(k\)</span> principal component. This would be the sum of the first <span class="math inline">\(k\)</span> eigenvalues divided by its total variation.</p>
<p><span class="math display">\[ \frac{\lambda_1 + \lambda_2 + \dots + \lambda_k}{\lambda_1 + \lambda_2 + \dots + \lambda_p} \]</span></p>
<p>In practice, these proportions are often expressed as percentages.</p>
<p>Naturally, if the proportion of variation explained by the first <span class="math inline">\(k\)</span> principal components is large, then not much information is lost by considering only the first <span class="math inline">\(k\)</span> principal components.</p>
<div id="why-it-may-be-possible-to-reduce-dimensions" class="section level3 unnumbered">
<h3>Why It May Be Possible to Reduce Dimensions</h3>
<p>When we have correlations (multicollinarity) between the features, the data may more or less fall on a line or plane in a lower number of dimensions. For instance, imagine a plot of two features that have a nearly perfect correlation. The data points will fall close to a straight line. That line could be used as a new (one-dimensional) axis to represent the variation among data points.</p>
<div class="rmdcaution">
<p>
All of this is defined in terms of the population variance-covariance matrix <span class="math inline"><em>Σ</em></span> which is <em>unknown</em>. However, we may estimate <span class="math inline"><em>Σ</em></span> by the sample variance-covariance matrix which is given in the standard formula here:
</p>
<p>
<br /><span class="math display"><span class="math display">\[ \textbf{S} = \frac{1}{n-1} \sum_{i=1}^{n}(\mathbf{X}_i-\bar{\textbf{x}})(\mathbf{X}_i-\bar{\textbf{x}})^T \]</span></span><br />
</p>
</div>
</div>
<div id="procedure" class="section level3 unnumbered">
<h3>Procedure</h3>
<p>Compute the eigenvalues <span class="math inline">\(\hat{\lambda}_1, \hat{\lambda}_2, \dots, \hat{\lambda}_p\)</span> of the sample variance-covariance matrix <span class="math inline">\(\textbf{S}\)</span>, and the corresponding eigenvectors <span class="math inline">\(\hat{\mathbf{a}}_1, \hat{\mathbf{a}}_2, \dots, \hat{\mathbf{a}}_p\)</span>.</p>
<p>Then we will define our estimated principal components using the eigenvectors as our coefficients:</p>
<p><span class="math display">\[ \begin{array}{lll} \hat{Y}_1 &amp; = &amp; \hat{a}_{11}X_1 + \hat{a}_{12}X_2 + \dots + \hat{a}_{1p}X_p \\ \hat{Y}_2 &amp; = &amp; \hat{a}_{21}X_1 + \hat{a}_{22}X_2 + \dots + \hat{a}_{2p}X_p \\&amp;&amp;\vdots\\ \hat{Y}_p &amp; = &amp; \hat{a}_{p1}X_1 + \hat{a}_{p2}X_2 + \dots + \hat{a}_{pp}X_p \\ \end{array} \]</span></p>
<p>Generally, we only retain the first <span class="math inline">\(k\)</span> principal component. There are a number of criteria that may be used to decide how many components should be retained:</p>
<ol style="list-style-type: decimal">
<li><p>To obtain the simplest possible interpretation, we want <span class="math inline">\(k\)</span> to be as small as possible. If we can explain most of the variation just by <strong>two</strong> principal components then this would give us a much simpler description of the data.</p></li>
<li><p>Retain the first <span class="math inline">\(k\)</span> components which explain a “large” proportion of the total variation, say <span class="math inline">\(70-80\%\)</span>.</p></li>
<li><p>Examine a scree plot. This is a plot of the eigenvalues versus the component number. The idea is to look for the “elbow” which corresponds to the point after which the eigenvalues decrease more slowly. Adding components after this point explains relatively little more of the variance. See the next figure for an example of a scree plot.</p></li>
</ol>
<center>
<figure>
<img src="img/screeplot.png" alt="Scree plot showing eigenvalue by number of principal component." style="width: 400px;"/>
<figcaption>
Scree plot showing eigenvalue by number of principal component.
</figcaption>
</figure>
</center>
</div>
</div>
<div id="standardization-of-the-features" class="section level2">
<h2><span class="header-section-number">1.5</span> Standardization of the features</h2>
<p>If we use the raw data, the principal component analysis will tend to give more emphasis to the variables that have higher variances than to those variables that have very low variances.</p>
<p>In effect the results of the analysis will depend on what units of measurement are used to measure each variable. That would imply that a principal component analysis should only be used with the raw data if all variables have the same units of measure. And even in this case, only if you wish to give those variables which have higher variances more weight in the analysis.</p>
<div class="rmdcaution">
<ul>
<li>
The results of principal component analysis depend on the scales at which the variables are measured.
</li>
<li>
Variables with the highest sample variances will tend to be emphasized in the first few principal components.
</li>
<li>
Principal component analysis using the covariance function should only be considered if all of the variables have the same units of measurement.
</li>
</ul>
</div>
<p>If the variables either have different units of measurement, or if we wish each variable to receive equal weight in the analysis, then the variables should be <strong>standardized</strong> (<em>scaled</em>) before a principal components analysis is carried out. Standardize the variables by subtracting its mean from that variable and dividing it by its standard deviation:</p>
<p><span class="math display">\[Z_{ij} = \frac{X_{ij}-\bar{x}_j}{\sigma_j}\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(X_{ij}\)</span> = Data for variable <span class="math inline">\(j\)</span> in sample unit <span class="math inline">\(i\)</span></li>
<li><span class="math inline">\(\bar{x}_j\)</span> = Sample mean for variable <span class="math inline">\(j\)</span></li>
<li><span class="math inline">\(\sigma_j\)</span> = Sample standard deviation for variable <span class="math inline">\(j\)</span></li>
</ul>
<p><strong>Note</strong>: <span class="math inline">\(Z_j\)</span> has mean = 0 and variance = 1.</p>
<div class="rmdinsight">
<p>
The variance-covariance matrix of the standardized data is equal to the correlation matrix for the unstandardized data. Therefore, principal component analysis using the standardized data is equivalent to principal component analysis using the correlation matrix.
</p>
</div>
</div>
<div id="projection-of-the-data" class="section level2">
<h2><span class="header-section-number">1.6</span> Projection of the data</h2>
<div id="scores" class="section level3 unnumbered">
<h3>Scores</h3>
<p>Using the coefficients (loadings) of every principal component, we can project the observations on the axis of the principal component, those projections are called <em>scores</em>. For example, the scores of the first principal component are</p>
<p><span class="math display">\[  \forall 1 \le i \le n \quad \hat{Y}_1^i  =  \hat{a}_{11}X_1^i + \hat{a}_{12}X_2^i + \dots + \hat{a}_{1p}X_p^i \]</span></p>
<p>(<span class="math inline">\(X_1^i\)</span> is the value of feature <span class="math inline">\(1\)</span> for the observation <span class="math inline">\(i\)</span>)</p>
<p>This can be written for all observations and all the principal components using the matrix formulation</p>
<p><span class="math display">\[ \mathbf{\hat{Y}} = \mathbf{\hat{A}} X\]</span></p>
<p>where <span class="math inline">\(\mathbf{\hat{A}}\)</span> is the matrix of the coefficients <span class="math inline">\(\hat{a}_{ij}\)</span>.</p>
</div>
<div id="visualization" class="section level3 unnumbered">
<h3>Visualization</h3>
<p>Once we have computed the principal components, we can plot them against each other in order to produce low-dimensional views of the data.</p>
<p>We can plot the score vector <span class="math inline">\(Y_1\)</span> against <span class="math inline">\(Y_2\)</span>, <span class="math inline">\(Y_1\)</span> against <span class="math inline">\(Y_3\)</span>, <span class="math inline">\(Y_2\)</span> against <span class="math inline">\(Y_3\)</span>, and so forth. Geometrically, this amounts to projecting the original data down onto the subspace spanned by <span class="math inline">\(\mathbf{a}_1\)</span>, <span class="math inline">\(\mathbf{a}_2\)</span>, and <span class="math inline">\(\mathbf{a}_3\)</span>, and plotting the projected points.</p>
<p>To interpret the results obtained by PCA, we plot on the same figure both the principal component scores and the loading vectors. This figure is called a <em>biplot</em>. An example is given later in this chapter.</p>
</div>
<div id="extra" class="section level3 unnumbered">
<h3>Extra</h3>
<div class="rmdinsight">
<ul>
<li>
<p>
You can read this tutorial <a target="_blank" href="files/PrincipalComponents.pdf"><i class="fa fa-file-pdf-o" aria-hidden="true"></i></a>. In the document, there is an introduction about the mathemtical concepts used in PCA. Plus a detailed example of PCA.
</p>
</li>
<li>
<p>
You can watch these videos for a nice explanation of PCA <a target="_blank"  href="https://fr.coursera.org/learn/machine-learning/lecture/GBFTt/principal-component-analysis-problem-formulation"><i class="fa fa-video-camera" aria-hidden="true"></i> 1</a> <a target="_blank" href="https://fr.coursera.org/learn/machine-learning/lecture/ZYIPa/principal-component-analysis-algorithm"><i class="fa fa-video-camera" aria-hidden="true"></i> 2</a>.
</p>
</li>
</ul>
</div>
</div>
</div>
<div id="case-study" class="section level2">
<h2><span class="header-section-number">1.7</span> Case study</h2>
<div id="employement-in-european-countries-in-the-late-70s" class="section level3 unnumbered">
<h3>Employement in European countries in the late 70s</h3>
<p>The purpose of this case study is to reveal the structure of the job market and economy in different developed countries. The final aim is to have a meaningful and rigorous plot that is able to show the most important features of the countries in a concise form.</p>
<p>The dataset <code>eurojob</code> (<a href="datasets/eurojob.txt">download</a>) contains the data employed in this case study. It contains the percentage of workforce employed in 1979 in 9 industries for 26 European countries. The industries measured are:</p>
<ul>
<li>Agriculture (<code>Agr</code>)</li>
<li>Mining (<code>Min</code>)</li>
<li>Manufacturing (<code>Man</code>)</li>
<li>Power supply industries <code>(Pow</code>)</li>
<li>Construction (<code>Con</code>)</li>
<li>Service industries (<code>Ser</code>)</li>
<li>Finance (<code>Fin</code>)</li>
<li>Social and personal services (<code>Soc</code>)</li>
<li>Transport and communications (<code>Tra</code>)</li>
</ul>
<p>If the dataset is imported into <code>R</code> and the case names are set as <code>Country</code> (important in order to have only numerical variables), then the data should look like this:</p>
<table>
<caption><span id="tab:eurotable">Table 1.1: </span>The <code>eurojob</code> dataset.</caption>
<thead>
<tr class="header">
<th align="left">Country</th>
<th align="right">Agr</th>
<th align="right">Min</th>
<th align="right">Man</th>
<th align="right">Pow</th>
<th align="right">Con</th>
<th align="right">Ser</th>
<th align="right">Fin</th>
<th align="right">Soc</th>
<th align="right">Tra</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Belgium</td>
<td align="right">3.3</td>
<td align="right">0.9</td>
<td align="right">27.6</td>
<td align="right">0.9</td>
<td align="right">8.2</td>
<td align="right">19.1</td>
<td align="right">6.2</td>
<td align="right">26.6</td>
<td align="right">7.2</td>
</tr>
<tr class="even">
<td align="left">Denmark</td>
<td align="right">9.2</td>
<td align="right">0.1</td>
<td align="right">21.8</td>
<td align="right">0.6</td>
<td align="right">8.3</td>
<td align="right">14.6</td>
<td align="right">6.5</td>
<td align="right">32.2</td>
<td align="right">7.1</td>
</tr>
<tr class="odd">
<td align="left">France</td>
<td align="right">10.8</td>
<td align="right">0.8</td>
<td align="right">27.5</td>
<td align="right">0.9</td>
<td align="right">8.9</td>
<td align="right">16.8</td>
<td align="right">6.0</td>
<td align="right">22.6</td>
<td align="right">5.7</td>
</tr>
<tr class="even">
<td align="left">WGerm</td>
<td align="right">6.7</td>
<td align="right">1.3</td>
<td align="right">35.8</td>
<td align="right">0.9</td>
<td align="right">7.3</td>
<td align="right">14.4</td>
<td align="right">5.0</td>
<td align="right">22.3</td>
<td align="right">6.1</td>
</tr>
<tr class="odd">
<td align="left">Ireland</td>
<td align="right">23.2</td>
<td align="right">1.0</td>
<td align="right">20.7</td>
<td align="right">1.3</td>
<td align="right">7.5</td>
<td align="right">16.8</td>
<td align="right">2.8</td>
<td align="right">20.8</td>
<td align="right">6.1</td>
</tr>
<tr class="even">
<td align="left">Italy</td>
<td align="right">15.9</td>
<td align="right">0.6</td>
<td align="right">27.6</td>
<td align="right">0.5</td>
<td align="right">10.0</td>
<td align="right">18.1</td>
<td align="right">1.6</td>
<td align="right">20.1</td>
<td align="right">5.7</td>
</tr>
<tr class="odd">
<td align="left">Luxem</td>
<td align="right">7.7</td>
<td align="right">3.1</td>
<td align="right">30.8</td>
<td align="right">0.8</td>
<td align="right">9.2</td>
<td align="right">18.5</td>
<td align="right">4.6</td>
<td align="right">19.2</td>
<td align="right">6.2</td>
</tr>
<tr class="even">
<td align="left">Nether</td>
<td align="right">6.3</td>
<td align="right">0.1</td>
<td align="right">22.5</td>
<td align="right">1.0</td>
<td align="right">9.9</td>
<td align="right">18.0</td>
<td align="right">6.8</td>
<td align="right">28.5</td>
<td align="right">6.8</td>
</tr>
<tr class="odd">
<td align="left">UK</td>
<td align="right">2.7</td>
<td align="right">1.4</td>
<td align="right">30.2</td>
<td align="right">1.4</td>
<td align="right">6.9</td>
<td align="right">16.9</td>
<td align="right">5.7</td>
<td align="right">28.3</td>
<td align="right">6.4</td>
</tr>
<tr class="even">
<td align="left">Austria</td>
<td align="right">12.7</td>
<td align="right">1.1</td>
<td align="right">30.2</td>
<td align="right">1.4</td>
<td align="right">9.0</td>
<td align="right">16.8</td>
<td align="right">4.9</td>
<td align="right">16.8</td>
<td align="right">7.0</td>
</tr>
<tr class="odd">
<td align="left">Finland</td>
<td align="right">13.0</td>
<td align="right">0.4</td>
<td align="right">25.9</td>
<td align="right">1.3</td>
<td align="right">7.4</td>
<td align="right">14.7</td>
<td align="right">5.5</td>
<td align="right">24.3</td>
<td align="right">7.6</td>
</tr>
<tr class="even">
<td align="left">Greece</td>
<td align="right">41.4</td>
<td align="right">0.6</td>
<td align="right">17.6</td>
<td align="right">0.6</td>
<td align="right">8.1</td>
<td align="right">11.5</td>
<td align="right">2.4</td>
<td align="right">11.0</td>
<td align="right">6.7</td>
</tr>
<tr class="odd">
<td align="left">Norway</td>
<td align="right">9.0</td>
<td align="right">0.5</td>
<td align="right">22.4</td>
<td align="right">0.8</td>
<td align="right">8.6</td>
<td align="right">16.9</td>
<td align="right">4.7</td>
<td align="right">27.6</td>
<td align="right">9.4</td>
</tr>
<tr class="even">
<td align="left">Portugal</td>
<td align="right">27.8</td>
<td align="right">0.3</td>
<td align="right">24.5</td>
<td align="right">0.6</td>
<td align="right">8.4</td>
<td align="right">13.3</td>
<td align="right">2.7</td>
<td align="right">16.7</td>
<td align="right">5.7</td>
</tr>
<tr class="odd">
<td align="left">Spain</td>
<td align="right">22.9</td>
<td align="right">0.8</td>
<td align="right">28.5</td>
<td align="right">0.7</td>
<td align="right">11.5</td>
<td align="right">9.7</td>
<td align="right">8.5</td>
<td align="right">11.8</td>
<td align="right">5.5</td>
</tr>
<tr class="even">
<td align="left">Sweden</td>
<td align="right">6.1</td>
<td align="right">0.4</td>
<td align="right">25.9</td>
<td align="right">0.8</td>
<td align="right">7.2</td>
<td align="right">14.4</td>
<td align="right">6.0</td>
<td align="right">32.4</td>
<td align="right">6.8</td>
</tr>
<tr class="odd">
<td align="left">Switz</td>
<td align="right">7.7</td>
<td align="right">0.2</td>
<td align="right">37.8</td>
<td align="right">0.8</td>
<td align="right">9.5</td>
<td align="right">17.5</td>
<td align="right">5.3</td>
<td align="right">15.4</td>
<td align="right">5.7</td>
</tr>
<tr class="even">
<td align="left">Turkey</td>
<td align="right">66.8</td>
<td align="right">0.7</td>
<td align="right">7.9</td>
<td align="right">0.1</td>
<td align="right">2.8</td>
<td align="right">5.2</td>
<td align="right">1.1</td>
<td align="right">11.9</td>
<td align="right">3.2</td>
</tr>
<tr class="odd">
<td align="left">Bulgaria</td>
<td align="right">23.6</td>
<td align="right">1.9</td>
<td align="right">32.3</td>
<td align="right">0.6</td>
<td align="right">7.9</td>
<td align="right">8.0</td>
<td align="right">0.7</td>
<td align="right">18.2</td>
<td align="right">6.7</td>
</tr>
<tr class="even">
<td align="left">Czech</td>
<td align="right">16.5</td>
<td align="right">2.9</td>
<td align="right">35.5</td>
<td align="right">1.2</td>
<td align="right">8.7</td>
<td align="right">9.2</td>
<td align="right">0.9</td>
<td align="right">17.9</td>
<td align="right">7.0</td>
</tr>
<tr class="odd">
<td align="left">EGerm</td>
<td align="right">4.2</td>
<td align="right">2.9</td>
<td align="right">41.2</td>
<td align="right">1.3</td>
<td align="right">7.6</td>
<td align="right">11.2</td>
<td align="right">1.2</td>
<td align="right">22.1</td>
<td align="right">8.4</td>
</tr>
<tr class="even">
<td align="left">Hungary</td>
<td align="right">21.7</td>
<td align="right">3.1</td>
<td align="right">29.6</td>
<td align="right">1.9</td>
<td align="right">8.2</td>
<td align="right">9.4</td>
<td align="right">0.9</td>
<td align="right">17.2</td>
<td align="right">8.0</td>
</tr>
<tr class="odd">
<td align="left">Poland</td>
<td align="right">31.1</td>
<td align="right">2.5</td>
<td align="right">25.7</td>
<td align="right">0.9</td>
<td align="right">8.4</td>
<td align="right">7.5</td>
<td align="right">0.9</td>
<td align="right">16.1</td>
<td align="right">6.9</td>
</tr>
<tr class="even">
<td align="left">Romania</td>
<td align="right">34.7</td>
<td align="right">2.1</td>
<td align="right">30.1</td>
<td align="right">0.6</td>
<td align="right">8.7</td>
<td align="right">5.9</td>
<td align="right">1.3</td>
<td align="right">11.7</td>
<td align="right">5.0</td>
</tr>
<tr class="odd">
<td align="left">USSR</td>
<td align="right">23.7</td>
<td align="right">1.4</td>
<td align="right">25.8</td>
<td align="right">0.6</td>
<td align="right">9.2</td>
<td align="right">6.1</td>
<td align="right">0.5</td>
<td align="right">23.6</td>
<td align="right">9.3</td>
</tr>
<tr class="even">
<td align="left">Yugoslavia</td>
<td align="right">48.7</td>
<td align="right">1.5</td>
<td align="right">16.8</td>
<td align="right">1.1</td>
<td align="right">4.9</td>
<td align="right">6.4</td>
<td align="right">11.3</td>
<td align="right">5.3</td>
<td align="right">4.0</td>
</tr>
</tbody>
</table>
<p><strong>Note</strong>: To set the case names as <code>Country</code>, we do</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">row.names</span>(eurojob) &lt;-<span class="st"> </span>eurojob<span class="op">$</span>Country
eurojob<span class="op">$</span>Country &lt;-<span class="st"> </span><span class="ot">NULL</span></code></pre></div>
<p>So far, we know how to compute summaries for <em>each variable</em>, and how to quantify and visualize relations between variables with the correlation matrix and the scatterplot matrix. But even for a moderate number of variables like this, their results are hard to process.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Summary of the data - marginal</span>
<span class="kw">summary</span>(eurojob)
<span class="co">#ans&gt;       Agr            Min             Man            Pow       </span>
<span class="co">#ans&gt;  Min.   : 2.7   Min.   :0.100   Min.   : 7.9   Min.   :0.100  </span>
<span class="co">#ans&gt;  1st Qu.: 7.7   1st Qu.:0.525   1st Qu.:23.0   1st Qu.:0.600  </span>
<span class="co">#ans&gt;  Median :14.4   Median :0.950   Median :27.6   Median :0.850  </span>
<span class="co">#ans&gt;  Mean   :19.1   Mean   :1.254   Mean   :27.0   Mean   :0.908  </span>
<span class="co">#ans&gt;  3rd Qu.:23.7   3rd Qu.:1.800   3rd Qu.:30.2   3rd Qu.:1.175  </span>
<span class="co">#ans&gt;  Max.   :66.8   Max.   :3.100   Max.   :41.2   Max.   :1.900  </span>
<span class="co">#ans&gt;       Con             Ser             Fin             Soc      </span>
<span class="co">#ans&gt;  Min.   : 2.80   Min.   : 5.20   Min.   : 0.50   Min.   : 5.3  </span>
<span class="co">#ans&gt;  1st Qu.: 7.53   1st Qu.: 9.25   1st Qu.: 1.23   1st Qu.:16.2  </span>
<span class="co">#ans&gt;  Median : 8.35   Median :14.40   Median : 4.65   Median :19.6  </span>
<span class="co">#ans&gt;  Mean   : 8.17   Mean   :12.96   Mean   : 4.00   Mean   :20.0  </span>
<span class="co">#ans&gt;  3rd Qu.: 8.97   3rd Qu.:16.88   3rd Qu.: 5.92   3rd Qu.:24.1  </span>
<span class="co">#ans&gt;  Max.   :11.50   Max.   :19.10   Max.   :11.30   Max.   :32.4  </span>
<span class="co">#ans&gt;       Tra      </span>
<span class="co">#ans&gt;  Min.   :3.20  </span>
<span class="co">#ans&gt;  1st Qu.:5.70  </span>
<span class="co">#ans&gt;  Median :6.70  </span>
<span class="co">#ans&gt;  Mean   :6.55  </span>
<span class="co">#ans&gt;  3rd Qu.:7.08  </span>
<span class="co">#ans&gt;  Max.   :9.40</span>

<span class="co"># Correlation matrix</span>
<span class="kw">cor</span>(eurojob)
<span class="co">#ans&gt;         Agr     Min    Man     Pow     Con    Ser     Fin    Soc    Tra</span>
<span class="co">#ans&gt; Agr  1.0000  0.0358 -0.671 -0.4001 -0.5383 -0.737 -0.2198 -0.747 -0.565</span>
<span class="co">#ans&gt; Min  0.0358  1.0000  0.445  0.4055 -0.0256 -0.397 -0.4427 -0.281  0.157</span>
<span class="co">#ans&gt; Man -0.6711  0.4452  1.000  0.3853  0.4945  0.204 -0.1558  0.154  0.351</span>
<span class="co">#ans&gt; Pow -0.4001  0.4055  0.385  1.0000  0.0599  0.202  0.1099  0.132  0.375</span>
<span class="co">#ans&gt; Con -0.5383 -0.0256  0.494  0.0599  1.0000  0.356  0.0163  0.158  0.388</span>
<span class="co">#ans&gt; Ser -0.7370 -0.3966  0.204  0.2019  0.3560  1.000  0.3656  0.572  0.188</span>
<span class="co">#ans&gt; Fin -0.2198 -0.4427 -0.156  0.1099  0.0163  0.366  1.0000  0.108 -0.246</span>
<span class="co">#ans&gt; Soc -0.7468 -0.2810  0.154  0.1324  0.1582  0.572  0.1076  1.000  0.568</span>
<span class="co">#ans&gt; Tra -0.5649  0.1566  0.351  0.3752  0.3877  0.188 -0.2459  0.568  1.000</span>

<span class="co"># Scatterplot matrix</span>
<span class="kw">library</span>(car)
<span class="co">#ans&gt; Warning: package &#39;car&#39; was built under R version 3.4.3</span>
<span class="kw">scatterplotMatrix</span>(eurojob, <span class="dt">reg.line =</span> lm, <span class="dt">smooth =</span> <span class="ot">FALSE</span>, <span class="dt">spread =</span> <span class="ot">FALSE</span>,
                  <span class="dt">span =</span> <span class="fl">0.5</span>, <span class="dt">ellipse =</span> <span class="ot">FALSE</span>, <span class="dt">levels =</span> <span class="kw">c</span>(.<span class="dv">5</span>, .<span class="dv">9</span>), <span class="dt">id.n =</span> <span class="dv">0</span>,
                  <span class="dt">diagonal =</span> <span class="st">&#39;histogram&#39;</span>)</code></pre></div>
<p><img src="Machine-Learning_files/figure-html/unnamed-chunk-9-1.png" width="70%" style="display: block; margin: auto;" /> We definitely need a way of visualizing and quantifying the relations between variables for a moderate to large amount of variables. PCA will be a handy way. Recall what PCA does:</p>
<ol style="list-style-type: decimal">
<li>Takes the data for the variables <span class="math inline">\(X_1,\ldots,X_p\)</span>.</li>
<li>Using this data, looks for new variables <span class="math inline">\(\text{PC}_1,\ldots \text{PC}_p\)</span> such that:
<ul>
<li><span class="math inline">\(\text{PC}_j\)</span> is a <strong>linear combination</strong> of <span class="math inline">\(X_1,\ldots,X_k\)</span>, <span class="math inline">\(1\leq j\leq p\)</span>. This is, <span class="math inline">\(\text{PC}_j=a_{1j}X_1+a_{2j}X_2+\ldots+a_{pj}X_p\)</span>.</li>
<li><span class="math inline">\(\text{PC}_1,\ldots \text{PC}_p\)</span> are <strong>sorted decreasingly in terms of variance</strong>. Hence <span class="math inline">\(\text{PC}_j\)</span> has more variance than <span class="math inline">\(\text{PC}_{j+1}\)</span>, <span class="math inline">\(1\leq j\leq p-1\)</span>,</li>
<li><span class="math inline">\(\text{PC}_{j_1}\)</span> and <span class="math inline">\(\text{PC}_{j_2}\)</span> are <strong>uncorrelated</strong>, for <span class="math inline">\(j_1\neq j_2\)</span>.</li>
<li><span class="math inline">\(\text{PC}_1,\ldots \text{PC}_p\)</span> have the <strong>same information</strong>, measured in terms of <strong>total variance</strong>, as <span class="math inline">\(X_1,\ldots,X_p\)</span>.</li>
</ul></li>
<li>Produces three key objects:
<ul>
<li><strong>Variances of the PCs</strong>. They are sorted decreasingly and give an idea of which PCs are contain most of the information of the data (the ones with more variance).</li>
<li><strong>Weights of the variables in the PCs</strong>. They give the interpretation of the PCs in terms of the original variables, as they are the coefficients of the linear combination. The weights of the variables <span class="math inline">\(X_1,\ldots,X_p\)</span> on the PC<span class="math inline">\(_j\)</span>, <span class="math inline">\(a_{1j},\ldots,a_{pj}\)</span>, are normalized: <span class="math inline">\(a_{1j}^2+\ldots+a_{pj}^2=1\)</span>, <span class="math inline">\(j=1,\ldots,p\)</span>. In <code>R</code>, they are called <code>loadings</code>.</li>
<li><strong>Scores of the data in the PCs</strong>: this is the data with <span class="math inline">\(\text{PC}_1,\ldots \text{PC}_p\)</span> variables instead of <span class="math inline">\(X_1,\ldots,X_p\)</span>. The <strong>scores are uncorrelated</strong>. Useful for knowing which PCs have more effect on a certain observation.</li>
</ul></li>
</ol>
<p>Hence, PCA rearranges our variables in an information-equivalent, but more convenient, layout where the variables are <strong>sorted according to the ammount of information they are able to explain</strong>. From this position, the next step is clear: <strong>stick only with a limited number of PCs such that they explain most of the information</strong> (e.g., 70% of the total variance) and do <em>dimension reduction</em>. The effectiveness of PCA in practice varies from the structure present in the dataset. For example, in the case of highly dependent data, it could explain more than the 90% of variability of a dataset with tens of variables with just two PCs.</p>
<p>Let’s see how to compute a full PCA in <code>R</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># The main function - use cor = TRUE to avoid scale distortions</span>
pca &lt;-<span class="st"> </span><span class="kw">princomp</span>(eurojob, <span class="dt">cor =</span> <span class="ot">TRUE</span>)

<span class="co"># What is inside?</span>
<span class="kw">str</span>(pca)
<span class="co">#ans&gt; List of 7</span>
<span class="co">#ans&gt;  $ sdev    : Named num [1:9] 1.867 1.46 1.048 0.997 0.737 ...</span>
<span class="co">#ans&gt;   ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Comp.1&quot; &quot;Comp.2&quot; &quot;Comp.3&quot; &quot;Comp.4&quot; ...</span>
<span class="co">#ans&gt;  $ loadings: loadings [1:9, 1:9] -0.52379 -0.00132 0.3475 0.25572 0.32518 ...</span>
<span class="co">#ans&gt;   ..- attr(*, &quot;dimnames&quot;)=List of 2</span>
<span class="co">#ans&gt;   .. ..$ : chr [1:9] &quot;Agr&quot; &quot;Min&quot; &quot;Man&quot; &quot;Pow&quot; ...</span>
<span class="co">#ans&gt;   .. ..$ : chr [1:9] &quot;Comp.1&quot; &quot;Comp.2&quot; &quot;Comp.3&quot; &quot;Comp.4&quot; ...</span>
<span class="co">#ans&gt;  $ center  : Named num [1:9] 19.131 1.254 27.008 0.908 8.165 ...</span>
<span class="co">#ans&gt;   ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Agr&quot; &quot;Min&quot; &quot;Man&quot; &quot;Pow&quot; ...</span>
<span class="co">#ans&gt;  $ scale   : Named num [1:9] 15.245 0.951 6.872 0.369 1.614 ...</span>
<span class="co">#ans&gt;   ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Agr&quot; &quot;Min&quot; &quot;Man&quot; &quot;Pow&quot; ...</span>
<span class="co">#ans&gt;  $ n.obs   : int 26</span>
<span class="co">#ans&gt;  $ scores  : num [1:26, 1:9] 1.71 0.953 0.755 0.853 -0.104 ...</span>
<span class="co">#ans&gt;   ..- attr(*, &quot;dimnames&quot;)=List of 2</span>
<span class="co">#ans&gt;   .. ..$ : chr [1:26] &quot;Belgium&quot; &quot;Denmark&quot; &quot;France&quot; &quot;WGerm&quot; ...</span>
<span class="co">#ans&gt;   .. ..$ : chr [1:9] &quot;Comp.1&quot; &quot;Comp.2&quot; &quot;Comp.3&quot; &quot;Comp.4&quot; ...</span>
<span class="co">#ans&gt;  $ call    : language princomp(x = eurojob, cor = TRUE)</span>
<span class="co">#ans&gt;  - attr(*, &quot;class&quot;)= chr &quot;princomp&quot;</span>

<span class="co"># The standard deviation of each PC</span>
pca<span class="op">$</span>sdev
<span class="co">#ans&gt;  Comp.1  Comp.2  Comp.3  Comp.4  Comp.5  Comp.6  Comp.7  Comp.8  Comp.9 </span>
<span class="co">#ans&gt; 1.86739 1.45951 1.04831 0.99724 0.73703 0.61922 0.47514 0.36985 0.00675</span>

<span class="co"># Weights: the expression of the original variables in the PCs</span>
<span class="co"># E.g. Agr = -0.524 * PC1 + 0.213 * PC5 - 0.152 * PC6 + 0.806 * PC9</span>
<span class="co"># And also: PC1 = -0.524 * Agr + 0.347 * Man + 0256 * Pow + 0.325 * Con + ...</span>
<span class="co"># (Because the matrix is orthogonal, so the transpose is the inverse)</span>
pca<span class="op">$</span>loadings
<span class="co">#ans&gt; </span>
<span class="co">#ans&gt; Loadings:</span>
<span class="co">#ans&gt;     Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 Comp.9</span>
<span class="co">#ans&gt; Agr -0.524                       0.213 -0.153                0.806</span>
<span class="co">#ans&gt; Min        -0.618 -0.201        -0.164  0.101  0.726              </span>
<span class="co">#ans&gt; Man  0.347 -0.355 -0.150  0.346 -0.385  0.288 -0.479  0.126  0.366</span>
<span class="co">#ans&gt; Pow  0.256 -0.261 -0.561 -0.393  0.295 -0.357 -0.256 -0.341       </span>
<span class="co">#ans&gt; Con  0.325         0.153  0.668  0.472 -0.130  0.221 -0.356       </span>
<span class="co">#ans&gt; Ser  0.379  0.350 -0.115        -0.284 -0.615  0.229  0.388  0.238</span>
<span class="co">#ans&gt; Fin         0.454 -0.587         0.280  0.526  0.187  0.174  0.145</span>
<span class="co">#ans&gt; Soc  0.387  0.222  0.312 -0.412 -0.220  0.263  0.191 -0.506  0.351</span>
<span class="co">#ans&gt; Tra  0.367 -0.203  0.375 -0.314  0.513  0.124         0.545       </span>
<span class="co">#ans&gt; </span>
<span class="co">#ans&gt;                Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8</span>
<span class="co">#ans&gt; SS loadings     1.000  1.000  1.000  1.000  1.000  1.000  1.000  1.000</span>
<span class="co">#ans&gt; Proportion Var  0.111  0.111  0.111  0.111  0.111  0.111  0.111  0.111</span>
<span class="co">#ans&gt; Cumulative Var  0.111  0.222  0.333  0.444  0.556  0.667  0.778  0.889</span>
<span class="co">#ans&gt;                Comp.9</span>
<span class="co">#ans&gt; SS loadings     1.000</span>
<span class="co">#ans&gt; Proportion Var  0.111</span>
<span class="co">#ans&gt; Cumulative Var  1.000</span>

<span class="co"># Scores of the data on the PCs: how is the data reexpressed into PCs</span>
<span class="kw">head</span>(pca<span class="op">$</span>scores, <span class="dv">10</span>)
<span class="co">#ans&gt;         Comp.1  Comp.2  Comp.3  Comp.4  Comp.5  Comp.6  Comp.7 Comp.8</span>
<span class="co">#ans&gt; Belgium  1.710  1.2218 -0.1148 -0.3395 -0.3245  0.0473  0.3401  0.403</span>
<span class="co">#ans&gt; Denmark  0.953  2.1278  0.9507 -0.5939  0.1027  0.8273  0.3029 -0.352</span>
<span class="co">#ans&gt; France   0.755  1.1212 -0.4980  0.5003 -0.2997 -0.1158  0.1855 -0.266</span>
<span class="co">#ans&gt; WGerm    0.853  0.0114 -0.5795  0.1105 -1.1652  0.6181 -0.4446  0.194</span>
<span class="co">#ans&gt; Ireland -0.104  0.4140 -0.3840 -0.9267  0.0152 -1.4242  0.0370 -0.334</span>
<span class="co">#ans&gt; Italy    0.375  0.7695  1.0606  1.4772 -0.6452 -1.0021  0.1418 -0.130</span>
<span class="co">#ans&gt; Luxem    1.059 -0.7558 -0.6515  0.8352 -0.8659 -0.2188  1.6942  0.547</span>
<span class="co">#ans&gt; Nether   1.688  2.0048  0.0637  0.0235  0.6352 -0.2120  0.3034 -0.591</span>
<span class="co">#ans&gt; UK       1.630  0.3731 -1.1409 -1.2669 -0.8129  0.0361 -0.0413 -0.349</span>
<span class="co">#ans&gt; Austria  1.176 -0.1431 -1.0434  0.1577  0.5210 -0.8019 -0.4150  0.215</span>
<span class="co">#ans&gt;            Comp.9</span>
<span class="co">#ans&gt; Belgium -0.001090</span>
<span class="co">#ans&gt; Denmark  0.015619</span>
<span class="co">#ans&gt; France  -0.000507</span>
<span class="co">#ans&gt; WGerm   -0.006539</span>
<span class="co">#ans&gt; Ireland  0.010879</span>
<span class="co">#ans&gt; Italy    0.005602</span>
<span class="co">#ans&gt; Luxem    0.003453</span>
<span class="co">#ans&gt; Nether  -0.010931</span>
<span class="co">#ans&gt; UK      -0.005478</span>
<span class="co">#ans&gt; Austria -0.002816</span>

<span class="co"># Scatterplot matrix of the scores - they are uncorrelated!</span>
<span class="kw">scatterplotMatrix</span>(pca<span class="op">$</span>scores, <span class="dt">reg.line =</span> lm, <span class="dt">smooth =</span> <span class="ot">FALSE</span>, <span class="dt">spread =</span> <span class="ot">FALSE</span>,
                  <span class="dt">span =</span> <span class="fl">0.5</span>, <span class="dt">ellipse =</span> <span class="ot">FALSE</span>, <span class="dt">levels =</span> <span class="kw">c</span>(.<span class="dv">5</span>, .<span class="dv">9</span>), <span class="dt">id.n =</span> <span class="dv">0</span>,
                  <span class="dt">diagonal =</span> <span class="st">&#39;histogram&#39;</span>)</code></pre></div>
<p><img src="Machine-Learning_files/figure-html/unnamed-chunk-10-1.png" width="70%" style="display: block; margin: auto;" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">
<span class="co"># Means of the variables - before PCA the variables are centered</span>
pca<span class="op">$</span>center
<span class="co">#ans&gt;    Agr    Min    Man    Pow    Con    Ser    Fin    Soc    Tra </span>
<span class="co">#ans&gt; 19.131  1.254 27.008  0.908  8.165 12.958  4.000 20.023  6.546</span>

<span class="co"># Rescalation done to each variable</span>
<span class="co"># - if cor = FALSE (default), a vector of ones</span>
<span class="co"># - if cor = TRUE, a vector with the standard deviations of the variables</span>
pca<span class="op">$</span>scale
<span class="co">#ans&gt;    Agr    Min    Man    Pow    Con    Ser    Fin    Soc    Tra </span>
<span class="co">#ans&gt; 15.245  0.951  6.872  0.369  1.614  4.486  2.752  6.697  1.364</span>

<span class="co"># Summary of the importance of components - the third row is key</span>
<span class="kw">summary</span>(pca)
<span class="co">#ans&gt; Importance of components:</span>
<span class="co">#ans&gt;                        Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7</span>
<span class="co">#ans&gt; Standard deviation      1.867  1.460  1.048  0.997 0.7370 0.6192 0.4751</span>
<span class="co">#ans&gt; Proportion of Variance  0.387  0.237  0.122  0.110 0.0604 0.0426 0.0251</span>
<span class="co">#ans&gt; Cumulative Proportion   0.387  0.624  0.746  0.857 0.9171 0.9597 0.9848</span>
<span class="co">#ans&gt;                        Comp.8   Comp.9</span>
<span class="co">#ans&gt; Standard deviation     0.3699 6.75e-03</span>
<span class="co">#ans&gt; Proportion of Variance 0.0152 5.07e-06</span>
<span class="co">#ans&gt; Cumulative Proportion  1.0000 1.00e+00</span>

<span class="co"># Scree plot - the variance of each component</span>
<span class="kw">plot</span>(pca)</code></pre></div>
<p><img src="Machine-Learning_files/figure-html/unnamed-chunk-10-2.png" width="70%" style="display: block; margin: auto;" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">
<span class="co"># With connected lines - useful for looking for the &quot;elbow&quot;</span>
<span class="kw">plot</span>(pca, <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>)</code></pre></div>
<p><img src="Machine-Learning_files/figure-html/unnamed-chunk-10-3.png" width="70%" style="display: block; margin: auto;" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">
<span class="co"># PC1 and PC2</span>
pca<span class="op">$</span>loadings[, <span class="dv">1</span><span class="op">:</span><span class="dv">2</span>]
<span class="co">#ans&gt;       Comp.1  Comp.2</span>
<span class="co">#ans&gt; Agr -0.52379 -0.0536</span>
<span class="co">#ans&gt; Min -0.00132 -0.6178</span>
<span class="co">#ans&gt; Man  0.34750 -0.3551</span>
<span class="co">#ans&gt; Pow  0.25572 -0.2611</span>
<span class="co">#ans&gt; Con  0.32518 -0.0513</span>
<span class="co">#ans&gt; Ser  0.37892  0.3502</span>
<span class="co">#ans&gt; Fin  0.07437  0.4537</span>
<span class="co">#ans&gt; Soc  0.38741  0.2215</span>
<span class="co">#ans&gt; Tra  0.36682 -0.2026</span></code></pre></div>
<div class="rmdinsight">
<p>
PCA produces <strong>uncorrelated</strong> variables from the original set <span class="math inline"><em>X</em><sub>1</sub>, …, <em>X</em><sub><em>p</em></sub></span>. This implies that:
</p>
<ul>
<li>
The PCs are uncorrelated, <strong>but not independent</strong> (uncorrelated does not imply independent).
</li>
<li>
An uncorrelated or independent variable in <span class="math inline"><em>X</em><sub>1</sub>, …, <em>X</em><sub><em>p</em></sub></span> will get a PC only associated to it. In the extreme case where all the <span class="math inline"><em>X</em><sub>1</sub>, …, <em>X</em><sub><em>p</em></sub></span> are uncorrelated, these coincide with the PCs (up to sign flips).
</li>
</ul>
</div>
<p>Based on the weights of the variables on the PCs, we can extract the following interpretation:</p>
<ul>
<li>PC1 is roughly a linear combination of <code>Agr</code>, with <em>negative</em> weight, and (<code>Man</code>, <code>Pow</code>, <code>Con</code>, <code>Ser</code>, <code>Soc</code>, <code>Tra</code>), with <em>positive</em> weights. So it can be interpreted as an <em>indicator</em> of the kind of economy of the country: agricultural (negative values) or industrial (positive values).</li>
<li>PC2 has <em>negative</em> weights on (<code>Min</code>, <code>Man</code>, <code>Pow</code>, <code>Tra</code>) and <em>positive</em> weights in (<code>Ser</code>, <code>Fin</code>, <code>Soc</code>). It can be interpreted as the contrast between relatively large or small service sectors. So it tends to be negative in communist countries and positive in capitalist countries.</li>
</ul>
<div class="rmdtip">
<p>
The interpretation of the PCs involves inspecting the weights and interpreting the linear combination of the original variables, which might be separating between two clear characteristics of the data
</p>
</div>
<p>To conclude, let’s see how we can represent our original data into a plot called <em>biplot</em> that summarizes all the analysis for two PCs.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Biplot - plot together the scores for PC1 and PC2 and the</span>
<span class="co"># variables expressed in terms of PC1 and PC2</span>
<span class="kw">biplot</span>(pca)</code></pre></div>
<p><img src="Machine-Learning_files/figure-html/unnamed-chunk-13-1.png" width="90%" style="display: block; margin: auto;" /></p>
<p align="right">
◼
</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>


    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["Machine-Learning.pdf"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
