<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>6 Principal Components Analysis | Machine Learning</title>
<meta name="author" content="Mohamad Ghassany">
<meta name="description" content="6.1 Introduction The central idea of principal component analysis (PCA) is to reduce the dimensionality of a data set consisting of a large number of interrelated variables, while retaining as...">
<meta name="generator" content="bookdown 0.24.10 with bs4_book()">
<meta property="og:title" content="6 Principal Components Analysis | Machine Learning">
<meta property="og:type" content="book">
<meta property="og:description" content="6.1 Introduction The central idea of principal component analysis (PCA) is to reduce the dimensionality of a data set consisting of a large number of interrelated variables, while retaining as...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="6 Principal Components Analysis | Machine Learning">
<meta name="twitter:description" content="6.1 Introduction The central idea of principal component analysis (PCA) is to reduce the dimensionality of a data set consisting of a large number of interrelated variables, while retaining as...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<link href="libs/vembedr-0.1.5/css/vembedr.css" rel="stylesheet">
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script><script src="libs/plotly-binding-4.10.0/plotly.js"></script><script src="libs/typedarray-0.1/typedarray.min.js"></script><link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet">
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script><link href="libs/plotly-htmlwidgets-css-2.5.1/plotly-htmlwidgets.css" rel="stylesheet">
<script src="libs/plotly-main-2.5.1/plotly-latest.min.js"></script><script type="text/x-mathjax-config">
    const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
    for (let popover of popovers){
      const div = document.createElement('div');
      div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
      div.innerHTML = popover.getAttribute('data-content');
      
      // Will this work with TeX on its own line?
      var has_math = div.querySelector("span.math");
      if (has_math) {
        document.body.appendChild(div);
      	MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
      	MathJax.Hub.Queue(function(){
          popover.setAttribute('data-content', div.innerHTML);
      	})
      }
    }
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="css/ims-style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Machine Learning</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="introduction.html">Introduction</a></li>
<li class="book-part">Supervised Learning</li>
<li class="book-part">Regression</li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">1</span> Linear Regression</a></li>
<li><a class="" href="practical-work-1.html">Practical Work 1</a></li>
<li><a class="" href="multiple-linear-regression.html"><span class="header-section-number">2</span> Multiple Linear Regression</a></li>
<li><a class="" href="pw-2.html">PW 2</a></li>
<li class="book-part">Classification</li>
<li><a class="" href="logistic-regression.html"><span class="header-section-number">3</span> Logistic Regression</a></li>
<li><a class="" href="pw-3.html">PW 3</a></li>
<li><a class="" href="discriminant-analysis.html"><span class="header-section-number">4</span> Discriminant Analysis</a></li>
<li><a class="" href="pw-4.html">PW 4</a></li>
<li><a class="" href="decision-trees-random-forests.html"><span class="header-section-number">5</span> Decision Trees &amp; Random Forests</a></li>
<li><a class="" href="pw-5.html">PW 5</a></li>
<li class="book-part">Dimensionality Reduction</li>
<li><a class="active" href="principal-components-analysis.html"><span class="header-section-number">6</span> Principal Components Analysis</a></li>
<li><a class="" href="pw-6.html">PW 6</a></li>
<li class="book-part">Unsupervised Learning</li>
<li><a class="" href="kmeans-hierarchical-clustering.html"><span class="header-section-number">7</span> Kmeans &amp; Hierarchical Clustering</a></li>
<li><a class="" href="pw-7.html">PW 7</a></li>
<li><a class="" href="gaussian-mixture-models-em.html"><span class="header-section-number">8</span> Gaussian Mixture Models &amp; EM</a></li>
<li><a class="" href="pw-8.html">PW 8</a></li>
<li class="book-part">Hackathon</li>
<li><a class="" href="hackathon.html">Hackathon</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="final-grades.html"><span class="header-section-number">A</span> Final Grades</a></li>
<li><a class="" href="app-introRStudio.html"><span class="header-section-number">B</span> Introduction to RStudio</a></li>
<li><a class="" href="app-ht.html"><span class="header-section-number">C</span> Review on hypothesis testing</a></li>
<li><a class="" href="use-qual.html"><span class="header-section-number">D</span> Use of qualitative predictors</a></li>
<li><a class="" href="model-selection.html"><span class="header-section-number">E</span> Model Selection</a></li>
<li><a class="" href="references-and-credits.html"><span class="header-section-number">F</span> References and Credits</a></li>
<li><a class="" href="other-references.html"><span class="header-section-number">G</span> Other References</a></li>
<li><a class="" href="main-references-credits.html">Main References &amp; Credits</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="principal-components-analysis" class="section level1" number="6">
<h1>
<span class="header-section-number">6</span> Principal Components Analysis<a class="anchor" aria-label="anchor" href="#principal-components-analysis"><i class="fas fa-link"></i></a>
</h1>
<div id="introduction-3" class="section level2" number="6.1">
<h2>
<span class="header-section-number">6.1</span> Introduction<a class="anchor" aria-label="anchor" href="#introduction-3"><i class="fas fa-link"></i></a>
</h2>
<p>The central idea of principal component analysis (PCA) is to reduce the dimensionality of a data set consisting of a large number of interrelated variables, while retaining as much as possible of the <strong><em>variation</em></strong> present in the data set. This is achieved by <em>transforming</em> to a new set of variables, the principal components (PCs), which are uncorrelated, and which are ordered so that the first few retain most of the variation present in all of the original variables.</p>
<p>Suppose that we wish to visualize <span class="math inline">\(n\)</span> observations with measurements on a set of <span class="math inline">\(p\)</span> features, <span class="math inline">\(X_1,X_2,\ldots,X_p\)</span>, as part of an exploratory data analysis. We could do this by examining two-dimensional scatterplots of the data, each of which contains the <span class="math inline">\(n\)</span> observations’ measurements on two of the features. However, there are <span class="math inline">\(C_p^2 = p(p−1)/2\)</span> such scatterplots. For example, with <span class="math inline">\(p =10\)</span> there are 45 plots! If <span class="math inline">\(p\)</span> is large, then it will certainly not be possible to look at all of them; moreover, most likely none of them will be informative since they each contain just a small fraction of the total information present in the data set. Clearly, a better method is required to visualize the <span class="math inline">\(n\)</span> observations when <span class="math inline">\(p\)</span> is large. In particular, we would like to find a low-dimensional representation of the data that captures as much of the information as possible. PCA provides a tool to do just this.</p>
<p>PCA finds a low-dimensional representation of a data set that contains as much as possible of the variation. The idea is that each of the <span class="math inline">\(n\)</span> observations lives in <span class="math inline">\(p\)</span>-dimensional space, but not all of these dimensions are equally interesting. PCA seeks a small number of dimensions that are as interesting as possible, where the concept of interesting is measured by the amount that the observations vary along each dimension. Each of the dimensions found by PCA is a <strong>linear combination of the <span class="math inline">\(p\)</span> features</strong>. We now explain the manner in which these dimensions, or principal components, are found.</p>
</div>
<div id="principal-components" class="section level2" number="6.2">
<h2>
<span class="header-section-number">6.2</span> Principal Components<a class="anchor" aria-label="anchor" href="#principal-components"><i class="fas fa-link"></i></a>
</h2>
<div id="notations-and-procedure" class="section level3 unnumbered">
<h3>Notations and Procedure<a class="anchor" aria-label="anchor" href="#notations-and-procedure"><i class="fas fa-link"></i></a>
</h3>
<p>Suppose that we have a random vector of the features <span class="math inline">\(X\)</span>.</p>
<p><span class="math display">\[ \textbf{X} = \left(\begin{array}{c} X_1\\ X_2\\ \vdots \\X_p\end{array}\right) \]</span></p>
<p>with population variance-covariance matrix</p>
<p><span class="math display">\[ \text{var}(\textbf{X}) = \Sigma = \left(\begin{array}{cccc}\sigma^2_1 &amp; \sigma_{12} &amp; \dots &amp;\sigma_{1p}\\ \sigma_{21} &amp; \sigma^2_2 &amp; \dots &amp;\sigma_{2p}\\  \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \sigma_{p1} &amp; \sigma_{p2} &amp; \dots &amp; \sigma^2_p\end{array}\right) \]</span></p>
<p>Consider the linear combinations</p>
<p><span class="math display">\[ \begin{array}{lll} Y_1 &amp; = &amp; a_{11}X_1 + a_{12}X_2 + \dots + a_{1p}X_p \\ Y_2 &amp; = &amp; a_{21}X_1 + a_{22}X_2 + \dots + a_{2p}X_p \\ &amp; &amp; \vdots \\ Y_p &amp; = &amp; a_{p1}X_1 + a_{p2}X_2 + \dots +a_{pp}X_p \end{array} \]</span></p>
<p>Note that <span class="math inline">\(Y_i\)</span> is a function of our random data, and so is also random. Therefore it has a population variance</p>
<p><span class="math display">\[ \text{var}(Y_i) = \sum_{k=1}^{p} \sum_{l=1}^{p} a_{ik} a_{il} \sigma_{kl} = \mathbf{a}^T_i \Sigma \mathbf{a}_i \]</span></p>
<p>Moreover, <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(Y_j\)</span> will have a population covariance</p>
<p><span class="math display">\[ \text{cov}(Y_i, Y_j) = \sum_{k=1}^{p} \sum_{l=1}^{p} a_{ik}a_{jl}\sigma_{kl} = \mathbf{a}^T_i\Sigma\mathbf{a}_j \]</span></p>
<p>and a correlation</p>
<p><span class="math display">\[ \text{cor}(Y_i, Y_j) = \frac{\text{cov}(Y_i, Y_j)}{\sigma^2_i \sigma^2_j}\]</span></p>
<p>Here the coefficients <span class="math inline">\(a_{ij}\)</span> are collected into the vector</p>
<p><span class="math display">\[ \mathbf{a}_i = \left(\begin{array}{c} a_{i1}\\ a_{i2}\\ \vdots \\ a_{ip}\end{array}\right) \]</span></p>
<p>The coefficients <span class="math inline">\(a_{ij}\)</span> are also called <em>loadings</em> of the principal component <span class="math inline">\(i\)</span> and <span class="math inline">\(\mathbf{a}_i\)</span> is a principal component loading vector.</p>
<div class="rmdtip">
<ul>
<li>
The total variation of <span class="math inline"><span class="math inline">\(X\)</span></span> is the
<em>trace</em> of the variance-covariance matrix <span class="math inline"><span class="math inline">\(\Sigma\)</span></span>.
</li>
<li>
The trace of <span class="math inline"><span class="math inline">\(\Sigma\)</span></span> is the sum
of the variances of the individual variables.
</li>
<li>
<span class="math inline"><span class="math inline">\(trace(\Sigma) \, = \, \sigma^2_1 + \sigma^2_2 + \dots +\sigma^2_p\)</span></span>
</li>
</ul>
</div>
</div>
<div id="first-principal-component-textpc_1-y_1" class="section level3 unnumbered">
<h3>First Principal Component (<span class="math inline">\(\text{PC}_1\)</span>): <span class="math inline">\(Y_1\)</span><a class="anchor" aria-label="anchor" href="#first-principal-component-textpc_1-y_1"><i class="fas fa-link"></i></a>
</h3>
<p>The <em>first principal component</em> is the <em>normalized</em> linear combination of the features <span class="math inline">\(X_1,X_2,\ldots,X_p\)</span> that has maximum variance (among all linear combinations), so it accounts for as much variation in the data as possible.</p>
<p>Specifically we will define coefficients <span class="math inline">\(a_{11},a_{12},\ldots,a_{1p}\)</span> for that component in such a way that its variance is maximized, subject to the constraint that the sum of the squared coefficients is equal to one (that is what we mean by <em>normalized</em>). This constraint is required so that a unique answer may be obtained.</p>
<p>More formally, select <span class="math inline">\(a_{11},a_{12},\ldots,a_{1p}\)</span> that maximizes</p>
<p><span class="math display">\[ \text{var}(Y_1) = \mathbf{a}^T_1\Sigma\mathbf{a}_1  = \sum_{k=1}^{p}\sum_{l=1}^{p}a_{1k}a_{1l}\sigma_{kl} \]</span></p>
<p>subject to the constraint that</p>
<p><span class="math display">\[ \sum_{j=1}^{p}a^2_{1j} = \mathbf{a}^T_1\mathbf{a}_1   = 1 \]</span></p>
</div>
<div id="second-principal-component-textpc_2-y_2" class="section level3 unnumbered">
<h3>Second Principal Component (<span class="math inline">\(\text{PC}_2\)</span>): <span class="math inline">\(Y_2\)</span><a class="anchor" aria-label="anchor" href="#second-principal-component-textpc_2-y_2"><i class="fas fa-link"></i></a>
</h3>
<p>The <em>second principal component</em> is the linear combination of the features <span class="math inline">\(X_1,X_2,\ldots,X_p\)</span> that accounts for as much of the remaining variation as possible, with the constraint that the correlation between the first and second component is 0. So the second principal component has maximal variance out of all linear combinations that are uncorrelated with <span class="math inline">\(Y_1\)</span>.</p>
<p>To compute the coefficients of the second principal component, we select <span class="math inline">\(a_{21},a_{22},\ldots,a_{2p}\)</span> that maximizes the variance of this new component</p>
<p><span class="math display">\[\text{var}(Y_2) = \sum_{k=1}^{p}\sum_{l=1}^{p}a_{2k}a_{2l}\sigma_{kl} = \mathbf{a}^T_2\Sigma\mathbf{a}_2 \]</span></p>
<p>subject to:</p>
<ul>
<li><p>The constraint that the sums of squared coefficients add up to one, <span class="math inline">\(\sum_{j=1}^{p}a^2_{2j} = \mathbf{a}^T_2\mathbf{a}_2 = 1\)</span>.</p></li>
<li><p>Along with the additional constraint that these two components will be uncorrelated with one another:</p></li>
</ul>
<p><span class="math display">\[ \text{cov}(Y_1, Y_2) = \mathbf{a}^T_1\Sigma\mathbf{a}_2  = \sum_{k=1}^{p}\sum_{l=1}^{p}a_{1k}a_{2l}\sigma_{kl} = 0 \]</span></p>
<p>All subsequent principal components have this same property: they are linear combinations that account for as much of the remaining variation as possible and they are not correlated with the other principal components.</p>
<p>We will do this in the same way with each additional component. For instance:</p>
</div>
<div id="ith-principal-component-textpc_i-y_i" class="section level3 unnumbered">
<h3>
<span class="math inline">\(i^{th}\)</span> Principal Component (<span class="math inline">\(\text{PC}_i\)</span>): <span class="math inline">\(Y_i\)</span><a class="anchor" aria-label="anchor" href="#ith-principal-component-textpc_i-y_i"><i class="fas fa-link"></i></a>
</h3>
<p>We select <span class="math inline">\(a_{i1},a_{i2},\ldots,a_{ip}\)</span> that maximizes</p>
<p><span class="math display">\[ \text{var}(Y_i) = \sum_{k=1}^{p}\sum_{l=1}^{p}a_{ik}a_{il}\sigma_{kl} = \mathbf{a}^T_i\Sigma\mathbf{a}_i \]</span></p>
<p>subject to the constraint that the sums of squared coefficients add up to one, along with the additional constraint that this new component will be uncorrelated with all the previously defined components:</p>
<p><span class="math display">\[ \sum_{j=1}^{p}a^2_{ij} \mathbf{a}^T_i\mathbf{a}_i = \mathbf{a}^T_i\mathbf{a}_i = 1\]</span></p>
<p><span class="math display">\[ \text{cov}(Y_1, Y_i) = \sum_{k=1}^{p}\sum_{l=1}^{p}a_{1k}a_{il}\sigma_{kl} = \mathbf{a}^T_1\Sigma\mathbf{a}_i = 0 \]</span></p>
<p><span class="math display">\[\text{cov}(Y_2, Y_i) = \sum_{k=1}^{p}\sum_{l=1}^{p}a_{2k}a_{il}\sigma_{kl} = \mathbf{a}^T_2\Sigma\mathbf{a}_i = 0\]</span>
<span class="math display">\[\vdots\]</span>
<span class="math display">\[\text{cov}(Y_{i-1}, Y_i) = \sum_{k=1}^{p}\sum_{l=1}^{p}a_{i-1,k}a_{il}\sigma_{kl} = \mathbf{a}^T_{i-1}\Sigma\mathbf{a}_i = 0\]</span></p>
<p>Therefore all principal components are uncorrelated with one another.</p>
</div>
</div>
<div id="how-do-we-find-the-coefficients" class="section level2" number="6.3">
<h2>
<span class="header-section-number">6.3</span> How do we find the coefficients?<a class="anchor" aria-label="anchor" href="#how-do-we-find-the-coefficients"><i class="fas fa-link"></i></a>
</h2>
<p>How do we find the coefficients <span class="math inline">\(a_{ij}\)</span> for a principal component? The solution involves the <strong>eigenvalues</strong> and <strong>eigenvectors</strong> of the variance-covariance matrix <span class="math inline">\(\Sigma\)</span>.</p>
<p>Let <span class="math inline">\(\lambda_1,\ldots,\lambda_p\)</span> denote the eigenvalues of the variance-covariance matrix <span class="math inline">\(\Sigma\)</span>. These are ordered so that <span class="math inline">\(\lambda_1\)</span> has the largest eigenvalue and <span class="math inline">\(\lambda_p\)</span> is the smallest.</p>
<p><span class="math display">\[ \lambda_1 \ge \lambda_2 \ge \dots \ge \lambda_p \]</span></p>
<p>We are also going to let the vectors <span class="math inline">\(\mathbf{a}_1, \ldots,\mathbf{a}_p\)</span> denote the corresponding eigenvectors.</p>
<p>It turns out that the elements for these eigenvectors will be the coefficients of the principal components.</p>
<div class="rmdtip">
<p>
The elements for the eigenvectors of <span class="math inline"><span class="math inline">\(\Sigma\)</span></span> are the coefficients of the
principal components.
</p>
</div>
<p>The variance for the <span class="math inline">\(i\)</span>th principal component is equal to the <span class="math inline">\(i\)</span>th eigenvalue.</p>
<p><span class="math display">\[ \textbf{var}(Y_i) = \text{var}(a_{i1}X_1 + a_{i2}X_2 + \dots a_{ip}X_p) = \lambda_i \]</span></p>
<p>Moreover, the principal components are uncorrelated with one another.</p>
<p><span class="math display">\[\text{cov}(Y_i, Y_j) = 0\]</span></p>
<p>The variance-covariance matrix may be written as a function of the eigenvalues and their corresponding eigenvectors. In fact, the variance-covariance matrix can be written as the sum over the <span class="math inline">\(p\)</span> eigenvalues, multiplied by the product of the corresponding eigenvector times its transpose as shown in the following expression</p>
<p><span class="math display">\[ \Sigma  =  \sum_{i=1}^{p}\lambda_i \mathbf{a}_i \mathbf{a}_i^T \]</span></p>
<p>If <span class="math inline">\(\lambda_{k+1}, \lambda_{k+2}, \dots , \lambda_{p}\)</span> are small, we might approximate <span class="math inline">\(\Sigma\)</span> by</p>
<p><span class="math display">\[ \Sigma  \cong  \sum_{i=1}^{k}\lambda_i \mathbf{a}_i\mathbf{a}_i^T \]</span></p>
<p>Earlier in the chapter we defined the total variation of <span class="math inline">\(X\)</span> as the trace of the variance-covariance matrix. This is also equal to the sum of the eigenvalues as shown below:</p>
<p><span class="math display">\[ \begin{array}{lll}trace(\Sigma) &amp; = &amp; \sigma^2_1 + \sigma^2_2 + \dots +\sigma^2_p \\ &amp; = &amp; \lambda_1 + \lambda_2 + \dots + \lambda_p\end{array} \]</span></p>
<p>This will give us an interpretation of the components in terms of the amount of the full variation explained by each component. The proportion of variation explained by the <span class="math inline">\(i\)</span>th principal component is then going to be defined to be the eigenvalue for that component divided by the sum of the eigenvalues. In other words, the <span class="math inline">\(i\)</span>th principal component explains the following proportion of the total variation:</p>
<p><span class="math display">\[ \frac{\lambda_i}{\lambda_1 + \lambda_2 + \dots + \lambda_p} \]</span></p>
<p>A related quantity is the proportion of variation explained by the first <span class="math inline">\(k\)</span> principal component. This would be the sum of the first <span class="math inline">\(k\)</span> eigenvalues divided by its total variation.</p>
<p><span class="math display">\[ \frac{\lambda_1 + \lambda_2 + \dots + \lambda_k}{\lambda_1 + \lambda_2 + \dots + \lambda_p} \]</span></p>
<p>In practice, these proportions are often expressed as percentages.</p>
<p>Naturally, if the proportion of variation explained by the first <span class="math inline">\(k\)</span> principal components is large, then not much information is lost by considering only the first <span class="math inline">\(k\)</span> principal components.</p>
<div id="why-it-may-be-possible-to-reduce-dimensions" class="section level3 unnumbered">
<h3>Why It May Be Possible to Reduce Dimensions<a class="anchor" aria-label="anchor" href="#why-it-may-be-possible-to-reduce-dimensions"><i class="fas fa-link"></i></a>
</h3>
<p>When we have correlations (multicollinarity) between the features, the data may more or less fall on a line or plane in a lower number of dimensions. For instance, imagine a plot of two features that have a nearly perfect correlation. The data points will fall close to a straight line. That line could be used as a new (one-dimensional) axis to represent the variation among data points.</p>
<div class="caution">
<p>
All of this is defined in terms of the population variance-covariance
matrix <span class="math inline"><span class="math inline">\(\Sigma\)</span></span> which is
<em>unknown</em>. However, we may estimate <span class="math inline"><span class="math inline">\(\Sigma\)</span></span> by the sample variance-covariance
matrix which is given in the standard formula here:
</p>
<p>
<span class="math display"><span class="math display">\[ \textbf{S} = \frac{1}{n-1}
\sum_{i=1}^{n}(\mathbf{X}_i-\bar{\textbf{x}})(\mathbf{X}_i-\bar{\textbf{x}})^T
\]</span></span>
</p>
</div>
</div>
<div id="procedure" class="section level3 unnumbered">
<h3>Procedure<a class="anchor" aria-label="anchor" href="#procedure"><i class="fas fa-link"></i></a>
</h3>
<p>Compute the eigenvalues <span class="math inline">\(\hat{\lambda}_1, \hat{\lambda}_2, \dots, \hat{\lambda}_p\)</span> of the sample variance-covariance matrix <span class="math inline">\(\textbf{S}\)</span>, and the corresponding eigenvectors <span class="math inline">\(\hat{\mathbf{a}}_1, \hat{\mathbf{a}}_2, \dots, \hat{\mathbf{a}}_p\)</span>.</p>
<p>Then we will define our estimated principal components using the eigenvectors as our coefficients:</p>
<p><span class="math display">\[ \begin{array}{lll} \hat{Y}_1 &amp; = &amp; \hat{a}_{11}X_1 + \hat{a}_{12}X_2 + \dots + \hat{a}_{1p}X_p \\ \hat{Y}_2 &amp; = &amp; \hat{a}_{21}X_1 + \hat{a}_{22}X_2 + \dots + \hat{a}_{2p}X_p \\&amp;&amp;\vdots\\ \hat{Y}_p &amp; = &amp; \hat{a}_{p1}X_1 + \hat{a}_{p2}X_2 + \dots + \hat{a}_{pp}X_p \\ \end{array} \]</span></p>
<p>Generally, we only retain the first <span class="math inline">\(k\)</span> principal component. There are a number of criteria that may be used to decide how many components should be retained:</p>
<ol style="list-style-type: decimal">
<li><p>To obtain the simplest possible interpretation, we want <span class="math inline">\(k\)</span> to be as small as possible. If we can explain most of the variation just by <strong>two</strong> principal components then this would give us a much simpler description of the data.</p></li>
<li><p>Retain the first <span class="math inline">\(k\)</span> components which explain a “large” proportion of the total variation, say <span class="math inline">\(70-80\%\)</span>.</p></li>
<li><p>Examine a scree plot. This is a plot of the eigenvalues versus the component number. The idea is to look for the “elbow” which corresponds to the point after which the eigenvalues decrease more slowly. Adding components after this point explains relatively little more of the variance. See the next figure for an example of a scree plot.</p></li>
</ol>
<center>
<figure><img src="img/screeplot.png" alt="Scree plot showing eigenvalue by number of principal component." style="width: 400px;"><figcaption>
Scree plot showing eigenvalue by number of principal component.
</figcaption></figure>
</center>
</div>
</div>
<div id="standardization-of-the-features" class="section level2" number="6.4">
<h2>
<span class="header-section-number">6.4</span> Standardization of the features<a class="anchor" aria-label="anchor" href="#standardization-of-the-features"><i class="fas fa-link"></i></a>
</h2>
<p>If we use the raw data, the principal component analysis will tend to give more emphasis to the variables that have higher variances than to those variables that have very low variances.</p>
<p>In effect the results of the analysis will depend on what units of measurement are used to measure each variable. That would imply that a principal component analysis should only be used with the raw data if all variables have the same units of measure. And even in this case, only if you wish to give those variables which have higher variances more weight in the analysis.</p>
<div class="caution">
<ul>
<li>
The results of principal component analysis depend on the scales at
which the variables are measured.
</li>
<li>
Variables with the highest sample variances will tend to be
emphasized in the first few principal components.
</li>
<li>
Principal component analysis using the covariance function should
only be considered if all of the variables have the same units of
measurement.
</li>
</ul>
</div>
<p>If the variables either have different units of measurement, or if we wish each variable to receive equal weight in the analysis, then the variables should be <strong>standardized</strong> (<em>scaled</em>) before a principal components analysis is carried out. Standardize the variables by subtracting its mean from that variable and dividing it by its standard deviation:</p>
<p><span class="math display">\[Z_{ij} = \frac{X_{ij}-\bar{x}_j}{\sigma_j}\]</span></p>
<p>where</p>
<ul>
<li>
<span class="math inline">\(X_{ij}\)</span> = Data for variable <span class="math inline">\(j\)</span> in sample unit <span class="math inline">\(i\)</span>
</li>
<li>
<span class="math inline">\(\bar{x}_j\)</span> = Sample mean for variable <span class="math inline">\(j\)</span>
</li>
<li>
<span class="math inline">\(\sigma_j\)</span> = Sample standard deviation for variable <span class="math inline">\(j\)</span>
</li>
</ul>
<p><strong>Note</strong>: <span class="math inline">\(Z_j\)</span> has mean = 0 and variance = 1.</p>
<div class="rmdtip">
<p>
The variance-covariance matrix of the standardized data is equal to
the correlation matrix for the unstandardized data. Therefore, principal
component analysis using the standardized data is equivalent to
principal component analysis using the correlation matrix.
<strong>Remark</strong>: You are going to prove it in the practical work
session.
</p>
</div>
</div>
<div id="projection-of-the-data" class="section level2" number="6.5">
<h2>
<span class="header-section-number">6.5</span> Projection of the data<a class="anchor" aria-label="anchor" href="#projection-of-the-data"><i class="fas fa-link"></i></a>
</h2>
<div id="scores" class="section level3 unnumbered">
<h3>Scores<a class="anchor" aria-label="anchor" href="#scores"><i class="fas fa-link"></i></a>
</h3>
<p>Using the coefficients (loadings) of every principal component, we can project the observations on the axis of the principal component, those projections are called <em>scores</em>. For example, the scores of the first principal component are</p>
<p><span class="math display">\[  \forall 1 \le i \le n \quad \hat{Y}_1^i  =  \hat{a}_{11}X_1^i + \hat{a}_{12}X_2^i + \dots + \hat{a}_{1p}X_p^i \]</span></p>
<p>(<span class="math inline">\(X_1^i\)</span> is the value of feature <span class="math inline">\(1\)</span> for the observation <span class="math inline">\(i\)</span>)</p>
<p>This can be written for all observations and all the principal components using the matrix formulation</p>
<p><span class="math display">\[ \mathbf{\hat{Y}} = \mathbf{\hat{A}} X\]</span></p>
<p>where <span class="math inline">\(\mathbf{\hat{A}}\)</span> is the matrix of the coefficients <span class="math inline">\(\hat{a}_{ij}\)</span>.</p>
</div>
<div id="visualization" class="section level3 unnumbered">
<h3>Visualization<a class="anchor" aria-label="anchor" href="#visualization"><i class="fas fa-link"></i></a>
</h3>
<p>Once we have computed the principal components, we can plot them against each other in order to produce low-dimensional views of the data.</p>
<p>We can plot the score vector <span class="math inline">\(Y_1\)</span> against <span class="math inline">\(Y_2\)</span>, <span class="math inline">\(Y_1\)</span> against <span class="math inline">\(Y_3\)</span>, <span class="math inline">\(Y_2\)</span> against <span class="math inline">\(Y_3\)</span>, and so forth. Geometrically, this amounts to projecting the original data down onto the subspace spanned by <span class="math inline">\(\mathbf{a}_1\)</span>, <span class="math inline">\(\mathbf{a}_2\)</span>, and <span class="math inline">\(\mathbf{a}_3\)</span>, and plotting the projected points.</p>
<p>To interpret the results obtained by PCA, we plot on the same figure both the principal component scores and the
loading vectors. This figure is called a <strong><em>biplot</em></strong>. An example is given later in this chapter.</p>
</div>
<div id="extra" class="section level3 unnumbered">
<h3>Extra<a class="anchor" aria-label="anchor" href="#extra"><i class="fas fa-link"></i></a>
</h3>
<div class="rmdtip">
<ul>
<li><p>You can read this <a target="_blank" href="files/PrincipalComponents.pdf">tutorial <i class="fa fa-file-pdf-o" aria-hidden="true"></i></a>. In the document, there is an introduction about the mathemtical concepts used in PCA. Plus a detailed example of PCA.</p></li>
<li><p>You can watch these videos for a nice explanation of PCA <a target="_blank" href="https://fr.coursera.org/learn/machine-learning/lecture/GBFTt/principal-component-analysis-problem-formulation"><i class="fa fa-video-camera" aria-hidden="true"></i> 1</a> <a target="_blank" href="https://fr.coursera.org/learn/machine-learning/lecture/ZYIPa/principal-component-analysis-algorithm"><i class="fa fa-video-camera" aria-hidden="true"></i> 2</a>.</p></li>
</ul>
</div>
</div>
</div>
<div id="case-study" class="section level2" number="6.6">
<h2>
<span class="header-section-number">6.6</span> Case study<a class="anchor" aria-label="anchor" href="#case-study"><i class="fas fa-link"></i></a>
</h2>
<div id="employement-in-european-countries-in-the-late-70s" class="section level3 unnumbered">
<h3>Employement in European countries in the late 70s<a class="anchor" aria-label="anchor" href="#employement-in-european-countries-in-the-late-70s"><i class="fas fa-link"></i></a>
</h3>
<p>The purpose of this case study is to reveal the structure of the job market and economy in different developed countries. The final aim is to have a meaningful and rigorous plot that is able to show the most important features of the countries in a concise form.</p>
<p>The dataset <a target="_blank" href="datasets/eurojob.txt"><code>eurojob</code> <i class="fa fa-table" aria-hidden="true"></i></a> contains the data employed in this case study. It contains the percentage of workforce employed in 1979 in 9 industries for 26 European countries. The industries measured are:</p>
<ul>
<li>Agriculture (<code>Agr</code>)</li>
<li>Mining (<code>Min</code>)</li>
<li>Manufacturing (<code>Man</code>)</li>
<li>Power supply industries <code>(Pow</code>)</li>
<li>Construction (<code>Con</code>)</li>
<li>Service industries (<code>Ser</code>)</li>
<li>Finance (<code>Fin</code>)</li>
<li>Social and personal services (<code>Soc</code>)</li>
<li>Transport and communications (<code>Tra</code>)</li>
</ul>
<p>If the dataset is imported into <svg aria-hidden="true" role="img" viewbox="0 0 581 512" style="height:1em;width:1.13em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:steelblue;overflow:visible;position:relative;"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"></path></svg> and the case names are set as <code>Country</code> (important in order to have only numerical variables), then the data should look like this:</p>
<div class="inline-table"><table class="table table-sm">
<caption>
<span id="tab:eurotable">Table 6.1: </span>The <code>eurojob</code> dataset.
</caption>
<thead><tr>
<th style="text-align:left;">
Country
</th>
<th style="text-align:right;">
Agr
</th>
<th style="text-align:right;">
Min
</th>
<th style="text-align:right;">
Man
</th>
<th style="text-align:right;">
Pow
</th>
<th style="text-align:right;">
Con
</th>
<th style="text-align:right;">
Ser
</th>
<th style="text-align:right;">
Fin
</th>
<th style="text-align:right;">
Soc
</th>
<th style="text-align:right;">
Tra
</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align:left;">
Belgium
</td>
<td style="text-align:right;">
3.3
</td>
<td style="text-align:right;">
0.9
</td>
<td style="text-align:right;">
27.6
</td>
<td style="text-align:right;">
0.9
</td>
<td style="text-align:right;">
8.2
</td>
<td style="text-align:right;">
19.1
</td>
<td style="text-align:right;">
6.2
</td>
<td style="text-align:right;">
26.6
</td>
<td style="text-align:right;">
7.2
</td>
</tr>
<tr>
<td style="text-align:left;">
Denmark
</td>
<td style="text-align:right;">
9.2
</td>
<td style="text-align:right;">
0.1
</td>
<td style="text-align:right;">
21.8
</td>
<td style="text-align:right;">
0.6
</td>
<td style="text-align:right;">
8.3
</td>
<td style="text-align:right;">
14.6
</td>
<td style="text-align:right;">
6.5
</td>
<td style="text-align:right;">
32.2
</td>
<td style="text-align:right;">
7.1
</td>
</tr>
<tr>
<td style="text-align:left;">
France
</td>
<td style="text-align:right;">
10.8
</td>
<td style="text-align:right;">
0.8
</td>
<td style="text-align:right;">
27.5
</td>
<td style="text-align:right;">
0.9
</td>
<td style="text-align:right;">
8.9
</td>
<td style="text-align:right;">
16.8
</td>
<td style="text-align:right;">
6.0
</td>
<td style="text-align:right;">
22.6
</td>
<td style="text-align:right;">
5.7
</td>
</tr>
<tr>
<td style="text-align:left;">
WGerm
</td>
<td style="text-align:right;">
6.7
</td>
<td style="text-align:right;">
1.3
</td>
<td style="text-align:right;">
35.8
</td>
<td style="text-align:right;">
0.9
</td>
<td style="text-align:right;">
7.3
</td>
<td style="text-align:right;">
14.4
</td>
<td style="text-align:right;">
5.0
</td>
<td style="text-align:right;">
22.3
</td>
<td style="text-align:right;">
6.1
</td>
</tr>
<tr>
<td style="text-align:left;">
Ireland
</td>
<td style="text-align:right;">
23.2
</td>
<td style="text-align:right;">
1.0
</td>
<td style="text-align:right;">
20.7
</td>
<td style="text-align:right;">
1.3
</td>
<td style="text-align:right;">
7.5
</td>
<td style="text-align:right;">
16.8
</td>
<td style="text-align:right;">
2.8
</td>
<td style="text-align:right;">
20.8
</td>
<td style="text-align:right;">
6.1
</td>
</tr>
<tr>
<td style="text-align:left;">
Italy
</td>
<td style="text-align:right;">
15.9
</td>
<td style="text-align:right;">
0.6
</td>
<td style="text-align:right;">
27.6
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
10.0
</td>
<td style="text-align:right;">
18.1
</td>
<td style="text-align:right;">
1.6
</td>
<td style="text-align:right;">
20.1
</td>
<td style="text-align:right;">
5.7
</td>
</tr>
<tr>
<td style="text-align:left;">
Luxem
</td>
<td style="text-align:right;">
7.7
</td>
<td style="text-align:right;">
3.1
</td>
<td style="text-align:right;">
30.8
</td>
<td style="text-align:right;">
0.8
</td>
<td style="text-align:right;">
9.2
</td>
<td style="text-align:right;">
18.5
</td>
<td style="text-align:right;">
4.6
</td>
<td style="text-align:right;">
19.2
</td>
<td style="text-align:right;">
6.2
</td>
</tr>
<tr>
<td style="text-align:left;">
Nether
</td>
<td style="text-align:right;">
6.3
</td>
<td style="text-align:right;">
0.1
</td>
<td style="text-align:right;">
22.5
</td>
<td style="text-align:right;">
1.0
</td>
<td style="text-align:right;">
9.9
</td>
<td style="text-align:right;">
18.0
</td>
<td style="text-align:right;">
6.8
</td>
<td style="text-align:right;">
28.5
</td>
<td style="text-align:right;">
6.8
</td>
</tr>
<tr>
<td style="text-align:left;">
UK
</td>
<td style="text-align:right;">
2.7
</td>
<td style="text-align:right;">
1.4
</td>
<td style="text-align:right;">
30.2
</td>
<td style="text-align:right;">
1.4
</td>
<td style="text-align:right;">
6.9
</td>
<td style="text-align:right;">
16.9
</td>
<td style="text-align:right;">
5.7
</td>
<td style="text-align:right;">
28.3
</td>
<td style="text-align:right;">
6.4
</td>
</tr>
<tr>
<td style="text-align:left;">
Austria
</td>
<td style="text-align:right;">
12.7
</td>
<td style="text-align:right;">
1.1
</td>
<td style="text-align:right;">
30.2
</td>
<td style="text-align:right;">
1.4
</td>
<td style="text-align:right;">
9.0
</td>
<td style="text-align:right;">
16.8
</td>
<td style="text-align:right;">
4.9
</td>
<td style="text-align:right;">
16.8
</td>
<td style="text-align:right;">
7.0
</td>
</tr>
<tr>
<td style="text-align:left;">
Finland
</td>
<td style="text-align:right;">
13.0
</td>
<td style="text-align:right;">
0.4
</td>
<td style="text-align:right;">
25.9
</td>
<td style="text-align:right;">
1.3
</td>
<td style="text-align:right;">
7.4
</td>
<td style="text-align:right;">
14.7
</td>
<td style="text-align:right;">
5.5
</td>
<td style="text-align:right;">
24.3
</td>
<td style="text-align:right;">
7.6
</td>
</tr>
<tr>
<td style="text-align:left;">
Greece
</td>
<td style="text-align:right;">
41.4
</td>
<td style="text-align:right;">
0.6
</td>
<td style="text-align:right;">
17.6
</td>
<td style="text-align:right;">
0.6
</td>
<td style="text-align:right;">
8.1
</td>
<td style="text-align:right;">
11.5
</td>
<td style="text-align:right;">
2.4
</td>
<td style="text-align:right;">
11.0
</td>
<td style="text-align:right;">
6.7
</td>
</tr>
<tr>
<td style="text-align:left;">
Norway
</td>
<td style="text-align:right;">
9.0
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
22.4
</td>
<td style="text-align:right;">
0.8
</td>
<td style="text-align:right;">
8.6
</td>
<td style="text-align:right;">
16.9
</td>
<td style="text-align:right;">
4.7
</td>
<td style="text-align:right;">
27.6
</td>
<td style="text-align:right;">
9.4
</td>
</tr>
<tr>
<td style="text-align:left;">
Portugal
</td>
<td style="text-align:right;">
27.8
</td>
<td style="text-align:right;">
0.3
</td>
<td style="text-align:right;">
24.5
</td>
<td style="text-align:right;">
0.6
</td>
<td style="text-align:right;">
8.4
</td>
<td style="text-align:right;">
13.3
</td>
<td style="text-align:right;">
2.7
</td>
<td style="text-align:right;">
16.7
</td>
<td style="text-align:right;">
5.7
</td>
</tr>
<tr>
<td style="text-align:left;">
Spain
</td>
<td style="text-align:right;">
22.9
</td>
<td style="text-align:right;">
0.8
</td>
<td style="text-align:right;">
28.5
</td>
<td style="text-align:right;">
0.7
</td>
<td style="text-align:right;">
11.5
</td>
<td style="text-align:right;">
9.7
</td>
<td style="text-align:right;">
8.5
</td>
<td style="text-align:right;">
11.8
</td>
<td style="text-align:right;">
5.5
</td>
</tr>
<tr>
<td style="text-align:left;">
Sweden
</td>
<td style="text-align:right;">
6.1
</td>
<td style="text-align:right;">
0.4
</td>
<td style="text-align:right;">
25.9
</td>
<td style="text-align:right;">
0.8
</td>
<td style="text-align:right;">
7.2
</td>
<td style="text-align:right;">
14.4
</td>
<td style="text-align:right;">
6.0
</td>
<td style="text-align:right;">
32.4
</td>
<td style="text-align:right;">
6.8
</td>
</tr>
<tr>
<td style="text-align:left;">
Switz
</td>
<td style="text-align:right;">
7.7
</td>
<td style="text-align:right;">
0.2
</td>
<td style="text-align:right;">
37.8
</td>
<td style="text-align:right;">
0.8
</td>
<td style="text-align:right;">
9.5
</td>
<td style="text-align:right;">
17.5
</td>
<td style="text-align:right;">
5.3
</td>
<td style="text-align:right;">
15.4
</td>
<td style="text-align:right;">
5.7
</td>
</tr>
<tr>
<td style="text-align:left;">
Turkey
</td>
<td style="text-align:right;">
66.8
</td>
<td style="text-align:right;">
0.7
</td>
<td style="text-align:right;">
7.9
</td>
<td style="text-align:right;">
0.1
</td>
<td style="text-align:right;">
2.8
</td>
<td style="text-align:right;">
5.2
</td>
<td style="text-align:right;">
1.1
</td>
<td style="text-align:right;">
11.9
</td>
<td style="text-align:right;">
3.2
</td>
</tr>
<tr>
<td style="text-align:left;">
Bulgaria
</td>
<td style="text-align:right;">
23.6
</td>
<td style="text-align:right;">
1.9
</td>
<td style="text-align:right;">
32.3
</td>
<td style="text-align:right;">
0.6
</td>
<td style="text-align:right;">
7.9
</td>
<td style="text-align:right;">
8.0
</td>
<td style="text-align:right;">
0.7
</td>
<td style="text-align:right;">
18.2
</td>
<td style="text-align:right;">
6.7
</td>
</tr>
<tr>
<td style="text-align:left;">
Czech
</td>
<td style="text-align:right;">
16.5
</td>
<td style="text-align:right;">
2.9
</td>
<td style="text-align:right;">
35.5
</td>
<td style="text-align:right;">
1.2
</td>
<td style="text-align:right;">
8.7
</td>
<td style="text-align:right;">
9.2
</td>
<td style="text-align:right;">
0.9
</td>
<td style="text-align:right;">
17.9
</td>
<td style="text-align:right;">
7.0
</td>
</tr>
<tr>
<td style="text-align:left;">
EGerm
</td>
<td style="text-align:right;">
4.2
</td>
<td style="text-align:right;">
2.9
</td>
<td style="text-align:right;">
41.2
</td>
<td style="text-align:right;">
1.3
</td>
<td style="text-align:right;">
7.6
</td>
<td style="text-align:right;">
11.2
</td>
<td style="text-align:right;">
1.2
</td>
<td style="text-align:right;">
22.1
</td>
<td style="text-align:right;">
8.4
</td>
</tr>
<tr>
<td style="text-align:left;">
Hungary
</td>
<td style="text-align:right;">
21.7
</td>
<td style="text-align:right;">
3.1
</td>
<td style="text-align:right;">
29.6
</td>
<td style="text-align:right;">
1.9
</td>
<td style="text-align:right;">
8.2
</td>
<td style="text-align:right;">
9.4
</td>
<td style="text-align:right;">
0.9
</td>
<td style="text-align:right;">
17.2
</td>
<td style="text-align:right;">
8.0
</td>
</tr>
<tr>
<td style="text-align:left;">
Poland
</td>
<td style="text-align:right;">
31.1
</td>
<td style="text-align:right;">
2.5
</td>
<td style="text-align:right;">
25.7
</td>
<td style="text-align:right;">
0.9
</td>
<td style="text-align:right;">
8.4
</td>
<td style="text-align:right;">
7.5
</td>
<td style="text-align:right;">
0.9
</td>
<td style="text-align:right;">
16.1
</td>
<td style="text-align:right;">
6.9
</td>
</tr>
<tr>
<td style="text-align:left;">
Romania
</td>
<td style="text-align:right;">
34.7
</td>
<td style="text-align:right;">
2.1
</td>
<td style="text-align:right;">
30.1
</td>
<td style="text-align:right;">
0.6
</td>
<td style="text-align:right;">
8.7
</td>
<td style="text-align:right;">
5.9
</td>
<td style="text-align:right;">
1.3
</td>
<td style="text-align:right;">
11.7
</td>
<td style="text-align:right;">
5.0
</td>
</tr>
<tr>
<td style="text-align:left;">
USSR
</td>
<td style="text-align:right;">
23.7
</td>
<td style="text-align:right;">
1.4
</td>
<td style="text-align:right;">
25.8
</td>
<td style="text-align:right;">
0.6
</td>
<td style="text-align:right;">
9.2
</td>
<td style="text-align:right;">
6.1
</td>
<td style="text-align:right;">
0.5
</td>
<td style="text-align:right;">
23.6
</td>
<td style="text-align:right;">
9.3
</td>
</tr>
<tr>
<td style="text-align:left;">
Yugoslavia
</td>
<td style="text-align:right;">
48.7
</td>
<td style="text-align:right;">
1.5
</td>
<td style="text-align:right;">
16.8
</td>
<td style="text-align:right;">
1.1
</td>
<td style="text-align:right;">
4.9
</td>
<td style="text-align:right;">
6.4
</td>
<td style="text-align:right;">
11.3
</td>
<td style="text-align:right;">
5.3
</td>
<td style="text-align:right;">
4.0
</td>
</tr>
</tbody>
</table></div>
<p><strong>Note</strong>: To set the case names as <code>Country</code>, we do</p>
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="fu"><a href="https://rdrr.io/r/base/row.names.html">row.names</a></span><span class="op">(</span><span class="va">eurojob</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="va">eurojob</span><span class="op">$</span><span class="va">Country</span>
<span class="va">eurojob</span><span class="op">$</span><span class="va">Country</span> <span class="op">&lt;-</span> <span class="cn">NULL</span></code></pre></div>
<p>So far, we know how to compute summaries for <em>each variable</em>, and how to quantify and visualize relations between variables with the correlation matrix and the scatterplot matrix. But even for a moderate number of variables like this, their results are hard to process.</p>
<div class="sourceCode" id="cb50"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Summary of the data - marginal</span>
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">eurojob</span><span class="op">)</span>
<span class="co">#ans&gt;       Agr            Min             Man            Pow             Con       </span>
<span class="co">#ans&gt;  Min.   : 2.7   Min.   :0.100   Min.   : 7.9   Min.   :0.100   Min.   : 2.80  </span>
<span class="co">#ans&gt;  1st Qu.: 7.7   1st Qu.:0.525   1st Qu.:23.0   1st Qu.:0.600   1st Qu.: 7.53  </span>
<span class="co">#ans&gt;  Median :14.4   Median :0.950   Median :27.6   Median :0.850   Median : 8.35  </span>
<span class="co">#ans&gt;  Mean   :19.1   Mean   :1.254   Mean   :27.0   Mean   :0.908   Mean   : 8.17  </span>
<span class="co">#ans&gt;  3rd Qu.:23.7   3rd Qu.:1.800   3rd Qu.:30.2   3rd Qu.:1.175   3rd Qu.: 8.97  </span>
<span class="co">#ans&gt;  Max.   :66.8   Max.   :3.100   Max.   :41.2   Max.   :1.900   Max.   :11.50  </span>
<span class="co">#ans&gt;       Ser             Fin             Soc            Tra      </span>
<span class="co">#ans&gt;  Min.   : 5.20   Min.   : 0.50   Min.   : 5.3   Min.   :3.20  </span>
<span class="co">#ans&gt;  1st Qu.: 9.25   1st Qu.: 1.23   1st Qu.:16.2   1st Qu.:5.70  </span>
<span class="co">#ans&gt;  Median :14.40   Median : 4.65   Median :19.6   Median :6.70  </span>
<span class="co">#ans&gt;  Mean   :12.96   Mean   : 4.00   Mean   :20.0   Mean   :6.55  </span>
<span class="co">#ans&gt;  3rd Qu.:16.88   3rd Qu.: 5.92   3rd Qu.:24.1   3rd Qu.:7.08  </span>
<span class="co">#ans&gt;  Max.   :19.10   Max.   :11.30   Max.   :32.4   Max.   :9.40</span>

<span class="co"># Correlation matrix</span>
<span class="fu"><a href="https://rdrr.io/r/stats/cor.html">cor</a></span><span class="op">(</span><span class="va">eurojob</span><span class="op">)</span>
<span class="co">#ans&gt;         Agr     Min    Man     Pow     Con    Ser     Fin    Soc    Tra</span>
<span class="co">#ans&gt; Agr  1.0000  0.0358 -0.671 -0.4001 -0.5383 -0.737 -0.2198 -0.747 -0.565</span>
<span class="co">#ans&gt; Min  0.0358  1.0000  0.445  0.4055 -0.0256 -0.397 -0.4427 -0.281  0.157</span>
<span class="co">#ans&gt; Man -0.6711  0.4452  1.000  0.3853  0.4945  0.204 -0.1558  0.154  0.351</span>
<span class="co">#ans&gt; Pow -0.4001  0.4055  0.385  1.0000  0.0599  0.202  0.1099  0.132  0.375</span>
<span class="co">#ans&gt; Con -0.5383 -0.0256  0.494  0.0599  1.0000  0.356  0.0163  0.158  0.388</span>
<span class="co">#ans&gt; Ser -0.7370 -0.3966  0.204  0.2019  0.3560  1.000  0.3656  0.572  0.188</span>
<span class="co">#ans&gt; Fin -0.2198 -0.4427 -0.156  0.1099  0.0163  0.366  1.0000  0.108 -0.246</span>
<span class="co">#ans&gt; Soc -0.7468 -0.2810  0.154  0.1324  0.1582  0.572  0.1076  1.000  0.568</span>
<span class="co">#ans&gt; Tra -0.5649  0.1566  0.351  0.3752  0.3877  0.188 -0.2459  0.568  1.000</span>

<span class="co"># Scatterplot matrix</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">require</a></span><span class="op">(</span><span class="va"><a href="https://r-forge.r-project.org/projects/car/">car</a></span><span class="op">)</span>
<span class="fu"><a href="https://rdrr.io/pkg/car/man/scatterplotMatrix.html">scatterplotMatrix</a></span><span class="op">(</span><span class="va">eurojob</span>, regLine <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>method<span class="op">=</span><span class="va">lm</span>, lty<span class="op">=</span><span class="fl">1</span>, lwd<span class="op">=</span><span class="fl">2</span>, col<span class="op">=</span><span class="st">"green"</span><span class="op">)</span>, smooth <span class="op">=</span> <span class="cn">FALSE</span>,
                  ellipse <span class="op">=</span> <span class="cn">FALSE</span>, plot.points <span class="op">=</span> <span class="cn">T</span>, col<span class="op">=</span><span class="st">"black"</span>,
                  diagonal <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></code></pre></div>
<div class="inline-figure">
<img src="Machine-Learning_files/figure-html/unnamed-chunk-167-1.png" width="70%" style="display: block; margin: auto;">
We definitely need a way of visualizing and quantifying the relations between variables for a moderate to large amount of variables. PCA will be a handy way. Recall what PCA does:</div>
<ol style="list-style-type: decimal">
<li>Takes the data for the variables <span class="math inline">\(X_1,\ldots,X_p\)</span>.</li>
<li>Using this data, looks for new variables <span class="math inline">\(\text{PC}_1,\ldots \text{PC}_p\)</span> such that:
<ul>
<li>
<span class="math inline">\(\text{PC}_j\)</span> is a <strong>linear combination</strong> of <span class="math inline">\(X_1,\ldots,X_p\)</span>, <span class="math inline">\(1\leq j\leq p\)</span>. This is, <span class="math inline">\(\text{PC}_j=a_{j1}X_1+a_{j2}X_2+\ldots+a_{jp}X_p\)</span>.</li>
<li>
<span class="math inline">\(\text{PC}_1,\ldots \text{PC}_p\)</span> are <strong>sorted decreasingly in terms of variance</strong>. Hence <span class="math inline">\(\text{PC}_j\)</span> has more variance than <span class="math inline">\(\text{PC}_{j+1}\)</span>, <span class="math inline">\(1\leq j\leq p-1\)</span>,</li>
<li>
<span class="math inline">\(\text{PC}_{j_1}\)</span> and <span class="math inline">\(\text{PC}_{j_2}\)</span> are <strong>uncorrelated</strong>, for <span class="math inline">\(j_1\neq j_2\)</span>.</li>
<li>
<span class="math inline">\(\text{PC}_1,\ldots \text{PC}_p\)</span> have the <strong>same information</strong>, measured in terms of <strong>total variance</strong>, as <span class="math inline">\(X_1,\ldots,X_p\)</span>.</li>
</ul>
</li>
<li>Produces three key objects:
<ul>
<li>
<strong>Variances of the PCs</strong>. They are sorted decreasingly and give an idea of which PCs contain most of the information of the data (the ones with more variance).</li>
<li>
<strong>Weights of the variables in the PCs</strong>. They give the interpretation of the PCs in terms of the original variables, as they are the coefficients of the linear combination. The weights of the variables <span class="math inline">\(X_1,\ldots,X_p\)</span> on the <span class="math inline">\(PC_j\)</span>, <span class="math inline">\(a_{j1},\ldots,a_{jp}\)</span>, are normalized: <span class="math inline">\(a_{1j}^2+\ldots+a_{pj}^2=1\)</span>, <span class="math inline">\(j=1,\ldots,p\)</span>. In <svg aria-hidden="true" role="img" viewbox="0 0 581 512" style="height:1em;width:1.13em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:steelblue;overflow:visible;position:relative;"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"></path></svg>, they are called <code>loadings</code>.</li>
<li>
<strong>Scores of the data in the PCs</strong>: this is the data with <span class="math inline">\(\text{PC}_1,\ldots \text{PC}_p\)</span> variables instead of <span class="math inline">\(X_1,\ldots,X_p\)</span>. The <strong>scores are uncorrelated</strong>. Useful for knowing which PCs have more effect on a certain observation.</li>
</ul>
</li>
</ol>
<p>Hence, PCA rearranges our variables in an information-equivalent, but more convenient, layout where the variables are <strong>sorted according to the ammount of information they are able to explain</strong>. From this position, the next step is clear: <strong>stick only with a limited number of PCs such that they explain most of the information</strong> (e.g., 70% of the total variance) and do <em>dimension reduction</em>. The effectiveness of PCA in practice varies from the structure present in the dataset. For example, in the case of highly dependent data, it could explain more than the 90% of variability of a dataset with tens of variables with just two PCs.</p>
<p>Let’s see how to compute a full PCA in <svg aria-hidden="true" role="img" viewbox="0 0 581 512" style="height:1em;width:1.13em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:steelblue;overflow:visible;position:relative;"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"></path></svg>.</p>
<div class="sourceCode" id="cb51"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># The main function - use cor = TRUE to avoid scale distortions</span>
<span class="va">pca</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/princomp.html">princomp</a></span><span class="op">(</span><span class="va">eurojob</span>, cor <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>

<span class="co"># What is inside?</span>
<span class="fu"><a href="https://rdrr.io/r/utils/str.html">str</a></span><span class="op">(</span><span class="va">pca</span><span class="op">)</span>
<span class="co">#ans&gt; List of 7</span>
<span class="co">#ans&gt;  $ sdev    : Named num [1:9] 1.867 1.46 1.048 0.997 0.737 ...</span>
<span class="co">#ans&gt;   ..- attr(*, "names")= chr [1:9] "Comp.1" "Comp.2" "Comp.3" "Comp.4" ...</span>
<span class="co">#ans&gt;  $ loadings: 'loadings' num [1:9, 1:9] 0.52379 0.00132 -0.3475 -0.25572 -0.32518 ...</span>
<span class="co">#ans&gt;   ..- attr(*, "dimnames")=List of 2</span>
<span class="co">#ans&gt;   .. ..$ : chr [1:9] "Agr" "Min" "Man" "Pow" ...</span>
<span class="co">#ans&gt;   .. ..$ : chr [1:9] "Comp.1" "Comp.2" "Comp.3" "Comp.4" ...</span>
<span class="co">#ans&gt;  $ center  : Named num [1:9] 19.131 1.254 27.008 0.908 8.165 ...</span>
<span class="co">#ans&gt;   ..- attr(*, "names")= chr [1:9] "Agr" "Min" "Man" "Pow" ...</span>
<span class="co">#ans&gt;  $ scale   : Named num [1:9] 15.245 0.951 6.872 0.369 1.614 ...</span>
<span class="co">#ans&gt;   ..- attr(*, "names")= chr [1:9] "Agr" "Min" "Man" "Pow" ...</span>
<span class="co">#ans&gt;  $ n.obs   : int 26</span>
<span class="co">#ans&gt;  $ scores  : num [1:26, 1:9] -1.71 -0.953 -0.755 -0.853 0.104 ...</span>
<span class="co">#ans&gt;   ..- attr(*, "dimnames")=List of 2</span>
<span class="co">#ans&gt;   .. ..$ : chr [1:26] "Belgium" "Denmark" "France" "WGerm" ...</span>
<span class="co">#ans&gt;   .. ..$ : chr [1:9] "Comp.1" "Comp.2" "Comp.3" "Comp.4" ...</span>
<span class="co">#ans&gt;  $ call    : language princomp(x = eurojob, cor = TRUE)</span>
<span class="co">#ans&gt;  - attr(*, "class")= chr "princomp"</span>

<span class="co"># The standard deviation of each PC</span>
<span class="va">pca</span><span class="op">$</span><span class="va">sdev</span>
<span class="co">#ans&gt;  Comp.1  Comp.2  Comp.3  Comp.4  Comp.5  Comp.6  Comp.7  Comp.8  Comp.9 </span>
<span class="co">#ans&gt; 1.86739 1.45951 1.04831 0.99724 0.73703 0.61922 0.47514 0.36985 0.00675</span>

<span class="co"># Weights: the expression of the original variables in the PCs</span>
<span class="co"># E.g. Agr = -0.524 * PC1 + 0.213 * PC5 - 0.152 * PC6 + 0.806 * PC9</span>
<span class="co"># And also: PC1 = -0.524 * Agr + 0.347 * Man + 0256 * Pow + 0.325 * Con + ...</span>
<span class="co"># (Because the matrix is orthogonal, so the transpose is the inverse)</span>
<span class="va">pca</span><span class="op">$</span><span class="va">loadings</span>
<span class="co">#ans&gt; </span>
<span class="co">#ans&gt; Loadings:</span>
<span class="co">#ans&gt;     Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 Comp.9</span>
<span class="co">#ans&gt; Agr  0.524                       0.213  0.153                0.806</span>
<span class="co">#ans&gt; Min         0.618 -0.201        -0.164 -0.101 -0.726              </span>
<span class="co">#ans&gt; Man -0.347  0.355 -0.150 -0.346 -0.385 -0.288  0.479  0.126  0.366</span>
<span class="co">#ans&gt; Pow -0.256  0.261 -0.561  0.393  0.295  0.357  0.256 -0.341       </span>
<span class="co">#ans&gt; Con -0.325         0.153 -0.668  0.472  0.130 -0.221 -0.356       </span>
<span class="co">#ans&gt; Ser -0.379 -0.350 -0.115        -0.284  0.615 -0.229  0.388  0.238</span>
<span class="co">#ans&gt; Fin        -0.454 -0.587         0.280 -0.526 -0.187  0.174  0.145</span>
<span class="co">#ans&gt; Soc -0.387 -0.222  0.312  0.412 -0.220 -0.263 -0.191 -0.506  0.351</span>
<span class="co">#ans&gt; Tra -0.367  0.203  0.375  0.314  0.513 -0.124         0.545       </span>
<span class="co">#ans&gt; </span>
<span class="co">#ans&gt;                Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 Comp.9</span>
<span class="co">#ans&gt; SS loadings     1.000  1.000  1.000  1.000  1.000  1.000  1.000  1.000  1.000</span>
<span class="co">#ans&gt; Proportion Var  0.111  0.111  0.111  0.111  0.111  0.111  0.111  0.111  0.111</span>
<span class="co">#ans&gt; Cumulative Var  0.111  0.222  0.333  0.444  0.556  0.667  0.778  0.889  1.000</span>

<span class="co"># Scores of the data on the PCs: how is the data reexpressed into PCs</span>
<span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">pca</span><span class="op">$</span><span class="va">scores</span>, <span class="fl">10</span><span class="op">)</span>
<span class="co">#ans&gt;         Comp.1  Comp.2  Comp.3  Comp.4  Comp.5  Comp.6  Comp.7 Comp.8    Comp.9</span>
<span class="co">#ans&gt; Belgium -1.710 -1.2218 -0.1148  0.3395 -0.3245 -0.0473 -0.3401  0.403 -0.001090</span>
<span class="co">#ans&gt; Denmark -0.953 -2.1278  0.9507  0.5939  0.1027 -0.8273 -0.3029 -0.352  0.015619</span>
<span class="co">#ans&gt; France  -0.755 -1.1212 -0.4980 -0.5003 -0.2997  0.1158 -0.1855 -0.266 -0.000507</span>
<span class="co">#ans&gt; WGerm   -0.853 -0.0114 -0.5795 -0.1105 -1.1652 -0.6181  0.4446  0.194 -0.006539</span>
<span class="co">#ans&gt; Ireland  0.104 -0.4140 -0.3840  0.9267  0.0152  1.4242 -0.0370 -0.334  0.010879</span>
<span class="co">#ans&gt; Italy   -0.375 -0.7695  1.0606 -1.4772 -0.6452  1.0021 -0.1418 -0.130  0.005602</span>
<span class="co">#ans&gt; Luxem   -1.059  0.7558 -0.6515 -0.8352 -0.8659  0.2188 -1.6942  0.547  0.003453</span>
<span class="co">#ans&gt; Nether  -1.688 -2.0048  0.0637 -0.0235  0.6352  0.2120 -0.3034 -0.591 -0.010931</span>
<span class="co">#ans&gt; UK      -1.630 -0.3731 -1.1409  1.2669 -0.8129 -0.0361  0.0413 -0.349 -0.005478</span>
<span class="co">#ans&gt; Austria -1.176  0.1431 -1.0434 -0.1577  0.5210  0.8019  0.4150  0.215 -0.002816</span>

<span class="co"># Scatterplot matrix of the scores - See how they are uncorrelated!</span>
<span class="fu"><a href="https://rdrr.io/pkg/car/man/scatterplotMatrix.html">scatterplotMatrix</a></span><span class="op">(</span><span class="va">pca</span><span class="op">$</span><span class="va">scores</span>, regLine <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>method<span class="op">=</span><span class="va">lm</span>, lty<span class="op">=</span><span class="fl">1</span>, lwd<span class="op">=</span><span class="fl">2</span>, col<span class="op">=</span><span class="st">"green"</span><span class="op">)</span>, smooth <span class="op">=</span> <span class="cn">FALSE</span>,
                  ellipse <span class="op">=</span> <span class="cn">FALSE</span>, plot.points <span class="op">=</span> <span class="cn">T</span>, col<span class="op">=</span><span class="st">"black"</span>,
                  diagonal <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="Machine-Learning_files/figure-html/unnamed-chunk-168-1.png" width="70%" style="display: block; margin: auto;"></div>
<div class="rmdtip">
<p>
PCA produces <strong>uncorrelated</strong> variables from the
original set <span class="math inline"><span class="math inline">\(X_1,\ldots,X_p\)</span></span>. This
implies that:
</p>
<ul>
<li>
The PCs are uncorrelated, <strong>but not independent</strong>
(uncorrelated does not imply independent).
</li>
<li>
An uncorrelated or independent variable in <span class="math inline"><span class="math inline">\(X_1,\ldots,X_p\)</span></span> will get a PC only
associated to it. In the extreme case where all the <span class="math inline"><span class="math inline">\(X_1,\ldots,X_p\)</span></span> are uncorrelated, these
coincide with the PCs (up to sign flips).
</li>
</ul>
</div>
<div class="sourceCode" id="cb52"><pre class="downlit sourceCode r">
<code class="sourceCode R">
<span class="co"># Means of the variables - before PCA the variables are centered</span>
<span class="va">pca</span><span class="op">$</span><span class="va">center</span>
<span class="co">#ans&gt;    Agr    Min    Man    Pow    Con    Ser    Fin    Soc    Tra </span>
<span class="co">#ans&gt; 19.131  1.254 27.008  0.908  8.165 12.958  4.000 20.023  6.546</span>

<span class="co"># Rescalation done to each variable</span>
<span class="co"># - if cor = FALSE (default), a vector of ones</span>
<span class="co"># - if cor = TRUE, a vector with the standard deviations of the variables</span>
<span class="va">pca</span><span class="op">$</span><span class="va">scale</span>
<span class="co">#ans&gt;    Agr    Min    Man    Pow    Con    Ser    Fin    Soc    Tra </span>
<span class="co">#ans&gt; 15.245  0.951  6.872  0.369  1.614  4.486  2.752  6.697  1.364</span>

<span class="co"># Summary of the importance of components - the third row is key</span>
<span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">pca</span><span class="op">)</span>
<span class="co">#ans&gt; Importance of components:</span>
<span class="co">#ans&gt;                        Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8</span>
<span class="co">#ans&gt; Standard deviation      1.867  1.460  1.048  0.997 0.7370 0.6192 0.4751 0.3699</span>
<span class="co">#ans&gt; Proportion of Variance  0.387  0.237  0.122  0.110 0.0604 0.0426 0.0251 0.0152</span>
<span class="co">#ans&gt; Cumulative Proportion   0.387  0.624  0.746  0.857 0.9171 0.9597 0.9848 1.0000</span>
<span class="co">#ans&gt;                          Comp.9</span>
<span class="co">#ans&gt; Standard deviation     6.75e-03</span>
<span class="co">#ans&gt; Proportion of Variance 5.07e-06</span>
<span class="co">#ans&gt; Cumulative Proportion  1.00e+00</span>

<span class="co"># Scree plot - the variance of each component</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">pca</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="Machine-Learning_files/figure-html/unnamed-chunk-170-1.png" width="70%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb53"><pre class="downlit sourceCode r">
<code class="sourceCode R">
<span class="co"># With connected lines - useful for looking for the "elbow"</span>
<span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">pca</span>, type <span class="op">=</span> <span class="st">"l"</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="Machine-Learning_files/figure-html/unnamed-chunk-170-2.png" width="70%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb54"><pre class="downlit sourceCode r">
<code class="sourceCode R">
<span class="co"># PC1 and PC2 -- These are the weights of the variables on the first two Principal Components</span>
<span class="va">pca</span><span class="op">$</span><span class="va">loadings</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">2</span><span class="op">]</span>
<span class="co">#ans&gt;       Comp.1  Comp.2</span>
<span class="co">#ans&gt; Agr  0.52379  0.0536</span>
<span class="co">#ans&gt; Min  0.00132  0.6178</span>
<span class="co">#ans&gt; Man -0.34750  0.3551</span>
<span class="co">#ans&gt; Pow -0.25572  0.2611</span>
<span class="co">#ans&gt; Con -0.32518  0.0513</span>
<span class="co">#ans&gt; Ser -0.37892 -0.3502</span>
<span class="co">#ans&gt; Fin -0.07437 -0.4537</span>
<span class="co">#ans&gt; Soc -0.38741 -0.2215</span>
<span class="co">#ans&gt; Tra -0.36682  0.2026</span></code></pre></div>
<p>Based on the weights of the variables on the PCs shown above, we can extract the following interpretation:</p>
<ul>
<li>PC1 is roughly a linear combination of <code>Agr</code>, with <em>negative</em> weight, and (<code>Man</code>, <code>Pow</code>, <code>Con</code>, <code>Ser</code>, <code>Soc</code>, <code>Tra</code>), with <em>positive</em> weights. So it can be interpreted as an <em>indicator</em> of the kind of economy of the country: agricultural (negative values) or industrial (positive values).</li>
<li>PC2 has <em>negative</em> weights on (<code>Min</code>, <code>Man</code>, <code>Pow</code>, <code>Tra</code>) and <em>positive</em> weights in (<code>Ser</code>, <code>Fin</code>, <code>Soc</code>). It can be interpreted as the contrast between relatively large or small service sectors. So it tends to be negative in communist countries and positive in capitalist countries.</li>
</ul>
<div class="rmdtip">
<p>
The interpretation of the PCs involves inspecting the weights and
interpreting the linear combination of the original variables, which
might be separating between two clear characteristics of the data
</p>
</div>
<p>To conclude, let’s see how we can represent our original data into a plot called <em>biplot</em> that summarizes all the analysis for two PCs.</p>
<div class="sourceCode" id="cb55"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Biplot - plot together the scores for PC1 and PC2 and the</span>
<span class="co"># variables expressed in terms of PC1 and PC2</span>
<span class="fu"><a href="https://rdrr.io/r/stats/biplot.html">biplot</a></span><span class="op">(</span><span class="va">pca</span><span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="Machine-Learning_files/figure-html/unnamed-chunk-172-1.png" width="90%" style="display: block; margin: auto;"></div>
<p>This biplot confirms the above interpretation.</p>
<p>Recall that the biplot is a superposition of two figures: the first one is the scatterplot of the principal component scores (the projections of the observations on the new low dimensional space - also know as graph of individuals) and the second one is the figure of loading vectors (also known as graph of variables). In fact, the graph of variables is a correlation circle, the variables whose unit vectors are close to each other are said to be positively correlated, meaning that their influence on the positioning of individuals is similar (again, these proximities are reflected in the projections of variables on the graph of individuals). However, variables far away from each other will be defined as being negatively correlated. Variables that have perpendicular unit vector are uncorrelated. To see both graphs we can use the <code>factoextra</code> package like follows:</p>
<div class="sourceCode" id="cb56"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># loading factoextra library</span>
<span class="kw"><a href="https://rdrr.io/r/base/library.html">require</a></span><span class="op">(</span><span class="va"><a href="http://www.sthda.com/english/rpkgs/factoextra">factoextra</a></span><span class="op">)</span>
<span class="co"># Calculating pca using prcomp(), which is a built-in function in R </span>
<span class="va">res.pca</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/prcomp.html">prcomp</a></span><span class="op">(</span><span class="va">eurojob</span>, scale <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>

<span class="co"># Graph of individuals</span>
<span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/fviz_pca.html">fviz_pca_ind</a></span><span class="op">(</span><span class="va">res.pca</span>,
             col.ind <span class="op">=</span> <span class="st">"contrib"</span>, <span class="co"># Color by their contribution to axes</span>
             gradient.cols <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"#00AFBB"</span>, <span class="st">"#E7B800"</span>, <span class="st">"#FC4E07"</span><span class="op">)</span>,
             repel <span class="op">=</span> <span class="cn">TRUE</span>     
             <span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="Machine-Learning_files/figure-html/unnamed-chunk-173-1.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb57"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Graph of variables</span>
<span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/fviz_pca.html">fviz_pca_var</a></span><span class="op">(</span><span class="va">res.pca</span>,
             col.var <span class="op">=</span> <span class="st">"contrib"</span>, 
             gradient.cols <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"#00AFBB"</span>, <span class="st">"#E7B800"</span>, <span class="st">"#FC4E07"</span><span class="op">)</span>,
             repel <span class="op">=</span> <span class="cn">TRUE</span>     
             <span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="Machine-Learning_files/figure-html/unnamed-chunk-173-2.png" width="90%" style="display: block; margin: auto;"></div>
<div class="sourceCode" id="cb58"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span class="co"># Biplot of individuals and variables</span>
<span class="fu"><a href="https://rdrr.io/pkg/factoextra/man/fviz_pca.html">fviz_pca_biplot</a></span><span class="op">(</span><span class="va">res.pca</span>, repel <span class="op">=</span> <span class="cn">TRUE</span>,
                col.var <span class="op">=</span> <span class="st">"#2E9FDF"</span>, 
                col.ind <span class="op">=</span> <span class="st">"#696969"</span>  
                <span class="op">)</span></code></pre></div>
<div class="inline-figure"><img src="Machine-Learning_files/figure-html/unnamed-chunk-173-3.png" width="90%" style="display: block; margin: auto;"></div>
<p align="right">
◼
</p>

</div>
</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="pw-5.html">PW 5</a></div>
<div class="next"><a href="pw-6.html">PW 6</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#principal-components-analysis"><span class="header-section-number">6</span> Principal Components Analysis</a></li>
<li><a class="nav-link" href="#introduction-3"><span class="header-section-number">6.1</span> Introduction</a></li>
<li>
<a class="nav-link" href="#principal-components"><span class="header-section-number">6.2</span> Principal Components</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#notations-and-procedure">Notations and Procedure</a></li>
<li><a class="nav-link" href="#first-principal-component-textpc_1-y_1">First Principal Component (\(\text{PC}_1\)): \(Y_1\)</a></li>
<li><a class="nav-link" href="#second-principal-component-textpc_2-y_2">Second Principal Component (\(\text{PC}_2\)): \(Y_2\)</a></li>
<li><a class="nav-link" href="#ith-principal-component-textpc_i-y_i">\(i^{th}\) Principal Component (\(\text{PC}_i\)): \(Y_i\)</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#how-do-we-find-the-coefficients"><span class="header-section-number">6.3</span> How do we find the coefficients?</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#why-it-may-be-possible-to-reduce-dimensions">Why It May Be Possible to Reduce Dimensions</a></li>
<li><a class="nav-link" href="#procedure">Procedure</a></li>
</ul>
</li>
<li><a class="nav-link" href="#standardization-of-the-features"><span class="header-section-number">6.4</span> Standardization of the features</a></li>
<li>
<a class="nav-link" href="#projection-of-the-data"><span class="header-section-number">6.5</span> Projection of the data</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#scores">Scores</a></li>
<li><a class="nav-link" href="#visualization">Visualization</a></li>
<li><a class="nav-link" href="#extra">Extra</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#case-study"><span class="header-section-number">6.6</span> Case study</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#employement-in-european-countries-in-the-late-70s">Employement in European countries in the late 70s</a></li></ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Machine Learning</strong>" was written by Mohamad Ghassany. It was last built on 2022-04-26.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
