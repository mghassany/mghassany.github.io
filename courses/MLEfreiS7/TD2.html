<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.90">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>Machine Learning - Lab2: Logistic Regression &amp; Regularization</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./TD3.html" rel="next">
<link href="./TD1.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script><script async="" src="https://hypothes.is/embed.js"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><link rel="stylesheet" href="mycss.css">
</head>
<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"><div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title">Lab2: Logistic Regression &amp; Regularization</h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header sidebar-header-stacked">
      <a href="./" class="sidebar-logo-link">
      <img src="./img/logo_efrei_assas_white.png" alt="" class="sidebar-logo py-0 d-lg-inline d-none"></a>
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Machine Learning</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-color-scheme-toggle sidebar-tool" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
  <a href="" class="quarto-reader-toggle sidebar-tool" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Overview</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./TD1.html" class="sidebar-item-text sidebar-link">Lab1: Linear Regression</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./TD2.html" class="sidebar-item-text sidebar-link active">Lab2: Logistic Regression &amp; Regularization</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./TD3.html" class="sidebar-item-text sidebar-link">Lab3: Decision Trees and Random Forests</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./TD4.html" class="sidebar-item-text sidebar-link">Lab4: Neural Networks</a>
  </div>
</li>
    </ul>
</div>
</nav><!-- margin-sidebar --><div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li>
<a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression">Logistic Regression</a>
  <ul class="collapse">
<li><a href="#visualizing-the-data" id="toc-visualizing-the-data" class="nav-link" data-scroll-target="#visualizing-the-data">Visualizing the data</a></li>
  <li><a href="#implementation" id="toc-implementation" class="nav-link" data-scroll-target="#implementation">Implementation</a></li>
  <li><a href="#learning-the-parameters" id="toc-learning-the-parameters" class="nav-link" data-scroll-target="#learning-the-parameters">Learning the parameters</a></li>
  <li><a href="#plotting-the-decision-boundary" id="toc-plotting-the-decision-boundary" class="nav-link" data-scroll-target="#plotting-the-decision-boundary">Plotting the decision boundary</a></li>
  <li><a href="#evaluating-logistic-regression" id="toc-evaluating-logistic-regression" class="nav-link" data-scroll-target="#evaluating-logistic-regression">Evaluating logistic regression</a></li>
  </ul>
</li>
  <li>
<a href="#regularized-logistic-regression" id="toc-regularized-logistic-regression" class="nav-link" data-scroll-target="#regularized-logistic-regression">Regularized logistic regression</a>
  <ul class="collapse">
<li><a href="#feature-mapping" id="toc-feature-mapping" class="nav-link" data-scroll-target="#feature-mapping">Feature mapping</a></li>
  <li><a href="#cost-function-and-gradient" id="toc-cost-function-and-gradient" class="nav-link" data-scroll-target="#cost-function-and-gradient">Cost function and gradient</a></li>
  <li><a href="#learning-parameters-using-scipy.optimize.minimize" id="toc-learning-parameters-using-scipy.optimize.minimize" class="nav-link" data-scroll-target="#learning-parameters-using-scipy.optimize.minimize">Learning parameters using <code>scipy.optimize.minimize</code></a></li>
  <li><a href="#extra" id="toc-extra" class="nav-link" data-scroll-target="#extra">Extra</a></li>
  </ul>
</li>
  </ul></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<h1 class="title d-none d-lg-block">Lab2: Logistic Regression &amp; Regularization</h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header><!-- ```{css, echo=FALSE} --><!-- #sidebar h2 { --><!--     background-color: #077BBF; --><!-- } --><!-- #postamble { --><!--     color: #fcfcfc; --><!--     background: #077BBF; --><!--     border-top: solid 10px #077BBF; --><!-- } --><!-- #sidebar { --><!--     background: #077bbfcc; --><!-- } --><!-- #sidebar a { --><!--     color: #fff; --><!-- } --><!-- h1, h2, h3, h4, h5, h6, legend { --><!--     color: #077BBF; --><!-- } --><!-- ``` --><!-- 

:::{.alert .alert-block .alert-warning}
**Important** After spending ~8 hours (class work hours) on the first lab (TP1). **The estimated work time on this lab is 4 hours**. You do not need to work on it at home unless you do not finish it during 4 spent hours in class. 
:::
 --><section id="introduction" class="level2"><h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>In this lab, you will implement logistic regression and apply it to two different datasets.</p>
<p>Before we begin with the exercises, we need to import all libraries required for this programming exercise. Throughout the course, we will be using <a href="http://www.numpy.org/"><code>numpy</code></a> for all arrays and matrix operations, and <a href="https://matplotlib.org/"><code>matplotlib</code></a> for plotting. In this assignment, we will also use <a href="https://docs.scipy.org/doc/scipy/reference/"><code>scipy</code></a>, which contains scientific and numerical computation functions and tools.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Scientific and vector computation for python</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Plotting library</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Optimization module in scipy</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> optimize</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># do not forget to tell matplotlib to embed plots within the notebook</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">

</div>
</section><section id="logistic-regression" class="level2"><h2 class="anchored" data-anchor-id="logistic-regression">Logistic Regression</h2>
<p>In this part of the exercise, you will build a logistic regression model to predict whether a student gets admitted into a university. Suppose that you are the administrator of a university department and you want to determine each applicant’s chance of admission based on their results on two exams. You have historical data from previous applicants that you can use as a training set for logistic regression. For each training example, you have the applicant’s scores on two exams and the admissions decision.</p>
<p>Your task is to build a classification model that estimates an applicant’s probability of admission based the scores from those two exams.</p>
<p><strong>1.</strong> Load the data <code>tp2data1.txt</code> from <a href="datasets/tp2data1.txt" target="_blank">here <svg aria-hidden="true" role="img" viewbox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M464 32H48C21.49 32 0 53.49 0 80v352c0 26.51 21.49 48 48 48h416c26.51 0 48-21.49 48-48V80c0-26.51-21.49-48-48-48zM224 416H64v-96h160v96zm0-160H64v-96h160v96zm224 160H288v-96h160v96zm0-160H288v-96h160v96z"></path></svg></a> using the <code>loadtxt()</code> from <code>numpy</code>. The first two columns contains the exam scores and the third column contains the label. Then separate the features from label. Name the feature matrix <code>X</code> and the label <code>y</code>.</p>
<div class="cell">

</div>
<div class="cell">

</div>
<p><strong>2.</strong> Print the first 5 samples from <code>X</code> and <code>y</code>.</p>
<section id="visualizing-the-data" class="level3"><h3 class="anchored" data-anchor-id="visualizing-the-data">Visualizing the data</h3>
<p><strong>3.</strong> Display the data on a 2-dimensional plot where the axes are the two exam scores, and the positive and negative examples are shown with different colors (or markers).</p>
<p>You should produce something like this:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="TD2_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<div class="cell">

</div>
</section><section id="implementation" class="level3"><h3 class="anchored" data-anchor-id="implementation">Implementation</h3>
<p><strong>Sigmoid function</strong></p>
<p>Recall that the logistic regression hypothesis is defined as:</p>
<p><span class="math display">\[ h_\omega(x) = g(\omega^T x)\]</span></p>
<p>where function <span class="math inline">\(g\)</span> is the sigmoid function. The sigmoid function is defined as:</p>
<p><span class="math display">\[g(z) = \frac{1}{1+e^{-z}}\]</span></p>
<p><strong>4.</strong> Implement the <code>sigmoid</code> function so it can be called by the rest of your program. When you are finished, try testing a few values by calling <code>sigmoid(x)</code> in a new cell. For large positive values of <code>x</code>, the sigmoid should be close to 1, while for large negative values, the sigmoid should be close to 0. Evaluating <code>sigmoid(0)</code> should give you exactly 0.5. Your code should also work with vectors and matrices. <strong>For a matrix, your function should perform the sigmoid function on every element.</strong></p>
<p><strong>5.</strong> Plot the sigmoid function, like this:</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="TD2_files/figure-html/unnamed-chunk-7-3.png" class="img-fluid" width="672"></p>
</div>
</div>
<div class="cell">

</div>
<div class="cell">

</div>
<p><strong>Cost function and gradient</strong></p>
<p><strong>6.</strong> Before proceeding, add the intercept term to <code>X</code>. (<strong>hint</strong>: you can use <code>np.concatenate</code> or <code>np.hstack</code>)</p>
<div class="cell">

</div>
<p><strong>7.</strong> Implement the <em>cost function</em> and its <em>gradient</em> for logistic regression.</p>
<p>Recall that the cost function for logistic regression is</p>
<p><span class="math display">\[ J(\omega) = \frac{1}{m} \sum_{i=1}^{m} \left[ -y^{(i)} \log\left(h_\omega\left( x^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - h_\omega\left( x^{(i)} \right) \right) \right]\]</span></p>
<p>Recall that the gradient of the cost is a vector of the same length as <span class="math inline">\(\omega\)</span> where the <span class="math inline">\(j^{th}\)</span> element (for <span class="math inline">\(j = 0, 1, \cdots , n\)</span>) is defined as follows:</p>
<p><span class="math display">\[ \frac{\partial J(\omega)}{\partial \omega_j} = \frac{1}{m} \sum_{i=1}^m \left( h_\omega \left( x^{(i)} \right) - y^{(i)} \right) x_j^{(i)} \]</span></p>
<div class="cell">

</div>
<p><strong>8.</strong> Call your implemented function using two test cases for <span class="math inline">\(\omega\)</span>. You should see that the cost is about 0.693 for <span class="math inline">\(\omega = (0,0,0)\)</span>.</p>
<div class="cell">

</div>
<div class="cell">

</div>
</section><section id="learning-the-parameters" class="level3"><h3 class="anchored" data-anchor-id="learning-the-parameters">Learning the parameters</h3>
<p><strong>Learning parameters using your implemented Gradient Descent</strong></p>
<p><strong>9.</strong> Implement the gradient descent algorithm for logistic regression: write a cost function and calculate its gradient, then take a gradient descent step accordingly in order to find the optimal parameters. Run it on the training set. Print the results (the parameters values and the cost function).</p>
<p><strong>Learning parameters using <code>scipy.optimize</code></strong></p>
<p>In this part you will use the <a href="https://docs.scipy.org/doc/scipy/reference/optimize.html"><code>scipy.optimize</code> module</a>. SciPy is a numerical computing library for <code>python</code>. It provides an optimization module for root finding and minimization. As of <code>scipy 1.0</code>, the function <code>scipy.optimize.minimize</code> is the method to use for optimization problems(both constrained and unconstrained).</p>
<p>For logistic regression, you want to optimize the cost function <span class="math inline">\(J(\omega)\)</span> with parameters <span class="math inline">\(\omega\)</span>. Concretely, you are going to use <code>optimize.minimize</code> to find the best parameters <span class="math inline">\(\omega\)</span> for the logistic regression cost function, given a fixed dataset (of X and y values). You will pass to <code>optimize.minimize</code> the following inputs:</p>
<ul>
<li>
<code>costFunction</code>: A cost function that, when given the training set and a particular <span class="math inline">\(\omega\)</span>, computes the logistic regression cost and gradient with respect to <span class="math inline">\(\omega\)</span> for the dataset (X, y). It is important to note that we only pass the name of the function without the parenthesis. This indicates that we are only providing a reference to this function, and not evaluating the result from this function.</li>
<li>
<code>initial_omega</code>: The initial values of the parameters we are trying to optimize.</li>
<li>
<code>(X, y)</code>: These are additional arguments to the cost function.</li>
<li>
<code>jac</code>: Indication if the cost function returns the Jacobian (gradient) along with cost value. (True)</li>
<li>
<code>method</code>: Optimization method/algorithm to use</li>
<li>
<code>options</code>: Additional options which might be specific to the specific optimization method. In the following, we only tell the algorithm the maximum number of iterations before it terminates.</li>
</ul>
<p>If you have calculated the cost function correctly, <code>optimize.minimize</code> will converge on the right optimization parameters and return the final values of the cost and <span class="math inline">\(\omega\)</span> in a class object. Notice that by using <code>optimize.minimize</code>, you did not have to write any loops yourself, or set a learning rate like you did for gradient descent. This is all done by <code>optimize.minimize</code>: you only needed to provide a function calculating the cost and the gradient.</p>
<p>In the following, a code written to call <code>optimize.minimize</code> with the correct arguments.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># set options for optimize.minimize</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>options<span class="op">=</span> {<span class="st">'maxiter'</span>: <span class="dv">400</span>}</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># see documention for scipy's optimize.minimize  for description about</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co"># the different parameters</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># The function returns an object `OptimizeResult`</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co"># We use truncated Newton algorithm for optimization which is </span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co"># equivalent to MATLAB's fminunc</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># See https://stackoverflow.com/questions/18801002/fminunc-alternate-in-numpy</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co"># initial_omega = np.array([0, 0, 0])</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> optimize.minimize(costFunction,</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>                        initial_omega,</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>                        (X_train, y),</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>                        jac<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>                        method<span class="op">=</span><span class="st">'TNC'</span>,</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>                        options<span class="op">=</span>options)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="co"># the fun property of `OptimizeResult` object returns</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="co"># the value of costFunction at optimized omega</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>cost <span class="op">=</span> res.fun</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="co"># the optimized omega is in the x property</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>omega <span class="op">=</span> res.x</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Print omega to screen</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Cost at omega found by optimize.minimize: </span><span class="sc">{:.3f}</span><span class="st">'</span>.<span class="bu">format</span>(cost))</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Expected cost (approx): 0.203</span><span class="ch">\n</span><span class="st">'</span>)<span class="op">;</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'omega:'</span>)</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'</span><span class="ch">\t</span><span class="st">[</span><span class="sc">{:.3f}</span><span class="st">, </span><span class="sc">{:.3f}</span><span class="st">, </span><span class="sc">{:.3f}</span><span class="st">]'</span>.<span class="bu">format</span>(<span class="op">*</span>omega))</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Expected omega (approx):</span><span class="ch">\n\t</span><span class="st">[-25.161, 0.206, 0.201]'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>10.</strong> Run the code above on your cost function and verify that you obtain the correct (or almost) values.</p>
<div class="cell">

</div>
</section><section id="plotting-the-decision-boundary" class="level3"><h3 class="anchored" data-anchor-id="plotting-the-decision-boundary">Plotting the decision boundary</h3>
<p>Since the decision boundary of logistic regression is a linear (<em>you know that right?</em>) and the dimension of the feature space here is 2, the decision boundary in this 2-dimensional space is a line that separates the predicted classes “0” and “1”.</p>
<p>For logistic regression, we predict <span class="math inline">\(y=1\)</span> if <span class="math inline">\(\omega^T X \geq 0\)</span> (right side of the line) and <span class="math inline">\(y=0\)</span> if <span class="math inline">\(\omega^T X \lt 0\)</span> (left side of the line). Where</p>
<p><span class="math display">\[
\omega = \begin{pmatrix} \omega_0 \\ \omega_1 \\ \omega2 \end{pmatrix} \,\, \text{and} \,\, X = \begin{pmatrix}
  1 \\
  X_1 \\
  X_2
  \end{pmatrix}
\]</span></p>
<p>So we predict <span class="math inline">\(y=1\)</span> if <span class="math inline">\(\omega_0 + \omega_1 X_1 + \omega_2 X_2 \geq 0\)</span> which means that the equation of the decision boundary (a line here) is <span class="math inline">\(X_2 = - \frac{\omega_1}{\omega_2}X_1 - \frac{\omega_0}{\omega_2}\)</span></p>
<p><strong>11.</strong> Plot the decision boundary obtained with logistic regression.</p>
</section><section id="evaluating-logistic-regression" class="level3"><h3 class="anchored" data-anchor-id="evaluating-logistic-regression">Evaluating logistic regression</h3>
<p><strong>12.</strong> After learning the parameters, you can use the model to predict whether a particular student will be admitted. For a student with an Exam 1 score of 45 and an Exam 2 score of 85, you should expect to see an admission probability of 0.776. Another way to evaluate the quality of the parameters we have found is to see how well the learned model predicts on our training set. Write a function <code>predict</code> that will produce “1” or “0” predictions given a dataset and a learned parameter vector <span class="math inline">\(\omega\)</span>.</p>
<p><strong>13.</strong> Calculate the confusion matrix, and use it to calculate the training accuracy of your classifier and the F1 score.</p>
<p><strong>14.</strong> In order to verify that your line (decision boundary) is well plotted, color the points on the last Figure with respect to the predicted response.</p>
<p><strong>15.</strong> Now make the same plot but color the points with respect to their real labels. From this figure, count the number of the false positive predictions.</p>
<p><strong>plotDecisionBoundary function</strong></p>
<p>For the rest, use the following function (or modify it to adapt it) for plotting the decision boundary.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plotDecisionBoundary(omega, X, y):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    x_min, x_max <span class="op">=</span> X[:, <span class="dv">0</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">1</span>, X[:, <span class="dv">0</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    y_min, y_max <span class="op">=</span> X[:, <span class="dv">1</span>].<span class="bu">min</span>() <span class="op">-</span> <span class="dv">1</span>, X[:, <span class="dv">1</span>].<span class="bu">max</span>() <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    xx, yy <span class="op">=</span> np.meshgrid(np.arange(x_min, x_max, <span class="fl">0.1</span>),</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>                         np.arange(y_min, y_max, <span class="fl">0.1</span>))</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    X_plot <span class="op">=</span> np.c_[xx.ravel(), yy.ravel()]</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    X_plot <span class="op">=</span> np.hstack((np.ones((X_plot.shape[<span class="dv">0</span>], <span class="dv">1</span>)), X_plot))</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    y_plot <span class="op">=</span> np.dot(X_plot, omega).reshape(xx.shape)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    plt.figure()</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    plt.scatter(X[y <span class="op">==</span> <span class="dv">1</span>][:, <span class="dv">0</span>], X[y <span class="op">==</span> <span class="dv">1</span>][:, <span class="dv">1</span>], label<span class="op">=</span><span class="st">"Admitted"</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    plt.scatter(X[y <span class="op">==</span> <span class="dv">0</span>][:, <span class="dv">0</span>], X[y <span class="op">==</span> <span class="dv">0</span>][:, <span class="dv">1</span>], label<span class="op">=</span><span class="st">"Not admitted"</span>)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    plt.contour(xx, yy, y_plot, levels<span class="op">=</span>[<span class="dv">0</span>])</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">"Exam 1 score"</span>)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">"Exam 2 score"</span>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>plotDecisionBoundary(res.x, X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>0.20365864300942144</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[-24.13930679   0.19805053   0.19320881]</code></pre>
</div>
<div class="cell-output-display">
<p><img src="TD2_files/figure-html/unnamed-chunk-18-5.png" class="img-fluid" width="672"></p>
</div>
</div>
</section></section><section id="regularized-logistic-regression" class="level2"><h2 class="anchored" data-anchor-id="regularized-logistic-regression">Regularized logistic regression</h2>
<p>In this part of the exercise, you will implement regularized logistic regression to predict whether microchips from a fabrication plant passes quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly.</p>
<p>Suppose you are the product manager of the factory and you have the test results for some microchips on two different tests. From these two tests, you would like to determine whether the microchips should be accepted or rejected. To help you make the decision, you have a dataset of test results on past microchips, from which you can build a logistic regression model.</p>
<p><strong>2.1.</strong> Load the data in file <code>tp2data2.txt</code> from from <a href="datasets/tp2data2.txt" target="_blank">here <svg aria-hidden="true" role="img" viewbox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M464 32H48C21.49 32 0 53.49 0 80v352c0 26.51 21.49 48 48 48h416c26.51 0 48-21.49 48-48V80c0-26.51-21.49-48-48-48zM224 416H64v-96h160v96zm0-160H64v-96h160v96zm224 160H288v-96h160v96zm0-160H288v-96h160v96z"></path></svg></a>. Separate the features from the labels in two differents objects.</p>
<div class="cell">

</div>
<p><strong>2.2.</strong> Visualize the data in a 2-dimensional plot. Color the point with respect to their labels. What do you notice ?</p>
<div class="cell">
<div class="cell-output-display">
<p><img src="TD2_files/figure-html/unnamed-chunk-20-7.png" class="img-fluid" width="672"></p>
</div>
</div>
<section id="feature-mapping" class="level3"><h3 class="anchored" data-anchor-id="feature-mapping">Feature mapping</h3>
<p>One way to fit the data better is to create more features from each data point. In the function <code>mapFeature</code> defined below, we will map the features into all polynomial terms of <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> up to the <strong>sixth</strong> power.</p>
<p><span class="math display">\[\text{mapFeature}(x) = \begin{bmatrix} 1 &amp; x_1 &amp; x_2 &amp; x_1^2 &amp; x_1 x_2 &amp; x_2^2 &amp; x_1^3 &amp; \dots &amp; x_1 x_2^5 &amp; x_2^6 \end{bmatrix}^T\]</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mapFeature(X1, X2, degree<span class="op">=</span><span class="dv">6</span>):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Maps the two input features to quadratic features used in the regularization exercise.</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns a new feature array with more features, comprising of</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co">    X1, X2, X1.^2, X2.^2, X1*X2, X1*X2.^2, etc..</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co">    X1 : array_like</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="co">        A vector of shape (m, 1), containing one feature for all examples.</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="co">    X2 : array_like</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="co">        A vector of shape (m, 1), containing a second feature for all examples.</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="co">        Inputs X1, X2 must be the same size.</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="co">    degree: int, optional</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="co">        The polynomial degree.</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a><span class="co">    -------</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="co">    : array_like</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a><span class="co">        A matrix of of m rows, and columns depend on the degree of polynomial.</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> X1.ndim <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> [np.ones(X1.shape[<span class="dv">0</span>])]</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> [np.ones(<span class="dv">1</span>)]</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, degree <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(i <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>            out.append((X1 <span class="op">**</span> (i <span class="op">-</span> j)) <span class="op">*</span> (X2 <span class="op">**</span> j))</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> X1.ndim <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.stack(out, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.array(out)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As a result of this mapping, our vector of two features (the scores on two QA tests) has been transformed into a 28-dimensional vector. A logistic regression classifier trained on this higher-dimension feature vector will have a more complex decision boundary and will appear nonlinear when drawn in our 2-dimensional plot.</p>
<p>While the feature mapping allows us to build a more expressive classifier, it also more susceptible to overfitting.</p>
<p>In the next parts of the exercise, you will implement regularized logistic regression to fit the data and also see for yourself how regularization can help combat the overfitting problem.</p>
<p><strong>2.3.</strong> Apply the <code>mapFeature()</code> function on the dataset. Verify that you get a 28-dimensional vector.</p>
<div class="cell">

</div>
</section><section id="cost-function-and-gradient" class="level3"><h3 class="anchored" data-anchor-id="cost-function-and-gradient">Cost function and gradient</h3>
<p>Recall that the regularized cost function in logistic regression is</p>
<p><span class="math display">\[ J(\omega) = \frac{1}{m} \sum_{i=1}^m \left[ -y^{(i)}\log \left( h_\omega \left(x^{(i)} \right) \right) - \left( 1 - y^{(i)} \right) \log \left( 1 - h_\omega \left( x^{(i)} \right) \right) \right] + \frac{\lambda}{2m} \sum_{j=1}^n \omega_j^2 \]</span></p>
<p>Note that we do not regularize the parameters <span class="math inline">\(\omega_0\)</span>. The gradient of the cost function is a vector where the <span class="math inline">\(j^{th}\)</span> element is defined as follows:</p>
<p><span class="math display">\[ \frac{\partial J(\omega)}{\partial \omega_0} = \frac{1}{m} \sum_{i=1}^m \left( h_\omega \left(x^{(i)}\right) - y^{(i)} \right) x_j^{(i)} \qquad \text{for } j =0 \]</span></p>
<p><span class="math display">\[ \frac{\partial J(\omega)}{\partial \omega_j} = \left( \frac{1}{m} \sum_{i=1}^m \left( h_\omega \left(x^{(i)}\right) - y^{(i)} \right) x_j^{(i)} \right) + \frac{\lambda}{m}\omega_j \qquad \text{for } j \ge 1 \]</span></p>
<p><strong>2.4.</strong> Complete the function <code>costFunctionReg()</code> below. This function computes and returns the cost function and gradient for regularized logistic regression.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> costFunctionReg(omega, X, y, lambda_):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute cost and gradient for logistic regression with regularization.</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Parameters</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co">    ----------</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co">    omega : array_like</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="co">        Logistic regression parameters. A vector with shape (n, ). n is </span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="co">        the number of features including any intercept. If we have mapped</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="co">        our initial features into polynomial features, then n is the total </span></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="co">        number of polynomial features. </span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="co">    X : array_like</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="co">        The data set with shape (m x n). m is the number of examples, and</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="co">        n is the number of features (after feature mapping).</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="co">    y : array_like</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a><span class="co">        The data labels. A vector with shape (m, ).</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="co">    lambda_ : float</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a><span class="co">        The regularization parameter. </span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns</span></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a><span class="co">    -------</span></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a><span class="co">    J : float</span></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a><span class="co">        The computed value for the regularized cost function. </span></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a><span class="co">    grad : array_like</span></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a><span class="co">        A vector of shape (n, ) which is the gradient of the cost</span></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a><span class="co">        function with respect to omega, at the current values of omega.</span></span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a><span class="co">    Instructions</span></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a><span class="co">    ------------</span></span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute the cost `J` of a particular choice of omega.</span></span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute the partial derivatives and set `grad` to the partial</span></span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a><span class="co">    derivatives of the cost w.r.t. each parameter in omega.</span></span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize some useful values</span></span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> y.size  <span class="co"># number of training examples</span></span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># You need to return the following variables correctly </span></span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>    J <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>    grad <span class="op">=</span> np.zeros(omega.shape)</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ===================== YOUR CODE HERE ======================</span></span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># =============================================================</span></span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> J, grad</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">

</div>
<p><strong>2.5.</strong> Once you are done with the <code>costFunctionReg</code>, call it using the initial value of <span class="math inline">\(\omega\)</span> (initialized to all zeros), and also another test case where <span class="math inline">\(\omega\)</span> is all ones. The code is given below with the expected values. You should obtain the same values.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize fitting parameters</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co"># X here has 28 columns</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>initial_omega <span class="op">=</span> np.zeros(X.shape[<span class="dv">1</span>])</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Set regularization parameter lambda to 1</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co"># DO NOT use `lambda` as a variable name in python</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co"># because it is a python keyword</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>lambda_ <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute and display initial cost and gradient for regularized logistic</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co"># regression</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>cost, grad <span class="op">=</span> costFunctionReg(initial_omega, X, y, lambda_)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Cost at initial omega (zeros): </span><span class="sc">{:.3f}</span><span class="st">'</span>.<span class="bu">format</span>(cost))</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Expected cost (approx)       : 0.693</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Gradient at initial omega (zeros) - first five values only:'</span>)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'</span><span class="ch">\t</span><span class="st">[</span><span class="sc">{:.4f}</span><span class="st">, </span><span class="sc">{:.4f}</span><span class="st">, </span><span class="sc">{:.4f}</span><span class="st">, </span><span class="sc">{:.4f}</span><span class="st">, </span><span class="sc">{:.4f}</span><span class="st">]'</span>.<span class="bu">format</span>(<span class="op">*</span>grad[:<span class="dv">5</span>]))</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Expected gradients (approx) - first five values only:'</span>)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'</span><span class="ch">\t</span><span class="st">[0.0085, 0.0188, 0.0001, 0.0503, 0.0115]</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute and display cost and gradient</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="co"># with all-ones omega and lambda = 10</span></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>test_omega <span class="op">=</span> np.ones(X.shape[<span class="dv">1</span>])</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>cost, grad <span class="op">=</span> costFunctionReg(test_omega, X, y, <span class="dv">10</span>)</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'------------------------------</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Cost at test omega    : </span><span class="sc">{:.2f}</span><span class="st">'</span>.<span class="bu">format</span>(cost))</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Expected cost (approx): 3.16</span><span class="ch">\n</span><span class="st">'</span>)</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Gradient at test omega - first five values only:'</span>)</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'</span><span class="ch">\t</span><span class="st">[</span><span class="sc">{:.4f}</span><span class="st">, </span><span class="sc">{:.4f}</span><span class="st">, </span><span class="sc">{:.4f}</span><span class="st">, </span><span class="sc">{:.4f}</span><span class="st">, </span><span class="sc">{:.4f}</span><span class="st">]'</span>.<span class="bu">format</span>(<span class="op">*</span>grad[:<span class="dv">5</span>]))</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Expected gradients (approx) - first five values only:'</span>)</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'</span><span class="ch">\t</span><span class="st">[0.3460, 0.1614, 0.1948, 0.2269, 0.0922]'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="learning-parameters-using-scipy.optimize.minimize" class="level3"><h3 class="anchored" data-anchor-id="learning-parameters-using-scipy.optimize.minimize">Learning parameters using <code>scipy.optimize.minimize</code>
</h3>
<p><strong>2.6.</strong> Use <code>optimize.minimize</code> to learn the optimal parameters <span class="math inline">\(\omega\)</span>.</p>
<div class="cell">

</div>
<p><strong>Plotting the decision boundary</strong></p>
<p><strong>2.7.</strong> To visualize the model learned by this classifier use the function <code>plotDecisionBoundary</code> to plot the (non-linear) decision boundary that separates the positive and negative examples.</p>
<div class="alert alert-block alert-warning">
<p>In <code>plotDecisionBoundary</code>, we plot the non-linear decision boundary by computing the classifier’s predictions on an evenly spaced grid and then and draw a contour plot where the predictions change from y = 0 to y = 1.</p>
</div>
<p>You should obtain something like this:</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>&lt;string&gt;:37: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.</code></pre>
</div>
<div class="cell-output-display">
<p><img src="TD2_files/figure-html/unnamed-chunk-27-9.png" class="img-fluid" width="672"></p>
</div>
</div>
</section><section id="extra" class="level3"><h3 class="anchored" data-anchor-id="extra">Extra</h3>
<p>Try out different regularization parameters for the dataset to understand how regularization prevents overfitting.</p>
<p>Notice the changes in the decision boundary as you vary <span class="math inline">\(\lambda\)</span>. With a small <span class="math inline">\(\lambda\)</span>, you should find that the classifier gets almost every training example correct, but draws a very complicated boundary, thus overfitting the data.</p>
<p><strong>Credits</strong></p>
<div class="alert alert-block alert-warning">
<p>This lab is hugely inspired from Andrew Ng’s Machine Learning course. Supplementary material from <a href="https://github.com/dibgerge/ml-coursera-python-assignments"><code>dibgerge</code></a>’s github was used.</p>
</div>


</section></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./TD1.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Lab1: Linear Regression</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./TD3.html" class="pagination-link">
        <span class="nav-page-text">Lab3: Decision Trees and Random Forests</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>