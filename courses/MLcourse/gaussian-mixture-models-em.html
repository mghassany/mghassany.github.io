<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>8 Gaussian Mixture Models &amp; EM | Machine Learning</title>
<meta name="author" content="Mohamad Ghassany">
<meta name="description" content="In the previous chapter we saw the \(k\)-means algorithm which is considered as a hard clustering technique, such that each point is allocated to only one cluster. In \(k\)-means, a cluster is...">
<meta name="generator" content="bookdown 0.24.10 with bs4_book()">
<meta property="og:title" content="8 Gaussian Mixture Models &amp; EM | Machine Learning">
<meta property="og:type" content="book">
<meta property="og:description" content="In the previous chapter we saw the \(k\)-means algorithm which is considered as a hard clustering technique, such that each point is allocated to only one cluster. In \(k\)-means, a cluster is...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="8 Gaussian Mixture Models &amp; EM | Machine Learning">
<meta name="twitter:description" content="In the previous chapter we saw the \(k\)-means algorithm which is considered as a hard clustering technique, such that each point is allocated to only one cluster. In \(k\)-means, a cluster is...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<link href="libs/vembedr-0.1.5/css/vembedr.css" rel="stylesheet">
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script><script src="libs/plotly-binding-4.10.0/plotly.js"></script><script src="libs/typedarray-0.1/typedarray.min.js"></script><link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet">
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script><link href="libs/plotly-htmlwidgets-css-2.5.1/plotly-htmlwidgets.css" rel="stylesheet">
<script src="libs/plotly-main-2.5.1/plotly-latest.min.js"></script><script type="text/x-mathjax-config">
    const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
    for (let popover of popovers){
      const div = document.createElement('div');
      div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
      div.innerHTML = popover.getAttribute('data-content');
      
      // Will this work with TeX on its own line?
      var has_math = div.querySelector("span.math");
      if (has_math) {
        document.body.appendChild(div);
      	MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
      	MathJax.Hub.Queue(function(){
          popover.setAttribute('data-content', div.innerHTML);
      	})
      }
    }
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="css/ims-style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Machine Learning</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="introduction.html">Introduction</a></li>
<li class="book-part">Supervised Learning</li>
<li class="book-part">Regression</li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">1</span> Linear Regression</a></li>
<li><a class="" href="practical-work-1.html">Practical Work 1</a></li>
<li><a class="" href="multiple-linear-regression.html"><span class="header-section-number">2</span> Multiple Linear Regression</a></li>
<li><a class="" href="pw-2.html">PW 2</a></li>
<li class="book-part">Classification</li>
<li><a class="" href="logistic-regression.html"><span class="header-section-number">3</span> Logistic Regression</a></li>
<li><a class="" href="pw-3.html">PW 3</a></li>
<li><a class="" href="discriminant-analysis.html"><span class="header-section-number">4</span> Discriminant Analysis</a></li>
<li><a class="" href="pw-4.html">PW 4</a></li>
<li><a class="" href="decision-trees-random-forests.html"><span class="header-section-number">5</span> Decision Trees &amp; Random Forests</a></li>
<li><a class="" href="pw-5.html">PW 5</a></li>
<li class="book-part">Dimensionality Reduction</li>
<li><a class="" href="principal-components-analysis.html"><span class="header-section-number">6</span> Principal Components Analysis</a></li>
<li><a class="" href="pw-6.html">PW 6</a></li>
<li class="book-part">Unsupervised Learning</li>
<li><a class="" href="kmeans-hierarchical-clustering.html"><span class="header-section-number">7</span> Kmeans &amp; Hierarchical Clustering</a></li>
<li><a class="" href="pw-7.html">PW 7</a></li>
<li><a class="active" href="gaussian-mixture-models-em.html"><span class="header-section-number">8</span> Gaussian Mixture Models &amp; EM</a></li>
<li><a class="" href="pw-8.html">PW 8</a></li>
<li class="book-part">Hackathon</li>
<li><a class="" href="hackathon.html">Hackathon</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="final-grades.html"><span class="header-section-number">A</span> Final Grades</a></li>
<li><a class="" href="app-introRStudio.html"><span class="header-section-number">B</span> Introduction to RStudio</a></li>
<li><a class="" href="app-ht.html"><span class="header-section-number">C</span> Review on hypothesis testing</a></li>
<li><a class="" href="use-qual.html"><span class="header-section-number">D</span> Use of qualitative predictors</a></li>
<li><a class="" href="model-selection.html"><span class="header-section-number">E</span> Model Selection</a></li>
<li><a class="" href="references-and-credits.html"><span class="header-section-number">F</span> References and Credits</a></li>
<li><a class="" href="other-references.html"><span class="header-section-number">G</span> Other References</a></li>
<li><a class="" href="main-references-credits.html">Main References &amp; Credits</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="gaussian-mixture-models-em" class="section level1" number="8">
<h1>
<span class="header-section-number">8</span> Gaussian Mixture Models &amp; EM<a class="anchor" aria-label="anchor" href="#gaussian-mixture-models-em"><i class="fas fa-link"></i></a>
</h1>
<!-- https://www.youtube.com/watch?v=qMTuMa86NzU -->
<!-- To label equation use \begin{equation} and (\#eq:normaldist1) -->
<!-- To cite equation use \@ref(eq:normal01) -->
<p>In the previous chapter we saw the <span class="math inline">\(k\)</span>-means algorithm which is considered as a hard clustering technique, such that each point is allocated to only one cluster. In <span class="math inline">\(k\)</span>-means, a cluster is described only by its centroid. This is not too flexible, as we may have problems with clusters that are <em>overlapping</em>, or ones that are not of <em>circular shape</em>.</p>
<p>In this chapter, we will introduce a model-based clustering technique, which is <strong>E</strong>xpectation <strong>M</strong>aximization (EM). We will apply it using Gaussian Mixture Models (GMM).</p>
<p>With EM Clustering, we can go a step further and describe each cluster by its centroid (mean), covariance (so that we can have elliptical clusters), and weight (the size of the cluster). The probability that a point belongs to a cluster is now given by a multivariate Gaussian probability distribution (multivariate - depending on multiple variables). That also means that we can calculate the probability of a point being under a Gaussian ‘bell’, i.e. the probability of a point belonging to a cluster. A comparison between the performances of <span class="math inline">\(k\)</span>-means and EM clustering on an artificial dataset is shown in Figure <a href="gaussian-mixture-models-em.html#fig:ClusterAnalysisMouse">8.1</a>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:ClusterAnalysisMouse"></span>
<img src="img/ClusterAnalysis_Mouse.png" alt="Comparison of $k$-means and EM on artificial data called [Mouse](https://elki-project.github.io/datasets/) dataset. Using the Variances, the EM algorithm can describe the normal distributions exact, while $k$-means splits the data in [Voronoi](https://bit.ly/1rVyJkt)-Cells." width="80%"><p class="caption">
Figure 8.1: Comparison of <span class="math inline">\(k\)</span>-means and EM on artificial data called <a href="https://elki-project.github.io/datasets/">Mouse</a> dataset. Using the Variances, the EM algorithm can describe the normal distributions exact, while <span class="math inline">\(k\)</span>-means splits the data in <a href="https://bit.ly/1rVyJkt">Voronoi</a>-Cells.
</p>
</div>
<p>We start this chapter by reminding what is a Gaussian distribution, then introduce the Mixture of Gaussians and finish by explaining the Expectation-Maximization algorithm.</p>
<div id="the-gaussian-distribution" class="section level2" number="8.1">
<h2>
<span class="header-section-number">8.1</span> The Gaussian distribution<a class="anchor" aria-label="anchor" href="#the-gaussian-distribution"><i class="fas fa-link"></i></a>
</h2>
<p>The Gaussian, also known as the normal distribution, is a widely used
model for the distribution of continuous variables. In the case of a
single variable <span class="math inline">\(x\)</span>, the Gaussian distribution can be written in the
form</p>
<p><span class="math display" id="eq:normaldist1">\[\begin{equation}
\mathcal{N}(x|m,\sigma^2)=\frac{1}{(2 \pi \sigma^2 )^{1/2}} \exp \left\lbrace - \frac{1}{2 \sigma^2} (x-m)^2\right\rbrace
\tag{8.1}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(m\)</span> is the mean and <span class="math inline">\(\sigma^2\)</span> is the variance.</p>
<p>For a <span class="math inline">\(D\)</span>-dimensional vector <span class="math inline">\(X\)</span>, the multivariate Gaussian distribution
take the form</p>
<p><span class="math display" id="eq:normaldist2">\[\begin{equation}
\mathcal{N}(X|\mu,\Sigma)=\frac{1}{(2 \pi)^{D/2}} \frac{1}{|\Sigma|^{1/2}} \exp \left\lbrace - \frac{1}{2} (X-\mu)^T \Sigma^{-1} (X-\mu) \right\rbrace
\tag{8.2}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mu\)</span> is a <span class="math inline">\(D\)</span>-dimensional mean vector, <span class="math inline">\(\Sigma\)</span> is a <span class="math inline">\(D\times D\)</span>
covariance matrix, and <span class="math inline">\(|\Sigma|\)</span> denotes the determinant of <span class="math inline">\(\Sigma\)</span>.</p>
<!-- ```{r} -->
<!-- #          Normal Distribution PDF -->
<!-- #range -->
<!-- x=seq(-5,5,length=300) -->
<!-- #plot each curve -->
<!-- plot(x,dnorm(x,mean=0,sd=sqrt(.2)),type="l",lwd=2,col="blue",main='Normal Distribution PDF',xlim=c(-5,5),ylim=c(0,1),xlab='X', -->
<!-- ylab=latex2exp::TeX('$N(x|m,\\sigma^2)$')) -->
<!-- curve(dnorm(x,mean=0,sd=1), add=TRUE,type="l",lwd=2,col="red") -->
<!-- curve(dnorm(x,mean=0,sd=sqrt(5)), add=TRUE,type="l",lwd=2,col="brown") -->
<!-- curve(dnorm(x,mean=-2,sd=sqrt(.5)), add=TRUE,type="l",lwd=2,col="green") -->
<!-- legend(3,0.9,inset = 0, -->
<!--         legend = latex2exp::TeX(c("$\\mu = 0$, $\\sigma^2 = 0.2$", "$\\mu = 0$, $\\sigma^2 = 1$","$\\mu = 0$, $\\sigma^2 = 5.0$","$\\mu = -2.0$, $\\sigma^2 = 0.5$")),  -->
<!--         col=c("blue","red","brown","green"), lty=1:4, cex=.5, horiz = TRUE) -->
<!-- ``` -->
<p>The Gaussian distribution arises in many different contexts and can be motivated from a variety of different perspectives. For example, when we consider the sum of multiple random variables. The <em>central limit theorem</em> (due to Laplace) tells us that, subject to certain mild conditions, the sum of a set of random variables, which is of course itself a random variable, has a distribution that becomes increasingly Gaussian as the number of terms in the sum increases.</p>
</div>
<div id="mixture-of-gaussians" class="section level2" number="8.2">
<h2>
<span class="header-section-number">8.2</span> Mixture of Gaussians<a class="anchor" aria-label="anchor" href="#mixture-of-gaussians"><i class="fas fa-link"></i></a>
</h2>
<p>While the Gaussian distribution has some important analytical properties, it suffers from
significant limitations when it comes to modeling real data sets.
Consider the example shown in Figure <a href="gaussian-mixture-models-em.html#fig:gaussianfaithful">8.2</a> applied on the
’Old Faithful’ data set, this data set comprises 272 measurements of the eruption of the <a href="https://en.wikipedia.org/wiki/Old_Faithful">Old Faithful geyser</a> at Yellowstone National Park in the USA. Each measurement comprises the duration of the eruption in minutes (horizontal axis) and the time in minutes to the next eruption (vertical axis). We see that the data set forms two dominant clumps, and that a simple Gaussian distribution is unable to capture this structure, whereas a linear superposition of two Gaussians gives a better characterization of the
data set. Such superpositions, formed by taking linear combinations of
more basic distributions such as Gaussians, can be formulated as
probabilistic models known as <em>mixture distributions</em>.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:gaussianfaithful"></span>
<img src="img/gaussian_faithful.png" alt="Plots of the ’old faithful’ data in which the blue curves show contours of constant probability density. On the left is a single Gaussian ditribution which has been fitted to the data using maximum likelihood. On the right the distribution is given by a linear combination of two Gaussians which has been fitted to the data by maximum likelihood using the EM technique, and which gives a better representation of the data" width="80%"><p class="caption">
Figure 8.2: Plots of the ’old faithful’ data in which the blue curves show contours of constant probability density. On the left is a single Gaussian ditribution which has been fitted to the data using maximum likelihood. On the right the distribution is given by a linear combination of two Gaussians which has been fitted to the data by maximum likelihood using the EM technique, and which gives a better representation of the data
</p>
</div>
<p>In Figure <a href="gaussian-mixture-models-em.html#fig:gaussianmixture">8.3</a> we see that a linear combination of Gaussians can give rise to very complex densities. By using a sufficient number of Gaussians, and by adjusting their means and covariances as well as the coefficients in the linear combination, almost any continuous density can be approximated to arbitrary accuracy.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:gaussianmixture"></span>
<img src="img/gaussian_mixture.png" alt="Example of a Gaussian mixture distribution in one dimension showing three Gaussians (each scaled by a coefficient) in blue and their sum in red." width="70%"><p class="caption">
Figure 8.3: Example of a Gaussian mixture distribution in one dimension showing three Gaussians (each scaled by a coefficient) in blue and their sum in red.
</p>
</div>
<p>We therefore consider a superposition of <span class="math inline">\(K\)</span> Gaussian densities of the
form</p>
<p><span class="math display">\[\label{eq:gaussian}
p(x)= \sum_{k=1}^{K} \pi_k \mathcal{N}(x| \mu_k, \Sigma_k)\]</span></p>
<p>which is called a <em>mixture of Gaussians</em>. Each Gaussian density
<span class="math inline">\(\mathcal{N}(x| \mu_k, \Sigma_k)\)</span> is called a <em>component</em> of the mixture
and has its own mean <span class="math inline">\(\mu_k\)</span> and covariance <span class="math inline">\(\Sigma_k\)</span>.</p>
<p>The parameters <span class="math inline">\(\pi_k\)</span> are called <em>mixing coefficients</em>. They verify the
conditions
<span class="math display">\[\sum_{k=1}^{K} \pi_k = 1 \quad \text{and} \quad 0 \leq \pi_k \leq 1\]</span></p>
<p>In order to find an equivalent formulation of the Gaussian mixture
involving an explicit <strong>latent</strong> variable, we introduce a
<span class="math inline">\(K\)</span>-dimensional binary random variable <span class="math inline">\(z\)</span> having a 1-of-<span class="math inline">\(K\)</span>
representation in which a particular element <span class="math inline">\(z_k\)</span> is equal to 1 and all
other elements are equal to 0. The values of <span class="math inline">\(z_k\)</span> therefore satisfy
<span class="math inline">\(z_k \in \{0,1\}\)</span> and <span class="math inline">\(\sum_k z_k =1\)</span>, and we see that there are <span class="math inline">\(K\)</span>
possible states for the vector <span class="math inline">\(z\)</span> according to which element is
nonzero. The marginal distribution over <span class="math inline">\(z\)</span> is specified in terms of the
mixing coefficients <span class="math inline">\(\pi_k\)</span> , such that <span class="math display">\[p(z_k=1)=\pi_k\]</span></p>
<div class="rmdtip">
<p>
A <a href="https://en.wikipedia.org/wiki/Latent_variable">latent
variable</a> is a variable that is not directly measurable, but its
value can be inferred by taking other measurements.
</p>
<p>
This happens a lot in machine learning, robotics, statistics and
other fields. For example, you may not be able to directly quantify
intelligence (it’s not a countable thing like the number of brain cells
you have), but we think it exists and we can run experiments that may
tell us about intelligence. So your intelligence is a latent variable
that affects your performance on multiple tasks even though it can not
be directly measured (<a href="https://www.quora.com/What-is-a-latent-variable">link</a>).
</p>
</div>
<p>The conditional distribution of <span class="math inline">\(x\)</span> given a particular value for <span class="math inline">\(z\)</span> is
a Gaussian</p>
<p><span class="math display">\[p(x|z_k=1)= \mathcal{N}(x| \mu_k, \Sigma_k)\]</span></p>
<p>The joint distribution is given by <span class="math inline">\(p(z)p(x|z)\)</span>, and the marginal
distribution of <span class="math inline">\(x\)</span> is then obtained by summing the joint distribution
over all possible states of <span class="math inline">\(z\)</span> to give</p>
<p><span class="math display">\[p(x)= \sum_z p(z)p(x|z) = \sum_{k=1}^{K} \pi_k \mathcal{N}(x| \mu_k, \Sigma_k)\]</span></p>
<p>Now, we are able to work with the joint distribution <span class="math inline">\(p(x|z)\)</span> instead of
the marginal distribution <span class="math inline">\(p(x)\)</span>. This leads to significant
simplification, most notably through the introduction of the
Expectation-Maximization (EM) algorithm.</p>
<p>Another quantity that play an important role is the conditional
probability of <span class="math inline">\(z\)</span> given <span class="math inline">\(x\)</span>. We shall use <span class="math inline">\(r(z_k)\)</span> to denote
<span class="math inline">\(p(z_k = 1|x)\)</span>, whose value can be found using Bayes’ theorem</p>
<p><span class="math display" id="eq:responsibilities">\[\begin{align}
r(z_k)= p(z_k = 1|x) &amp;= \frac{ p(z_k = 1) p(x|(z_k=1)}{\displaystyle \sum_{j=1}^{K} p(z_j = 1) p(x|(z_j=1)} \nonumber \\
&amp;= \frac{\pi_k \mathcal{N}(x| \mu_k, \Sigma_k)}{\displaystyle \sum_{j=1}^{K} \pi_j \mathcal{N}(x| \mu_j, \Sigma_j)}
\tag{8.3}
\end{align}\]</span></p>
<p>We shall view <span class="math inline">\(\pi_k\)</span> as the prior probability of <span class="math inline">\(z_k = 1\)</span>, and the
quantity <span class="math inline">\(r(z_k)\)</span> as the corresponding posterior probability once we
have observed <span class="math inline">\(x\)</span>. As we shall see in next section, <span class="math inline">\(r(z_k)\)</span> can also be
viewed as the <em>responsibility</em> that component <span class="math inline">\(k\)</span> takes for ’explaining’
the observation <span class="math inline">\(x\)</span>.</p>
<div class="rmdtip">
<p>
Doesn’t this reminds you of the Equation <a href="discriminant-analysis.html#eq:bayes">4.1</a>
when we used Bayes’ theorm for Classification? where <span class="math inline"><span class="math inline">\(p_k(x)\)</span></span> was the <em>posterior</em>
probability that an observation <span class="math inline"><span class="math inline">\(X=x\)</span></span>
belongs to <span class="math inline"><span class="math inline">\(k\)</span></span>-th class. The
difference is that here the data is unlabled (we have no class), so we
create a <strong>latent</strong> (hidden, unobserved) variable <span class="math inline"><span class="math inline">\(z\)</span></span> that will play a similar role.
</p>
</div>
<p>In Figure <a href="gaussian-mixture-models-em.html#fig:gaussianmixture500samples">8.4</a> the role of the responsibilities is illustrated on a sample of 500 points drawn from a mixture of three Gaussians.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:gaussianmixture500samples"></span>
<img src="img/gaussianmixture_500samples.png" alt="Example of 500 points drawn from a mixture of 3 Gaussians. (a) Samples from the joint distribution $p(z)p(x|z)$ in which the three states of $z$, corresponding to the three components of the mixture, are depicted in red, green, and blue, and (b) the corresponding samples from the marginal distribution $p(x)$, which is obtained by simply ignoring the values of $z$ and just plotting the $x$ values. The data set in (a) is said to be complete, whereas that in (b) is incomplete. (c) The same samples in which the colours represent the value of the responsibilities $r(z_{nk})$ associated with data point $x_n$, obtained by plotting the corresponding point using proportions of red, blue, and green ink given by $r(z_{nk})$ for $k = 1,2,3$, respectively." width="90%"><p class="caption">
Figure 8.4: Example of 500 points drawn from a mixture of 3 Gaussians. (a) Samples from the joint distribution <span class="math inline">\(p(z)p(x|z)\)</span> in which the three states of <span class="math inline">\(z\)</span>, corresponding to the three components of the mixture, are depicted in red, green, and blue, and (b) the corresponding samples from the marginal distribution <span class="math inline">\(p(x)\)</span>, which is obtained by simply ignoring the values of <span class="math inline">\(z\)</span> and just plotting the <span class="math inline">\(x\)</span> values. The data set in (a) is said to be complete, whereas that in (b) is incomplete. (c) The same samples in which the colours represent the value of the responsibilities <span class="math inline">\(r(z_{nk})\)</span> associated with data point <span class="math inline">\(x_n\)</span>, obtained by plotting the corresponding point using proportions of red, blue, and green ink given by <span class="math inline">\(r(z_{nk})\)</span> for <span class="math inline">\(k = 1,2,3\)</span>, respectively.
</p>
</div>
<p>So the form of the Gaussian mixture distribution is governed by the parameters <span class="math inline">\(\pi\)</span>, <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\Sigma\)</span>, where we have used the notation <span class="math inline">\(\pi=\{\pi_1,\ldots,\pi_K\}\)</span>, <span class="math inline">\(\mu=\{\mu_1,\ldots,\mu_K\}\)</span> and <span class="math inline">\(\Sigma=\{\Sigma_1,\ldots,\Sigma_K\}\)</span>. One way to set the values of these parameters is to use maximum likelihood. The log of the likelihood function is given by</p>
<p><span class="math display">\[\ln p(X|\pi,\mu,\Sigma)=\sum_{n=1}^{N} \ln \left\lbrace \sum_{k=1}^{K} \pi_k \mathcal{N}(x_n | \mu_k, \Sigma_k) \right\rbrace\]</span>
We immediately see that the situation is now much more complex than with a single Gaussian, due to the presence of the summation over <span class="math inline">\(k\)</span> inside the logarithm. As a result, the maximum likelihood solution for the
parameters no longer has a closed-form analytical solution. One approach to maximizing the likelihood function is to use iterative numerical optimization techniques. Alternatively we can employ a powerful framework called <strong>E</strong>xpectation <strong>M</strong>aximization, which
will be discussed in this chapter.</p>
</div>
<div id="em-for-gaussian-mixtures" class="section level2" number="8.3">
<h2>
<span class="header-section-number">8.3</span> EM for Gaussian Mixtures<a class="anchor" aria-label="anchor" href="#em-for-gaussian-mixtures"><i class="fas fa-link"></i></a>
</h2>
<p>Suppose we have a data set of observations <span class="math inline">\(\{x_1, \ldots, x_N\}\)</span>, which
gives a data set <span class="math inline">\(X\)</span> of size <span class="math inline">\(N \times D\)</span>, and we wish to model this data using a mixture of
Gaussians. Similarly, the corresponding latent variable are denoted by
a <span class="math inline">\(N \times K\)</span> matrix <span class="math inline">\(Z\)</span> with rows <span class="math inline">\(z_n^K\)</span>.</p>
<div class="rmdtip">
<p>
Recall that the objective is to estimate the parameters <span class="math inline"><span class="math inline">\(\pi\)</span></span>, <span class="math inline"><span class="math inline">\(\mu\)</span></span> and <span class="math inline"><span class="math inline">\(\Sigma\)</span></span> in order to estimate the posterior
probabilities (named also <em>responsibilities</em>, called <span class="math inline"><span class="math inline">\(\, r(z_k)\)</span></span> in this chapter). To do so, we
find the estimators that maximize the log of the likelihood
function.
</p>
</div>
<p>If we assume that the data points are i.i.d. (independent and
identically distributed), then we can calculate the log of the
likelihood function, which is given by</p>
<p><span class="math display" id="eq:gaussianlikelihood">\[\begin{equation}
\tag{8.4}
\ln p(X|\pi,\mu,\Sigma)=\sum_{n=1}^{N} \ln \left\lbrace \sum_{k=1}^{K} \pi_k \mathcal{N}(x_n | \mu_k, \Sigma_k) \right\rbrace
\end{equation}\]</span></p>
<p>An elegant and powerful method for finding maximum likelihood solutions
for this models with latent variables is called the
<strong>E</strong>xpectation <strong>M</strong>aximization algorithm, or EM algorithm.</p>
<p>Setting the derivatives of <span class="math inline">\(\ln p(X|\pi,\mu,\Sigma)\)</span> in
<a href="gaussian-mixture-models-em.html#eq:gaussianlikelihood">(8.4)</a> respectively with respect to the
<span class="math inline">\(\mu_k,\Sigma_k\)</span> and <span class="math inline">\(\pi_k\)</span> to zero, we obtain</p>
<p><span class="math display" id="eq:means">\[\begin{equation}
\tag{8.5}
\mu_k = \frac{1}{N_k} \sum_{n=1}^{N} r(z_{nk}) x_n
\end{equation}\]</span></p>
<p>where we define <span class="math display">\[N_k= \sum_{n=1}^{N}r(z_{nk})\]</span></p>
<p>We can interpret <span class="math inline">\(N_k\)</span> as the effective number of points assigned to cluster <span class="math inline">\(k\)</span>. Note
carefully the form of this solution. We see that the mean <span class="math inline">\(\mu_k\)</span> for the <span class="math inline">\(k\)</span>-th Gaussian
component is obtained by taking a weighted mean of all of the points in the data set,
in which the weighting factor for data point <span class="math inline">\(x_n\)</span> is given by the posterior probability
<span class="math inline">\(r(z_{nk})\)</span> that component <span class="math inline">\(k\)</span> was responsible for generating <span class="math inline">\(x_n\)</span>.</p>
<p>As for <span class="math inline">\(\sigma_k\)</span> we obtain</p>
<p><span class="math display" id="eq:sigma">\[\begin{equation}
\tag{8.6}
\Sigma_k= \frac{1}{N_k} \sum_{n=1}^{N} r(z_{nk}) (x_n - \mu_k)(x_n - \mu_k)^T
\end{equation}\]</span></p>
<p>which has the same form as the corresponding result for a single Gaussian fitted to
the data set, but again with each data point weighted by the corresponding posterior probability and with the denominator given by the effective number of points
associated with the corresponding component.</p>
<p>Finally, for the mixing coefficients <span class="math inline">\(\pi_k\)</span> we obtain</p>
<p><span class="math display" id="eq:pi">\[\begin{equation}
\tag{8.7}
\pi_k=\frac{N_k}{N}
\end{equation}\]</span></p>
<p>so that the mixing coefficient for the <span class="math inline">\(k\)</span>-th component is given by the average responsibility which that component takes for explaining the data points.</p>
<p>We first choose some initial values for the means, covariances, and
mixing coefficients. Then we alternate between the following two updates
that we shall call the <strong>E</strong> step and the <strong>M</strong> step. In the <em>expectation</em> step,
or E step, we use the current values for the parameters to evaluate the
posterior probabilities, or responsibilities, given by Eq.
<a href="gaussian-mixture-models-em.html#eq:responsibilities">(8.3)</a>. We then use these probabilities in the
<em>maximization</em> step, or M step, to re-estimate the means, covariances,
and mixing coefficients using the results in Equations <a href="gaussian-mixture-models-em.html#eq:means">(8.5)</a>,
<a href="gaussian-mixture-models-em.html#eq:sigma">(8.6)</a> and <a href="gaussian-mixture-models-em.html#eq:pi">(8.7)</a>. The algorithm of EM for mixtures of
Gaussians is shown in the following Algorithm:</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="37%">
<col width="62%">
</colgroup>
<thead><tr class="header">
<th>The EM for Gaussian mixtures</th>
<th></th>
</tr></thead>
<tbody>
<tr class="odd">
<td>
<strong>Data</strong>:</td>
<td>
<span class="math inline">\(\mathbf{X}= \{x_{kd}, \,\,\,\, k=1,\ldots,N, d=1,\ldots,D\}\)</span>
where <span class="math inline">\(D\)</span> is the dimension of the feature space. <span class="math inline">\(Z\)</span> the latent variables matrix.</td>
</tr>
<tr class="even">
<td>
<strong>Result</strong>:</td>
<td>Posterior probabilities <span class="math inline">\(r(z_{nk})\)</span> and the model parameters <span class="math inline">\(\mu,\Sigma\)</span> and <span class="math inline">\(\pi\)</span>.</td>
</tr>
<tr class="odd">
<td>
<strong>Initialization</strong>:</td>
<td><ul>
<li>Choose a value for <span class="math inline">\(K\)</span>, <span class="math inline">\(1 &lt; K &lt; N\)</span>.</li>
<li>Initialize the means <span class="math inline">\(\mu_k\)</span>, the covariances <span class="math inline">\(\Sigma_k\)</span> and mixing coefficients <span class="math inline">\(\pi_k\)</span> randomly.</li>
<li>Evaluate the initial value of the log likelihood.</li>
</ul></td>
</tr>
<tr class="even">
<td>
<strong>Learning</strong>: <strong>repeat</strong>
</td>
<td>
<p><strong>E step</strong>:</p>
<ul>
<li>Evaluate the responsibilities using the current parameter values:
<span class="math display">\[r(z_{nk})= \frac{\pi_k \mathcal{N}(x| \mu_k, \Sigma_k)}{\displaystyle \sum_{j=1}^{K} \pi_j \mathcal{N}(x| \mu_j, \Sigma_j)}\]</span>
</li>
</ul>
<p><strong>M step</strong>:</p>
<ul>
<li><p>Re-estimate the parameters using the current responsibilities:
<span class="math display">\[\mu_k = \frac{1}{N_k} \sum_{n=1}^{N} r(z_{nk}) x_n\]</span>
<span class="math display">\[\Sigma_k= \frac{1}{N_k} \sum_{n=1}^{N} r(z_{nk}) (x_n - \mu_k)(x_n - \mu_k)^T\]</span>
<span class="math display">\[\pi_k=\frac{N_k}{N}\]</span>
<span class="math display">\[\text{where} \quad N_k= \sum_{n=1}^{N}r(z_{nk})\]</span></p></li>
<li><p>Evaluate the log likelihood:
<span class="math display">\[\ln p(X|\pi,\mu,\Sigma)=\sum_{n=1}^{N} \ln \left\lbrace \sum_{k=1}^{K} \pi_k \mathcal{N}(x_n | \mu_k, \Sigma_k) \right\rbrace\]</span>
Until convergence of either the parameters or the log likelihood. If the convergence criterion is not satisfied return to E step.</p></li>
</ul>
</td>
</tr>
</tbody>
</table></div>
<p>The EM algorithm for a mixture of two Gaussians applied to the rescaled
Old Faithful data set is illustrated in Figure <a href="gaussian-mixture-models-em.html#fig:emfaithful">8.5</a>. In
plot (a) we see the initial configuration, the Gaussian component are
shown as blue and red circles. Plot (b) shows the result of the initial
E step where we update the responsibilities. Plot (c) shows the M step
where we update the parameters. Plots (d), (e), and (f) show the results
after 2, 5, and 20 complete cycles of EM, respectively. In plot (f) the
algorithm is close to convergence.</p>
<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:emfaithful"></span>
<img src="img/em_faithful.png" alt="Illustration of the EM algorithm using the Old Faithful dataset. A mixture of two Gaussians is used." width="80%"><p class="caption">
Figure 8.5: Illustration of the EM algorithm using the Old Faithful dataset. A mixture of two Gaussians is used.
</p>
</div>
<!-- 
## The EM Algorithm in General

In this section, we present the general view of the EM algorithm. The
goal of the EM algorithm is to find maximum likelihood solutions for
models having latent variables. We denote $X$ the data matrix, $Z$ the
latent variables matrix. Let us denote $\theta$ the set of all model
parameters. Then the log likelihood function is given by

$$\ln p(X|\theta) = \ln \left\lbrace \sum_Z p(X,Z|\theta) \right\rbrace$$

Note that if the latent variables are continuous we get similar
equations, we only replace the over $Z$ with an integral.

The presence of the sum prevents the logarithm from acting directly on
the joint distribution, resulting in complicated expressions for the
maximum likelihood solution.

Suppose that, for each observation in $X$, we were told the
corresponding value of the latent variable $Z$. We shall call $\{X,Z\}$
the *complete* data set, and we shall refer to the actual observed data
$X$ as *incomplete*. The likelihood function for the complete data set
simply takes the form $\ln p(X,Z|\theta)$, and we shall suppose that
maximization of this complete-data log likelihood function is
straightforward.

**Initialization**:\
-Choose an initial setting for the parameters $\theta^{\text{old}}$.\
**Learning**: repeat\
Until convergence of either the parameters or the log likelihood. If the
convergence criterion is not satisfied then let $$\theta^{\text{old}}
\leftarrow \theta^{\text{new}}$$ and return to the E step.

In practice, however, we are not given the complete data set $\{X,Z\}$,
but only the incomplete data $X$. Our state of knowledge of the values
of the latent variables in $Z$ is given only by the posterior
distribution $p(Z|X, \theta)$. Because we cannot use the complete-data
log likelihood, we consider instead its expected value under the
posterior distribution of the latent variable, which corresponds to the
E step of the EM algorithm. In the subsequent M step, we maximize this
expectation. If the current estimate for the parameters is denoted
$\theta^{\text{old}}$, then a pair of successive E and M steps gives
rise to a revised estimate $\theta^{\text{new}}$. The algorithm is
initialized by choosing some starting value for the parameters
$\theta_0$.

In the E step, we use the current parameter values $\theta^{\text{old}}$
to find the posterior distribution of the latent variables given by
$p(Z|X, \theta^{\text{old}})$. We then use this posterior distribution
to find the expectation of the complete-data log likelihood evaluated
for some general parameter value $\theta$. This expectation, denoted
$\mathcal{Q}(\theta,\theta^{\text{old}})$, is given by

$$\label{eq:complete-likelihood}
 \mathcal{Q}(\theta,\theta^{\text{old}}) = \sum_Z p(Z|X, \theta^{\text{old}}) \ln p(X,Z|\theta)$$

In the M step, we determine the revised parameter estimate
$\theta^{\text{new}}$ by maximizing this function

$$\theta^{\text{new}} = \argmax_{\theta}  \mathcal{Q}(\theta,\theta^{\text{old}})$$

Note that in the definition of
$\mathcal{Q}(\theta,\theta^{\text{old}})$, the logarithm acts directly
on the joint distribution $p(X,Z|\theta)$, so the corresponding M-step
maximization will, by supposition, be tractable.

The general EM algorithm is summarized in Algorithm \[algo:em:general\].
It has the property that each cycle of EM will increase the
incomplete-data log likelihood (unless it is already at a local
maximum). -->
<div class="rmdtip">
<p>
Summary of this chapter:
</p>
<ul>
<li>
Gaussian Mixture Models (GMM) take a Gaussian and add another
Gaussian(s).
</li>
<li>
This allows to model more complex data.
</li>
<li>
We fit a GMM with the Expectation-Maximization (EM) algorithm.
</li>
<li>
Expectation-Maximization (EM) algorithm is a series of steps to find
good parameter estimates when there are latent variables.
</li>
<li>
EM steps:
<ol style="list-style-type: decimal">
<li>
Initialize t he parameter estimates.
</li>
<li>
Given the current parameter estimates, find the minimum log
likelihood for <span class="math inline"><span class="math inline">\(Z\)</span></span> (data + latent
variables).
</li>
<li>
Givent the current data, find better parameter estimates.
</li>
<li>
Repeat steps 2 &amp; 3.
</li>
</ol>
</li>
</ul>
</div>
<p align="right">
◼
</p>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="pw-7.html">PW 7</a></div>
<div class="next"><a href="pw-8.html">PW 8</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#gaussian-mixture-models-em"><span class="header-section-number">8</span> Gaussian Mixture Models &amp; EM</a></li>
<li><a class="nav-link" href="#the-gaussian-distribution"><span class="header-section-number">8.1</span> The Gaussian distribution</a></li>
<li><a class="nav-link" href="#mixture-of-gaussians"><span class="header-section-number">8.2</span> Mixture of Gaussians</a></li>
<li><a class="nav-link" href="#em-for-gaussian-mixtures"><span class="header-section-number">8.3</span> EM for Gaussian Mixtures</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Machine Learning</strong>" was written by Mohamad Ghassany. It was last built on 2022-04-26.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
