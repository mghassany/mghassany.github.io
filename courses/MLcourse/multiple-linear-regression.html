<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>2 Multiple Linear Regression | Machine Learning</title>
<meta name="author" content="Mohamad Ghassany">
<meta name="description" content="Simple linear regression is a useful approach for predicting a response on the basis of a single predictor variable. However, in practice we often have more than one predictor. In the previous...">
<meta name="generator" content="bookdown 0.24.10 with bs4_book()">
<meta property="og:title" content="2 Multiple Linear Regression | Machine Learning">
<meta property="og:type" content="book">
<meta property="og:description" content="Simple linear regression is a useful approach for predicting a response on the basis of a single predictor variable. However, in practice we often have more than one predictor. In the previous...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="2 Multiple Linear Regression | Machine Learning">
<meta name="twitter:description" content="Simple linear regression is a useful approach for predicting a response on the basis of a single predictor variable. However, in practice we often have more than one predictor. In the previous...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/kePrint-0.0.1/kePrint.js"></script><link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet">
<link href="libs/vembedr-0.1.5/css/vembedr.css" rel="stylesheet">
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script><script src="libs/plotly-binding-4.10.0/plotly.js"></script><script src="libs/typedarray-0.1/typedarray.min.js"></script><link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet">
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script><link href="libs/plotly-htmlwidgets-css-2.5.1/plotly-htmlwidgets.css" rel="stylesheet">
<script src="libs/plotly-main-2.5.1/plotly-latest.min.js"></script><script type="text/x-mathjax-config">
    const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
    for (let popover of popovers){
      const div = document.createElement('div');
      div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
      div.innerHTML = popover.getAttribute('data-content');
      
      // Will this work with TeX on its own line?
      var has_math = div.querySelector("span.math");
      if (has_math) {
        document.body.appendChild(div);
      	MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
      	MathJax.Hub.Queue(function(){
          popover.setAttribute('data-content', div.innerHTML);
      	})
      }
    }
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><link rel="stylesheet" href="css/ims-style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Machine Learning</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Welcome</a></li>
<li><a class="" href="introduction.html">Introduction</a></li>
<li class="book-part">Supervised Learning</li>
<li class="book-part">Regression</li>
<li><a class="" href="linear-regression.html"><span class="header-section-number">1</span> Linear Regression</a></li>
<li><a class="" href="practical-work-1.html">Practical Work 1</a></li>
<li><a class="active" href="multiple-linear-regression.html"><span class="header-section-number">2</span> Multiple Linear Regression</a></li>
<li><a class="" href="pw-2.html">PW 2</a></li>
<li class="book-part">Classification</li>
<li><a class="" href="logistic-regression.html"><span class="header-section-number">3</span> Logistic Regression</a></li>
<li><a class="" href="pw-3.html">PW 3</a></li>
<li><a class="" href="discriminant-analysis.html"><span class="header-section-number">4</span> Discriminant Analysis</a></li>
<li><a class="" href="pw-4.html">PW 4</a></li>
<li><a class="" href="decision-trees-random-forests.html"><span class="header-section-number">5</span> Decision Trees &amp; Random Forests</a></li>
<li><a class="" href="pw-5.html">PW 5</a></li>
<li class="book-part">Dimensionality Reduction</li>
<li><a class="" href="principal-components-analysis.html"><span class="header-section-number">6</span> Principal Components Analysis</a></li>
<li><a class="" href="pw-6.html">PW 6</a></li>
<li class="book-part">Unsupervised Learning</li>
<li><a class="" href="kmeans-hierarchical-clustering.html"><span class="header-section-number">7</span> Kmeans &amp; Hierarchical Clustering</a></li>
<li><a class="" href="pw-7.html">PW 7</a></li>
<li><a class="" href="gaussian-mixture-models-em.html"><span class="header-section-number">8</span> Gaussian Mixture Models &amp; EM</a></li>
<li><a class="" href="pw-8.html">PW 8</a></li>
<li class="book-part">Hackathon</li>
<li><a class="" href="hackathon.html">Hackathon</a></li>
<li class="book-part">Appendix</li>
<li><a class="" href="final-grades.html"><span class="header-section-number">A</span> Final Grades</a></li>
<li><a class="" href="app-introRStudio.html"><span class="header-section-number">B</span> Introduction to RStudio</a></li>
<li><a class="" href="app-ht.html"><span class="header-section-number">C</span> Review on hypothesis testing</a></li>
<li><a class="" href="use-qual.html"><span class="header-section-number">D</span> Use of qualitative predictors</a></li>
<li><a class="" href="model-selection.html"><span class="header-section-number">E</span> Model Selection</a></li>
<li><a class="" href="references-and-credits.html"><span class="header-section-number">F</span> References and Credits</a></li>
<li><a class="" href="other-references.html"><span class="header-section-number">G</span> Other References</a></li>
<li><a class="" href="main-references-credits.html">Main References &amp; Credits</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="multiple-linear-regression" class="section level1" number="2">
<h1>
<span class="header-section-number">2</span> Multiple Linear Regression<a class="anchor" aria-label="anchor" href="#multiple-linear-regression"><i class="fas fa-link"></i></a>
</h1>
<p>Simple linear regression is a useful approach for predicting a response on the basis of a single predictor variable. However, in practice we often have more than one predictor.
In the previous chapter, we took for example the prediction of housing prices considering we had the size of each house. We had a single feature <span class="math inline">\(X\)</span>, the size of the house. But now imagine if we had not only the size of the house as a feature but we also knew the number of bedrooms, the number of flours and the age of the house in years. It seems like this would give us a lot more information with which to predict the price.</p>
<div id="the-model" class="section level2" number="2.1">
<h2>
<span class="header-section-number">2.1</span> The Model<a class="anchor" aria-label="anchor" href="#the-model"><i class="fas fa-link"></i></a>
</h2>
<p>In general, suppose that we have <span class="math inline">\(p\)</span> distinct predictors. Then the multiple linear regression model takes the form</p>
<p><span class="math display">\[ Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p + \epsilon \]</span></p>
<p>where <span class="math inline">\(X_j\)</span> represents the <span class="math inline">\(j\)</span>th predictor and <span class="math inline">\(\beta_j\)</span> quantifies the association between that variable and the response. We interpret <span class="math inline">\(\beta_j\)</span> as the average effect on <span class="math inline">\(Y\)</span> of a one unit increase in <span class="math inline">\(X_j\)</span>, <em>holding all other predictors fixed</em>.</p>
<p>In matrix terms, supposing we have <span class="math inline">\(n\)</span> observations and <span class="math inline">\(p\)</span> variables, we need to define the following matrices:</p>
<p><span class="math display">\[\begin{equation}
\textbf{Y}_{n \times 1} = \begin{pmatrix}
    Y_{1} \\
    Y_{2} \\
    \vdots \\
    Y_{n}
\end{pmatrix}   \,\,\,\,\,\,\,\,\,\,\,\,  \textbf{X}_{n \times (p+1)}  = \begin{pmatrix}
    1      &amp; X_{11} &amp; X_{12} &amp; \dots  &amp; X_{1p} \\
    1      &amp; X_{21} &amp; X_{22} &amp; \dots  &amp; X_{2p} \\
    \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    1      &amp; X_{n1} &amp; X_{n2} &amp; \dots  &amp; X_{np}
\end{pmatrix}
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
{\mathbb{\beta}}_{(p+1) \times 1} = \begin{pmatrix}
    \beta_{0} \\
    \beta_{1} \\
    \vdots \\
    \beta_{p}
    \end{pmatrix}   \,\,\,\,\,\,\,\,\,\,\,\,  {\epsilon}_{n \times 1} = \begin{pmatrix}
        \epsilon_{1} \\
        \epsilon_{2} \\
        \vdots \\
        \epsilon_{n}
    \end{pmatrix}
\end{equation}\]</span></p>
<p>In matrix terms, the general linear regression model is</p>
<p><span class="math display">\[ \textbf{Y}_{n \times 1} = \textbf{X}_{n \times (p+1)} {\mathbb{\beta}}_{(p+1) \times 1} + {\epsilon}_{n \times 1} \]</span></p>
<p>where,</p>
<ul>
<li>
<span class="math inline">\(\textbf{Y}\)</span> is a vector of responses.</li>
<li>
<span class="math inline">\(\mathbb{\beta}\)</span> is a vector of parameters.</li>
<li>
<span class="math inline">\(\textbf{X}\)</span> is a matrix of constants.</li>
<li>
<span class="math inline">\(\epsilon\)</span> is a vector of independent <em>normal</em> (Gaussian) random variables.</li>
</ul>
</div>
<div id="estimating-the-regression-coefficients" class="section level2" number="2.2">
<h2>
<span class="header-section-number">2.2</span> Estimating the Regression Coefficients<a class="anchor" aria-label="anchor" href="#estimating-the-regression-coefficients"><i class="fas fa-link"></i></a>
</h2>
<p>As was the case in the simple linear regression setting, the regression coefficients <span class="math inline">\(\beta_{0}, \beta_{1}, \ldots, \beta_{p}\)</span> are unknown, and must be estimated. Given estimates <span class="math inline">\(\hat{\beta_{0}}, \hat{\beta_{1}}, \ldots, \hat{\beta_{p}}\)</span>, we can make predictions using the formula</p>
<p><span class="math display">\[ \hat{y} = \hat{\beta_{0}} + \hat{\beta_{1}} x_1 + \hat{\beta_{2}} x_2 + \ldots + \hat{\beta_{p}} x_p \]</span></p>
<p>We choose <span class="math inline">\(\beta_{0}, \beta_{1}, \ldots, \beta_{p}\)</span> to minimize the residual sum of squares</p>
<p><span class="math display">\[ \begin{aligned}
RSS &amp;= \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 \\
    &amp;= \sum_{i=1}^{n} (y_i - \hat{\beta_0} - \hat{\beta_1} {x}_{i1}  - \hat{\beta_2} {x}_{i2} - \ldots  -  \hat{\beta_p} {x}_{ip})^2 \\
\end{aligned}
\]</span></p>
<p>The values <span class="math inline">\(\hat{\beta_{0}}, \hat{\beta_{1}}, \ldots, \hat{\beta_{p}}\)</span> that minimize the RSS are the multiple least squares regression coefficient estimates, they are calculated using this formula (in matrix terms):</p>
<p><span class="math display">\[ \hat{\beta} = (\textbf{X}^T \textbf{X})^{-1}\textbf{X}^T \textbf{Y} \]</span></p>
<p>To obtain <span class="math inline">\(\hat{\beta}\)</span>, we can write the residual sum of squares as</p>
<p><span class="math display">\[ RSS = (\textbf{Y}-\textbf{X}\beta)^T(\textbf{Y}-\textbf{X}\beta) \]</span></p>
<p>This is a quadratic function in the <span class="math inline">\(p+1\)</span> parameters. Differentiating with respect to <span class="math inline">\(\beta\)</span> we obtain
<span class="math display">\[ \begin{aligned}
\frac{\partial RSS}{\partial \beta} &amp;= -2\textbf{X}^T(\textbf{Y}-\textbf{X}\beta) \\
\frac{\partial^2 RSS}{\partial \beta \partial \beta^T}   &amp;= 2\textbf{X}^T\textbf{X}.\\
\end{aligned}
\]</span></p>
<p>Assuming (for the moment) that <span class="math inline">\(\textbf{X}\)</span> has full column rank, and hence <span class="math inline">\(\textbf{X}^T\textbf{X}\)</span> is positive definite<a href="main-references-credits.html#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a>, we set the first derivative to zero</p>
<p><span class="math display">\[\textbf{X}^T(\textbf{Y}-\textbf{X}\beta)=0\]</span>
to obtain the unique solution</p>
<p><span class="math display">\[ \hat{\beta} = (\textbf{X}^T \textbf{X})^{-1}\textbf{X}^T \textbf{Y} \]</span></p>
<p>Note 1:</p>
<div class="rmdtip">
<p>It is a remarkable property of matrix algebra that the results for the general linear regression model in matrix notation appear exactly as those for the simple linear regression model. Only the degrees of freedom and other constants related to the number of <span class="math inline">\(X\)</span> variables and the dimensions of some matrices are different. Which means there are some similarities between <span class="math inline">\(\hat{\beta} = (\textbf{X}^T \textbf{X})^{-1}\textbf{X}^T \textbf{Y}\)</span> and <span class="math inline">\(\hat\beta_1=(s_x^2)^{-1}s_{xy}\)</span> from the simple linear model: both are related to the covariance between <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{Y}\)</span> weighted by the variance of <span class="math inline">\(\mathbf{X}\)</span>.</p>
</div>
<p>Note 2:</p>
<div class="rmdtip">
<p>If <span class="math inline">\(\textbf{X}^T \textbf{X}\)</span> is noninvertible, the common causes might be having:</p>
<ul>
<li>Redundant features, where two features are very closely related (i.e. they are linearly dependent)</li>
<li>Too many features (e.g. <span class="math inline">\(p \geq n\)</span>). In this case, we delete some features or we use “regularization” (to be, maybe, explained in a later lesson).</li>
</ul>
</div>
</div>
<div id="some-important-questions" class="section level2" number="2.3">
<h2>
<span class="header-section-number">2.3</span> Some important questions<a class="anchor" aria-label="anchor" href="#some-important-questions"><i class="fas fa-link"></i></a>
</h2>
<p>When we perform multiple linear regression, we usually are interested in answering a few important questions.</p>
<ol style="list-style-type: decimal">
<li>Is at least one of the predictors <span class="math inline">\(X_1 ,X_2 ,\ldots,X_p\)</span> useful in predicting the response?</li>
<li>Do all the predictors help to explain <span class="math inline">\(Y\)</span>, or is only a subset of the predictors useful?</li>
<li>How well does the model fit the data?</li>
<li>Given a set of predictor values, what response value should we predict, and how accurate is our prediction?</li>
</ol>
<p><strong><em>Relationship Between the Response and Predictors?</em></strong></p>
<p><strong><span class="math inline">\(F\)</span>-Statistic</strong></p>
<p>Recall that in the simple linear regression setting, in order to determine whether there is a relationship between the response and the predictor we can simply check whether <span class="math inline">\(\beta_1 = 0\)</span>. In the multiple regression setting with <span class="math inline">\(p\)</span> predictors, we need to ask whether all of the regression coefficients are zero, i.e. whether <span class="math inline">\(\beta_1 = \beta_2 = \ldots = \beta_p = 0\)</span>. As in the simple linear regression setting, we use a hypothesis test to answer this question. We test the null hypothesis,</p>
<p><span class="math display">\[ H_0 : \beta_1 = \beta_2 = \ldots = \beta_p = 0 \]</span></p>
<p>versus the alternative hypothesis</p>
<p><span class="math display">\[ H_1 : \text{at least one} \, \beta_j \, \text{is non-zero} \]</span></p>
<p>This hypothesis test is performed by computing the <span class="math inline">\(F\)</span>-statistic (<em>Fisher</em>):</p>
<p><span class="math display">\[ F = \frac{ (\text{TSS} - \text{RSS})/p}{\text{RSS}/(n-p-1)} \sim F_{p,n-p-1} \]</span></p>
<p>where, as with simple linear regression, <span class="math inline">\(\text{TSS} = \sum (y_i - \bar{y})^2\)</span> and <span class="math inline">\(\text{RSS} = \sum (y_i - \hat{y}_i)^2\)</span>.</p>
<p>Note that <span class="math inline">\(F_{p,n-p-1}\)</span> represents the <em>Fisher-Snedecor’s <span class="math inline">\(F\)</span> distribution</em> with <span class="math inline">\(p\)</span> and <span class="math inline">\(n-p-1\)</span> degrees of freedom. If <span class="math inline">\(H_0\)</span> is true, then <span class="math inline">\(F\)</span> is expected to be <em>small</em> since ESS<a href="main-references-credits.html#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a> will be close to zero (little variation is explained by the regression model since <span class="math inline">\(\hat{\boldsymbol{\beta}}\approx\mathbf{0}\)</span>).</p>
<p>So the question we ask here: <em>Is the whole regression explaining anything at all?</em> The answer comes from the <span class="math inline">\(F\)</span>-test in the ANOVA (ANalysis Of VAriance) table. This is what we get in an ANOVA table:</p>
<div class="inline-table"><table style="width:100%;" class="table table-sm">
<colgroup>
<col width="16%">
<col width="16%">
<col width="16%">
<col width="16%">
<col width="16%">
<col width="16%">
</colgroup>
<thead><tr class="header">
<th>Source</th>
<th>df</th>
<th>SS</th>
<th>MS</th>
<th><span class="math inline">\(F\)</span></th>
<th>p-value</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Factor (Explained)</td>
<td><span class="math inline">\(p\)</span></td>
<td>ESS=SSR</td>
<td>MSR=ESS/<span class="math inline">\((p)\)</span>
</td>
<td>F=MSR/MSE</td>
<td>p-value</td>
</tr>
<tr class="even">
<td>Error (Unexplained)</td>
<td><span class="math inline">\(n-p-1\)</span></td>
<td>RSS=SSE</td>
<td>MSE=RSS/<span class="math inline">\((n-p-1)\)</span>
</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>Total</td>
<td><span class="math inline">\(n-1\)</span></td>
<td>TSS=SST</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table></div>
<p>The ANOVA table has many pieces of information. What we care about is the <span class="math inline">\(F\)</span> Ratio and the corresponding p-value. We compare the <span class="math inline">\(F\)</span> Ratio with <span class="math inline">\(F_{(p,n-p-1)}\)</span> and a corresponding <span class="math inline">\(\alpha\)</span> value (error).</p>
<div class="caution">
<p>
The “ANOVA table” is a broad concept in statistics, with different
variants. Here we are only covering the basic ANOVA table from the
relation <span class="math inline"><span class="math inline">\(\text{SST} = \text{SSR} + \text{SSE}\)</span></span>. However, further sophistications are possible when
<span class="math inline"><span class="math inline">\(\text{SSR}\)</span></span> is decomposed into the
variations contributed by <em>each</em> predictor. In particular, for
multiple linear regression
<code>r fontawesome::fa(“r-project”, fill =“steelblue”)</code>’s
<code>anova</code> implements a <em>sequential (type I) ANOVA
table</em>, which is <strong>not</strong> the previous table!
</p>
</div>
<p>The <code>anova</code> function in <svg aria-hidden="true" role="img" viewbox="0 0 581 512" style="height:1em;width:1.13em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:steelblue;overflow:visible;position:relative;"><path d="M581 226.6C581 119.1 450.9 32 290.5 32S0 119.1 0 226.6C0 322.4 103.3 402 239.4 418.1V480h99.1v-61.5c24.3-2.7 47.6-7.4 69.4-13.9L448 480h112l-67.4-113.7c54.5-35.4 88.4-84.9 88.4-139.7zm-466.8 14.5c0-73.5 98.9-133 220.8-133s211.9 40.7 211.9 133c0 50.1-26.5 85-70.3 106.4-2.4-1.6-4.7-2.9-6.4-3.7-10.2-5.2-27.8-10.5-27.8-10.5s86.6-6.4 86.6-92.7-90.6-87.9-90.6-87.9h-199V361c-74.1-21.5-125.2-67.1-125.2-119.9zm225.1 38.3v-55.6c57.8 0 87.8-6.8 87.8 27.3 0 36.5-38.2 28.3-87.8 28.3zm-.9 72.5H365c10.8 0 18.9 11.7 24 19.2-16.1 1.9-33 2.8-50.6 2.9v-22.1z"></path></svg> takes a model as an input and returns the following <em>sequential</em> ANOVA table<a href="main-references-credits.html#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a>:</p>
<div class="inline-table"><table class="table table-sm">
<colgroup>
<col width="19%">
<col width="16%">
<col width="11%">
<col width="12%">
<col width="22%">
<col width="18%">
</colgroup>
<thead><tr class="header">
<th></th>
<th>Degrees of freedom</th>
<th>Sum Squares</th>
<th>Mean Squares</th>
<th>
<span class="math inline">\(F\)</span>-value</th>
<th>
<span class="math inline">\(p\)</span>-value</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Predictor 1</td>
<td><span class="math inline">\(1\)</span></td>
<td>ESS<span class="math inline">\(_1\)</span>
</td>
<td><span class="math inline">\(\frac{\text{ESS}_1}{1}\)</span></td>
<td><span class="math inline">\(\frac{\text{ESS}_1/1}{\text{RSS}/(n-p-1)}\)</span></td>
<td><span class="math inline">\(p_1\)</span></td>
</tr>
<tr class="even">
<td>Predictor 2</td>
<td><span class="math inline">\(1\)</span></td>
<td>ESS<span class="math inline">\(_2\)</span>
</td>
<td><span class="math inline">\(\frac{\text{ESS}_2}{1}\)</span></td>
<td><span class="math inline">\(\frac{\text{ESS}_2/1}{\text{RSS}/(n-p-1)}\)</span></td>
<td><span class="math inline">\(p_2\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\vdots\)</span></td>
<td><span class="math inline">\(\vdots\)</span></td>
<td><span class="math inline">\(\vdots\)</span></td>
<td><span class="math inline">\(\vdots\)</span></td>
<td><span class="math inline">\(\vdots\)</span></td>
<td><span class="math inline">\(\vdots\)</span></td>
</tr>
<tr class="even">
<td>Predictor <span class="math inline">\(p\)</span>
</td>
<td><span class="math inline">\(1\)</span></td>
<td>ESS<span class="math inline">\(_p\)</span>
</td>
<td><span class="math inline">\(\frac{\text{ESS}_p}{1}\)</span></td>
<td><span class="math inline">\(\frac{\text{ESS}_p/1}{\text{RSS}/(n-p-1)}\)</span></td>
<td><span class="math inline">\(p_p\)</span></td>
</tr>
<tr class="odd">
<td>Residuals</td>
<td><span class="math inline">\(n - p - 1\)</span></td>
<td>RSS</td>
<td><span class="math inline">\(\frac{\text{RSS}}{n-p-1}\)</span></td>
<td></td>
<td></td>
</tr>
</tbody>
</table></div>
<p>Here the ESS<span class="math inline">\(_j\)</span> represents the explained sum of squares (same as regression sum of squares) associated to the inclusion of <span class="math inline">\(X_j\)</span> in the model with predictors <span class="math inline">\(X_1,\ldots,X_{j-1}\)</span>, this is:
<span class="math display">\[
\text{ESS}_j=\text{ESS}(X_1,\ldots,X_j)-\text{ESS}(X_1,\ldots,X_{j-1}).
\]</span>
The <span class="math inline">\(p\)</span>-values <span class="math inline">\(p_1,\ldots,p_p\)</span> correspond to the testing of the hypotheses
<span class="math display">\[\begin{align*}
H_0:\beta_j=0\quad\text{vs.}\quad H_1:\beta_j\neq 0,
\end{align*}\]</span>
carried out <em>inside the linear model</em> <span class="math inline">\(Y=\beta_0+\beta_1X_1+\ldots+\beta_jX_j+\varepsilon\)</span>. This is like the <span class="math inline">\(t\)</span>-test for <span class="math inline">\(\beta_j\)</span> for the model with predictors <span class="math inline">\(X_1,\ldots,X_j\)</span>. Recall that there is no <span class="math inline">\(F\)</span>-test in this version of the ANOVA table.</p>
<p><strong>p-values</strong></p>
<p>The p-values provide information about whether each individual predictor is related to the response, after adjusting for the other predictors. Let’s look at the following table we obtain in general using a statistical software for example</p>
<div class="inline-table"><table class="table table-sm">
<thead><tr class="header">
<th></th>
<th>Coefficient</th>
<th>Std. error</th>
<th>
<span class="math inline">\(t\)</span>-statistic</th>
<th>p-value</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Constant</td>
<td>2.939</td>
<td>0.3119</td>
<td>9.42</td>
<td>&lt;0.0001</td>
</tr>
<tr class="even">
<td><span class="math inline">\(X_1\)</span></td>
<td>0.046</td>
<td>0.0014</td>
<td>32.81</td>
<td>&lt;0.0001</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(X_2\)</span></td>
<td>0.189</td>
<td>0.0086</td>
<td>21.89</td>
<td>&lt;0.0001</td>
</tr>
<tr class="even">
<td><span class="math inline">\(X_3\)</span></td>
<td>-0.001</td>
<td>0.0059</td>
<td>-0.18</td>
<td>0.8599</td>
</tr>
</tbody>
</table></div>
<p>In this table we have the following model</p>
<p><span class="math display">\[ Y = 2.939 + 0.046 X_1 + 0.189 X_2 - 0.001 X_3 \]</span></p>
<p>Note that for each individual predictor a <span class="math inline">\(t\)</span>-statistic and a p-value were reported. These p-values indicate that <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are related to <span class="math inline">\(Y\)</span>, but that there is no evidence that <span class="math inline">\(X_3\)</span> is associated with <span class="math inline">\(Y\)</span>, in the presence of these two.</p>
<p><strong><em>Deciding on Important Variables</em></strong></p>
<p>The most direct approach is called <em>all subsets</em> or <em>best subsets</em> regression: we compute the least squares fit for all possible subsets and then choose between them based on some criterion that balances training error with model size.</p>
<p>However we often can’t examine all possible models, since they are <span class="math inline">\(2^p\)</span> of them; for example when <span class="math inline">\(p = 40\)</span> there are over a billion models! Instead we need an automated approach that searches through a subset of them. Here are two commonly use approaches:</p>
<p><strong>Forward selection</strong>:</p>
<ul>
<li>Begin with the <em>null model</em> — a model that contains an intercept (constant) but no predictors.</li>
<li>Fit <span class="math inline">\(p\)</span> simple linear regressions and add to the null model the variable that results in the lowest RSS.</li>
<li>Add to that model the variable that results in the lowest RSS amongst all two-variable models.</li>
<li>Continue until some stopping rule is satisfied, for example when all remaining variables have a p-value above some threshold.</li>
</ul>
<p><strong>Backward selection</strong>:</p>
<ul>
<li>Start with all variables in the model.</li>
<li>Remove the variable with the largest p-value — that is, the variable that is the least statistically significant.</li>
<li>The new <span class="math inline">\((p − 1)\)</span>-variable model is fit, and the variable with the largest p-value is removed.</li>
<li>Continue until a stopping rule is reached. For instance, we may stop when all remaining variables have a significant p-value defined by some significance threshold.</li>
</ul>
<div class="rmdtip">
<p>There are more systematic criteria for choosing an “optimal” member in the path of models produced by forward or backward stepwise selection. These include <em>Mallow’s <span class="math inline">\(C_p\)</span></em> , <em>Akaike information criterion (AIC)</em>, <em>Bayesian information criterion (BIC)</em>, <em>adjusted <span class="math inline">\(R^2\)</span></em> and <em>Cross-validation (CV)</em>.</p>
</div>
<p><strong><em>Model Fit</em></strong></p>
<p>Two of the most common numerical measures of model fit are the RSE and <span class="math inline">\(R^2\)</span>, the fraction of variance explained. These quantities are computed and interpreted in the same fashion as for simple linear regression. Recall that in simple regression, <span class="math inline">\(R^2\)</span> is the square of the correlation of the response and the variable. In multiple linear regression, it turns out that it equals <span class="math inline">\(Cor(Y, \hat{Y})^2\)</span> , the square of the correlation between the response and the fitted linear model; in fact one property of the fitted linear model is that it maximizes this correlation among all possible linear models. An <span class="math inline">\(R^2\)</span> value close to 1 indicates that the model explains a large portion of the variance in the response variable.</p>
<p>In general RSE is defined as</p>
<p><span class="math display">\[ \text{RSE} = \sqrt{\frac{1}{n-p-1}\text{RSS}} \]</span></p>
<div id="other-consid" class="section level3" number="2.3.1">
<h3>
<span class="header-section-number">2.3.1</span> Other Considerations in Regression Model<a class="anchor" aria-label="anchor" href="#other-consid"><i class="fas fa-link"></i></a>
</h3>
<p><strong>Qualitative Predictors</strong></p>
<ul>
<li>If we have a categorial (qualitative) variable (feature), how do we fit into a regression equation?</li>
<li>For example, if <span class="math inline">\(X_1\)</span> is the gender (male or female).</li>
<li>We can code, for example, male = 0 and female = 1.</li>
<li>Suppose <span class="math inline">\(X_2\)</span> is a quantitative variable, the regression equation becomes:</li>
</ul>
<p><span class="math display">\[ Y_i \approx \beta_0 + \beta_1 X_1 + \beta_2 X_2 = \begin{cases}
  \beta_0 + \beta_2 X_2 &amp; \text{ if male} \\
  \beta_0 + \beta_1 X_1 + \beta_2 X_2 &amp; \text{ if female}
\end{cases} \]</span></p>
<ul>
<li>Another possible coding scheme is to let male = -1 and female = 1, the regression equation is then:</li>
</ul>
<p><span class="math display">\[ Y_i \approx \beta_0 + \beta_1 X_1 + \beta_2 X_2 = \begin{cases}
  \beta_0 -\beta_1 X_1 + \beta_2 X_2 &amp; \text{ if male} \\
  \beta_0 + \beta_1 X_1 + \beta_2 X_2 &amp; \text{ if female}
\end{cases} \]</span></p>
<p><strong>Interaction Terms</strong></p>
<ul>
<li>When the effect on <span class="math inline">\(Y\)</span> of increasing <span class="math inline">\(X_1\)</span> depends on another <span class="math inline">\(X_2\)</span>.</li>
<li>We may in this case try the model</li>
</ul>
<p><span class="math display">\[ Y_i = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2 \]</span></p>
<ul>
<li>
<span class="math inline">\(X_1 X_2\)</span> is the Interaction term.</li>
</ul>
</div>
</div>
<div id="how-to-select-the-best-performing-model" class="section level2" number="2.4">
<h2>
<span class="header-section-number">2.4</span> How to select the best performing model<a class="anchor" aria-label="anchor" href="#how-to-select-the-best-performing-model"><i class="fas fa-link"></i></a>
</h2>
<p>After trying different linear models, you need to make a choice which model you want to use. More specifically, the questions that one can ask: “How to determine which model suits best to my data? Do I just look at the R square, SSE, etc.?” and “As the interpretation of that model (quadratic, root, etc.) will be very different, won’t it be an issue?”</p>
<p>The second question can be answered easily. First, find a model that best suits to your data and then interpret its results. It is good if you have ideas how your data might be explained. However, interpret the best model, only. Now we will address the first question. Note that there are multiple ways to select a best model. In addition, this approach only applies to univariate models (simple models) whith just one input variable.</p>
<p>Use the following <strong>interactive application</strong> and play around with different datasets and models. Notice how parameters change and become more confident with assessing simple linear models.</p>
<iframe src="https://bjoernhartmann.shinyapps.io/linear_model_selection/?showcase=0" width="100%" height="900px" data-external="1">
</iframe>
<div id="use-the-adjusted-r_adj2-for-multivariate-models" class="section level3 unnumbered">
<h3>Use the Adjusted <span class="math inline">\(R_{adj}^2\)</span> for multivariate models<a class="anchor" aria-label="anchor" href="#use-the-adjusted-r_adj2-for-multivariate-models"><i class="fas fa-link"></i></a>
</h3>
<p>If you only use one input variable, the adjusted <span class="math inline">\(R_{adj}^2\)</span> value gives you a good indication of how well your model performs. It illustrates how much variation is explained by your model.</p>
<p>In contrast to the simple <span class="math inline">\(R^2\)</span><a href="main-references-credits.html#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a>, the adjusted adjusted <span class="math inline">\(R_{adj}^2\)</span><a href="main-references-credits.html#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a> takes the number of input factors into account. It penalizes too many input factors and favors parsimonious models.</p>
<p>The adjusted <span class="math inline">\(R_{adj}^2\)</span> is sensitive to the amount of noise in the data. As such, only compare this indicator of models for the same dataset than comparing it across different datasets.</p>

<div class="figure" style="text-align: center">
<span style="display:block;" id="fig:R2"></span>
<img src="img/R2vsAdjR2.png" alt="Comparison of \(R^2\) and \(R^2_{\text{adj}}\) for \(n=200\) and \(p\) ranging from \(1\) to \(198\). \(M=100\) datasets were simulated with only the first two predictors being significant. The thicker curves are the mean of each color’s curves." width="70%"><p class="caption">
Figure 2.1: Comparison of <span class="math inline">\(R^2\)</span> and <span class="math inline">\(R^2_{\text{adj}}\)</span> for <span class="math inline">\(n=200\)</span> and <span class="math inline">\(p\)</span> ranging from <span class="math inline">\(1\)</span> to <span class="math inline">\(198\)</span>. <span class="math inline">\(M=100\)</span> datasets were simulated with <strong>only the first two</strong> predictors being significant. The thicker curves are the mean of each color’s curves.
</p>
</div>
<p>Figure <a href="multiple-linear-regression.html#fig:R2">2.1</a> contains the results of an experiment where 100 datasets were simulated with <strong>only the first two</strong> predictors being significant. As you can see <span class="math inline">\(R^2\)</span> increases linearly with the number of predictors considered, although only the first two ones were important! On the contrary, <span class="math inline">\(R^2_\text{adj}\)</span> only increases in the first two variables and then is flat on average, but it has a huge variability when <span class="math inline">\(p\)</span> approaches <span class="math inline">\(n-2\)</span>. The experiment evidences that <strong><span class="math inline">\(R^2_\text{adj}\)</span> is more adequate than the <span class="math inline">\(R^2\)</span> for evaluating the fit of a multiple linear regression</strong>.</p>
</div>
<div id="have-a-look-at-the-residuals-or-error-terms" class="section level3 unnumbered">
<h3>Have a look at the residuals or error terms<a class="anchor" aria-label="anchor" href="#have-a-look-at-the-residuals-or-error-terms"><i class="fas fa-link"></i></a>
</h3>
<p>What is often ignored are error terms or so-called residuals. They often tell you more than what you might think. The residuals are the difference between your predicted values and the actual values. Their benefit is that they can show both the magnitude as well as the direction of the errors.</p>
<p>Let’s have a look at an example:</p>
<center>
<img src="img/residuals.png">
</center>
<p>Here, we try to predict a polynomial dataset with a linear function. Analyzing the residuals shows that there are areas where the model has an upward or downward bias.</p>
<p>For <code>50 &lt; x &lt; 100</code>, the residuals are above zero. So in this area, the actual values have been higher than the predicted values — our model has a downward bias.</p>
<p>For <code>100 &lt; x &lt; 150</code>, however, the residuals are below zero. Thus, the actual values have been lower than the predicted values — the model has an upward bias.</p>
<p>It is always good to know, whether your model suggests too high or too low values. But you usually do not want to have patterns like this.</p>
<p>The residuals should be zero on average (as indicated by the mean) and they should be equally distributed. Predicting the same dataset with a polynomial function of <code>3 degrees</code> suggests a much better fit:</p>
<center>
<img src="img/residuals2.png">
</center>
<p>In addition, you can observe whether the variance of your errors increases. In statistics, this is called <a href="https://en.wikipedia.org/wiki/Heteroscedasticity">Heteroscedasticity</a>. You can fix this easily with robust standard errors. Otherwise, your hypothesis tests are likely to be wrong.</p>
</div>
<div id="histogram-of-residuals" class="section level3 unnumbered">
<h3>Histogram of residuals<a class="anchor" aria-label="anchor" href="#histogram-of-residuals"><i class="fas fa-link"></i></a>
</h3>
<p>Finally, the histogram summarizes the magnitude of your error terms. It provides information about the bandwidth of errors and indicates how often which errors occurred.</p>
<center>
<img src="img/residuals_hist.png"><img src="img/residuals_hist2.png">
</center>
<p>The above screenshots show two models for the same dataset. In the first histogram, errors occur within a range of -338 and 520. In the second histogram, errors occur within -293 and 401. So the outliers are much lower. Furthermore, most errors in the model of the second histogram are closer to zero. So we would favor the second model.</p>
<p align="right">
◼
</p>

</div>
</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="practical-work-1.html">Practical Work 1</a></div>
<div class="next"><a href="pw-2.html">PW 2</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#multiple-linear-regression"><span class="header-section-number">2</span> Multiple Linear Regression</a></li>
<li><a class="nav-link" href="#the-model"><span class="header-section-number">2.1</span> The Model</a></li>
<li><a class="nav-link" href="#estimating-the-regression-coefficients"><span class="header-section-number">2.2</span> Estimating the Regression Coefficients</a></li>
<li>
<a class="nav-link" href="#some-important-questions"><span class="header-section-number">2.3</span> Some important questions</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#other-consid"><span class="header-section-number">2.3.1</span> Other Considerations in Regression Model</a></li></ul>
</li>
<li>
<a class="nav-link" href="#how-to-select-the-best-performing-model"><span class="header-section-number">2.4</span> How to select the best performing model</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#use-the-adjusted-r_adj2-for-multivariate-models">Use the Adjusted \(R_{adj}^2\) for multivariate models</a></li>
<li><a class="nav-link" href="#have-a-look-at-the-residuals-or-error-terms">Have a look at the residuals or error terms</a></li>
<li><a class="nav-link" href="#histogram-of-residuals">Histogram of residuals</a></li>
</ul>
</li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Machine Learning</strong>" was written by Mohamad Ghassany. It was last built on 2022-04-26.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
