<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Machine Learning</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Machine Learning course">
  <meta name="generator" content="bookdown 0.2 and GitBook 2.6.7">

  <meta property="og:title" content="Machine Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Machine Learning course" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Machine Learning" />
  
  <meta name="twitter:description" content="Machine Learning course" />
  

<meta name="author" content="Mohamad Ghassany">


<meta name="date" content="2016-12-15">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="introduction.html">
<link rel="next" href="pw-1.html">

<script src="book_assets/jquery-2.2.3/jquery.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">ESILV Intoduction to Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a><ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#what-is-machine-learning"><i class="fa fa-check"></i>What is Machine Learning ?</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#supervised-learning"><i class="fa fa-check"></i>Supervised Learning</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#unsupervised-learning"><i class="fa fa-check"></i>Unsupervised Learning</a></li>
</ul></li>
<li class="part"><span><b>Supervised Learning</b></span></li>
<li class="part"><span><b>Regression</b></span></li>
<li class="chapter" data-level="1" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>1</b> Linear Regression</a><ul>
<li class="chapter" data-level="1.1" data-path="linear-regression.html"><a href="linear-regression.html#notation"><i class="fa fa-check"></i><b>1.1</b> Notation</a></li>
<li class="chapter" data-level="1.2" data-path="linear-regression.html"><a href="linear-regression.html#model-representation"><i class="fa fa-check"></i><b>1.2</b> Model Representation</a></li>
<li class="chapter" data-level="1.3" data-path="linear-regression.html"><a href="linear-regression.html#why-estimate-f"><i class="fa fa-check"></i><b>1.3</b> Why Estimate <span class="math inline">\(f\)</span> ?</a><ul>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html#prediction"><i class="fa fa-check"></i>Prediction</a></li>
<li class="chapter" data-level="" data-path="linear-regression.html"><a href="linear-regression.html#inference"><i class="fa fa-check"></i>Inference</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="linear-regression.html"><a href="linear-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>1.4</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="1.4.1" data-path="linear-regression.html"><a href="linear-regression.html#model"><i class="fa fa-check"></i><b>1.4.1</b> Model</a></li>
<li class="chapter" data-level="1.4.2" data-path="linear-regression.html"><a href="linear-regression.html#estimating-the-coefficients"><i class="fa fa-check"></i><b>1.4.2</b> Estimating the Coefficients</a></li>
<li class="chapter" data-level="1.4.3" data-path="linear-regression.html"><a href="linear-regression.html#assessing-the-accuracy-of-the-coefficient-estimates"><i class="fa fa-check"></i><b>1.4.3</b> Assessing the Accuracy of the Coefficient Estimates</a></li>
<li class="chapter" data-level="1.4.4" data-path="linear-regression.html"><a href="linear-regression.html#assessing-the-accuracy-of-the-model"><i class="fa fa-check"></i><b>1.4.4</b> Assessing the Accuracy of the Model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="pw-1.html"><a href="pw-1.html"><i class="fa fa-check"></i>PW 1</a><ul>
<li class="chapter" data-level="" data-path="pw-1.html"><a href="pw-1.html#how-to-use-r"><i class="fa fa-check"></i>How to use R</a></li>
<li class="chapter" data-level="" data-path="pw-1.html"><a href="pw-1.html#get-familiar-with-r"><i class="fa fa-check"></i>Get familiar with R</a></li>
<li class="chapter" data-level="" data-path="pw-1.html"><a href="pw-1.html#linear-regression-1"><i class="fa fa-check"></i>Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html"><i class="fa fa-check"></i><b>2</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="2.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#the-model"><i class="fa fa-check"></i><b>2.1</b> The Model</a></li>
<li class="chapter" data-level="2.2" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#estimating-the-regression-coefficients"><i class="fa fa-check"></i><b>2.2</b> Estimating the Regression Coefficients</a></li>
<li class="chapter" data-level="2.3" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#some-important-questions"><i class="fa fa-check"></i><b>2.3</b> Some important questions</a><ul>
<li class="chapter" data-level="2.3.1" data-path="multiple-linear-regression.html"><a href="multiple-linear-regression.html#other-considerations-in-regression-model"><i class="fa fa-check"></i><b>2.3.1</b> Other Considerations in Regression Model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="pw-2.html"><a href="pw-2.html"><i class="fa fa-check"></i>PW 2</a><ul>
<li class="chapter" data-level="" data-path="pw-2.html"><a href="pw-2.html#rmarkdown"><i class="fa fa-check"></i>RMarkdown</a></li>
<li class="chapter" data-level="" data-path="pw-2.html"><a href="pw-2.html#multiple-linear-regression-1"><i class="fa fa-check"></i>Multiple Linear Regression</a></li>
</ul></li>
<li class="part"><span><b>Classification</b></span></li>
<li class="chapter" data-level="3" data-path="logistic-regression-k-nn.html"><a href="logistic-regression-k-nn.html"><i class="fa fa-check"></i><b>3</b> Logistic Regression + K-NN</a><ul>
<li class="chapter" data-level="3.1" data-path="logistic-regression-k-nn.html"><a href="logistic-regression-k-nn.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="logistic-regression-k-nn.html"><a href="logistic-regression-k-nn.html#logistic-regression"><i class="fa fa-check"></i><b>3.2</b> Logistic Regression</a><ul>
<li class="chapter" data-level="3.2.1" data-path="logistic-regression-k-nn.html"><a href="logistic-regression-k-nn.html#the-logistic-model"><i class="fa fa-check"></i><b>3.2.1</b> The Logistic Model</a></li>
<li class="chapter" data-level="3.2.2" data-path="logistic-regression-k-nn.html"><a href="logistic-regression-k-nn.html#estimating-the-regression-coefficients-1"><i class="fa fa-check"></i><b>3.2.2</b> Estimating the Regression Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="logistic-regression-k-nn.html"><a href="logistic-regression-k-nn.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>3.3</b> Multiple Logistic Regression</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="pw-3.html"><a href="pw-3.html"><i class="fa fa-check"></i>PW 3</a><ul>
<li class="chapter" data-level="" data-path="pw-3.html"><a href="pw-3.html#subsection"><i class="fa fa-check"></i>Subsection</a></li>
<li class="chapter" data-level="" data-path="pw-3.html"><a href="pw-3.html#subsection-1"><i class="fa fa-check"></i>Subsection</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html"><i class="fa fa-check"></i><b>4</b> Discriminant Analysis</a><ul>
<li class="chapter" data-level="4.1" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#subsection-1-1"><i class="fa fa-check"></i><b>4.1</b> Subsection 1</a></li>
<li class="chapter" data-level="4.2" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#subsection-2"><i class="fa fa-check"></i><b>4.2</b> Subsection 2</a><ul>
<li class="chapter" data-level="4.2.1" data-path="discriminant-analysis.html"><a href="discriminant-analysis.html#subsection-3"><i class="fa fa-check"></i><b>4.2.1</b>  Subsection 3</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="pw-4.html"><a href="pw-4.html"><i class="fa fa-check"></i>PW 4</a><ul>
<li class="chapter" data-level="" data-path="pw-4.html"><a href="pw-4.html#subsection-4"><i class="fa fa-check"></i>Subsection</a></li>
<li class="chapter" data-level="" data-path="pw-4.html"><a href="pw-4.html#subsection-5"><i class="fa fa-check"></i>Subsection</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>5</b> Support Vector Machines</a><ul>
<li class="chapter" data-level="5.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#subsection-1-2"><i class="fa fa-check"></i><b>5.1</b> Subsection 1</a></li>
<li class="chapter" data-level="5.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#subsection-2-1"><i class="fa fa-check"></i><b>5.2</b> Subsection 2</a><ul>
<li class="chapter" data-level="5.2.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#subsection-3-1"><i class="fa fa-check"></i><b>5.2.1</b>  Subsection 3</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="pw-5.html"><a href="pw-5.html"><i class="fa fa-check"></i>PW 5</a><ul>
<li class="chapter" data-level="" data-path="pw-5.html"><a href="pw-5.html#subsection-6"><i class="fa fa-check"></i>Subsection</a></li>
<li class="chapter" data-level="" data-path="pw-5.html"><a href="pw-5.html#subsection-7"><i class="fa fa-check"></i>Subsection</a></li>
</ul></li>
<li class="part"><span><b>Unsupervised Learning</b></span></li>
<li class="chapter" data-level="6" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html"><i class="fa fa-check"></i><b>6</b> Dimensionality Reduction</a><ul>
<li class="chapter" data-level="6.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#subsection-1-3"><i class="fa fa-check"></i><b>6.1</b> Subsection 1</a></li>
<li class="chapter" data-level="6.2" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#subsection-2-2"><i class="fa fa-check"></i><b>6.2</b> Subsection 2</a><ul>
<li class="chapter" data-level="6.2.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#subsection-3-2"><i class="fa fa-check"></i><b>6.2.1</b>  Subsection 3</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="pw-6.html"><a href="pw-6.html"><i class="fa fa-check"></i>PW 6</a><ul>
<li class="chapter" data-level="" data-path="pw-6.html"><a href="pw-6.html#subsection-8"><i class="fa fa-check"></i>Subsection</a></li>
<li class="chapter" data-level="" data-path="pw-6.html"><a href="pw-6.html#subsection-9"><i class="fa fa-check"></i>Subsection</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="clustering-kmeans.html"><a href="clustering-kmeans.html"><i class="fa fa-check"></i><b>7</b> Clustering: Kmeans</a><ul>
<li class="chapter" data-level="7.1" data-path="clustering-kmeans.html"><a href="clustering-kmeans.html#subsection-1-4"><i class="fa fa-check"></i><b>7.1</b> Subsection 1</a></li>
<li class="chapter" data-level="7.2" data-path="clustering-kmeans.html"><a href="clustering-kmeans.html#subsection-2-3"><i class="fa fa-check"></i><b>7.2</b> Subsection 2</a><ul>
<li class="chapter" data-level="7.2.1" data-path="clustering-kmeans.html"><a href="clustering-kmeans.html#subsection-3-3"><i class="fa fa-check"></i><b>7.2.1</b>  Subsection 3</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="pw-7.html"><a href="pw-7.html"><i class="fa fa-check"></i>PW 7</a><ul>
<li class="chapter" data-level="" data-path="pw-7.html"><a href="pw-7.html#subsection-10"><i class="fa fa-check"></i>Subsection</a></li>
<li class="chapter" data-level="" data-path="pw-7.html"><a href="pw-7.html#subsection-11"><i class="fa fa-check"></i>Subsection</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="expectation-maximisation.html"><a href="expectation-maximisation.html"><i class="fa fa-check"></i><b>8</b> Expectation Maximisation</a><ul>
<li class="chapter" data-level="8.1" data-path="expectation-maximisation.html"><a href="expectation-maximisation.html#subsection-1-5"><i class="fa fa-check"></i><b>8.1</b> Subsection 1</a></li>
<li class="chapter" data-level="8.2" data-path="expectation-maximisation.html"><a href="expectation-maximisation.html#subsection-2-4"><i class="fa fa-check"></i><b>8.2</b> Subsection 2</a><ul>
<li class="chapter" data-level="8.2.1" data-path="expectation-maximisation.html"><a href="expectation-maximisation.html#subsection-3-4"><i class="fa fa-check"></i><b>8.2.1</b>  Subsection 3</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="pw-8.html"><a href="pw-8.html"><i class="fa fa-check"></i>PW 8</a><ul>
<li class="chapter" data-level="" data-path="pw-8.html"><a href="pw-8.html#subsection-12"><i class="fa fa-check"></i>Subsection</a></li>
<li class="chapter" data-level="" data-path="pw-8.html"><a href="pw-8.html#subsection-13"><i class="fa fa-check"></i>Subsection</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-resources.html"><a href="references-resources.html"><i class="fa fa-check"></i>References &amp; Resources</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linear-regression" class="section level1">
<h1><span class="header-section-number">1</span> Linear Regression</h1>
<!-- Take a look to these slides -->
<!-- <iframe src="linear_regression.pdf" frameborder="0" width="700" height="422" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe> -->
<div id="notation" class="section level2">
<h2><span class="header-section-number">1.1</span> Notation</h2>
<p>In general, we will let <span class="math inline">\(x_{ij}\)</span> represent the value of the <span class="math inline">\(j\)</span>th variable for the <span class="math inline">\(i\)</span>th observation, where <span class="math inline">\(i=1,2,\ldots,n\)</span> and <span class="math inline">\(j=1,2,\ldots,p\)</span>. We will use <span class="math inline">\(i\)</span> to index the samples or observations (from <span class="math inline">\(1\)</span> tp <span class="math inline">\(n\)</span>) and <span class="math inline">\(j\)</span> will be used to index the variables (or features) (from <span class="math inline">\(1\)</span> to <span class="math inline">\(p\)</span>). We let <span class="math inline">\(\textbf{X}\)</span> denote a <span class="math inline">\(n \times p\)</span> matrix whose <span class="math inline">\((i,j)\)</span>th element is <span class="math inline">\(x_{ij}\)</span>. That is,</p>
<p><span class="math display">\[ \textbf{X}  = \begin{pmatrix}
    x_{11} &amp; x_{12} &amp; x_{13} &amp; \dots  &amp; x_{1p} \\
    x_{21} &amp; x_{22} &amp; x_{23} &amp; \dots  &amp; x_{2p} \\
    \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    x_{n1} &amp; x_{n2} &amp; x_{n3} &amp; \dots  &amp; x_{np}
\end{pmatrix} \]</span></p>
<p>Note that it is useful to visualize <span class="math inline">\(\textbf{X}\)</span> as a spreadsheet of numbers with <span class="math inline">\(n\)</span> rows and <span class="math inline">\(p\)</span> columns. We will write the rows of <span class="math inline">\(\textbf{X}\)</span> as <span class="math inline">\(x_1 , x_2 , \ldots, x_n\)</span>. Here <span class="math inline">\(x_i\)</span> is a vector of length <span class="math inline">\(p\)</span>, containing the <span class="math inline">\(p\)</span> variable measurements for the <span class="math inline">\(i\)</span>th observation. That is,</p>
<p><span class="math display">\[ x_i = \begin{pmatrix}
    x_{i1} \\
    x_{i2} \\
    \vdots \\
    x_{ip}
\end{pmatrix}\]</span></p>
<p>(Vectors are by default represented as columns.)</p>
<p>We will write the columns of <span class="math inline">\(\textbf{X}\)</span> as <span class="math inline">\(\textbf{x}_1 , \textbf{x}_2, \ldots, \textbf{x}_p\)</span>. Each is a vector of length <span class="math inline">\(n\)</span>. That is,</p>
<p><span class="math display">\[ \textbf{x}_j = \begin{pmatrix}
    \textbf{x}_{1j} \\
    \textbf{x}_{2j} \\
    \vdots \\
    \textbf{x}_{nj}
\end{pmatrix}\]</span></p>
<p>Using this notation, the matrix <span class="math inline">\(\textbf{X}\)</span> can be written as</p>
<p><span class="math display">\[ \textbf{X} = (\textbf{x}_1  \textbf{x}_2 \ldots \textbf{x}_p) \]</span></p>
<p>or</p>
<p><span class="math display">\[ \textbf{X} = \begin{pmatrix}
    x_{1}^T \\
    x_{2}^T \\
    \vdots \\
    x_{n}^T
\end{pmatrix}\]</span></p>
<p>The <span class="math inline">\(^T\)</span> notation denotes the transpose of a matrix or vector.</p>
<p>We use <span class="math inline">\(y_i\)</span> to denote the <span class="math inline">\(i\)</span>th observation of the variable on which we wish to make predictions. We write the set of all <span class="math inline">\(n\)</span> observations in vector form as</p>
<p><span class="math display">\[ \textbf{y} = \begin{pmatrix}
    y_{1}^T \\
    y_{2}^T \\
    \vdots \\
    y_{n}^T
\end{pmatrix}\]</span></p>
<p>Then the observed data consists of <span class="math inline">\(\{(x_1, y_1), (x_2 , y_2 ), \ldots , (x_n , y_n )\}\)</span>, where each <span class="math inline">\(x_i\)</span> is a vector of length <span class="math inline">\(p\)</span>. (If <span class="math inline">\(p = 1\)</span>, then <span class="math inline">\(x_i\)</span> is simply a scalar).</p>
</div>
<div id="model-representation" class="section level2">
<h2><span class="header-section-number">1.2</span> Model Representation</h2>
<p>Let’s consider the example about predicting housing prices. We’re going to use this data set as an example,</p>
<div class="figure">
<img src="img/mr1.png" />

</div>
<p>Suppose that there is a person trying to sell a house of size 1250 square feet and he wants to know how much he might be able to sell the house for. One thing we could do is fit a model. Maybe fit a straight line to this data. Looks something like this,</p>
<div class="figure">
<img src="img/mr2.png" />

</div>
<p>and based on that, maybe he can sell the house for around $220,000. Recall that this is an example of a supervised learning algorithm. And it’s supervised learning because we’re given the “right answer” for each of our examples. More precisely, this is an example of a regression problem where the term regression refers to the fact that we are predicting a real-valued output namely the price.</p>
<p>More formally, in supervised learning, we have a data set and this data set is called a <strong>training set</strong>. So for housing prices example, we have a training set of different housing prices and our job is to learn from this data how to predict prices of the houses.</p>
<p>Let’s define some notation from this data set:</p>
<ul>
<li>The size of the house is the input variable.</li>
<li>The house price is the output variable.</li>
<li>The input variables are typically denoted using the variable symbol <span class="math inline">\(X\)</span>,</li>
<li>The inputs go by different names, such as <em>predictors</em>, <em>independent variables</em>, <em>features</em>, <em>predictor</em> or sometimes just <em>variables</em>.</li>
<li>The output variable is often called the <em>response</em>, <em>dependent variable</em> or <em>target</em>, and is typically denoted using the symbol <span class="math inline">\(Y\)</span>.</li>
<li><span class="math inline">\((x_i,y_i)\)</span> is the <span class="math inline">\(i\)</span>th training example.</li>
<li>The set of <span class="math inline">\(\{(x_i, y_i)\}\)</span> is the training set.</li>
<li><span class="math inline">\(n\)</span> is the number of training examples.</li>
</ul>
<p>So here’s how this supervised learning algorithm works. Suppose that we observe a quantitative response <span class="math inline">\(Y\)</span> and <span class="math inline">\(p\)</span> different predictors, <span class="math inline">\(X_1 , X_2 ,\ldots, X_p\)</span> . We assume that there is some relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X = (X_1 , X_2 ,\ldots, X_p)\)</span>, which can be written in the very general form</p>
<p><span class="math display">\[Y = f(X) + \epsilon\]</span></p>
<center>
<img src="img/mr3.png" />
</center>
<p>Here <span class="math inline">\(f\)</span> is some fixed but unknown function of <span class="math inline">\(X_1 , X_2 ,\ldots, X_p\)</span> , and <span class="math inline">\(\epsilon\)</span> is a random error term, which is independent of <span class="math inline">\(X\)</span> and has mean zero. The <span class="math inline">\(f\)</span> function is also called <em>hypothesis</em> in Machine Learning. In general, the function <span class="math inline">\(f\)</span> may involve more than one input variable. In essence, Supervised Learning refers to a set of approaches for estimating <span class="math inline">\(f\)</span>.</p>
</div>
<div id="why-estimate-f" class="section level2">
<h2><span class="header-section-number">1.3</span> Why Estimate <span class="math inline">\(f\)</span> ?</h2>
<p>There are two main reasons that we may wish to estimate <span class="math inline">\(f\)</span>: <em>prediction</em> and <em>inference</em>.</p>
<div id="prediction" class="section level3 unnumbered">
<h3>Prediction</h3>
<p>In many situations, a set of inputs <span class="math inline">\(X\)</span> are readily available, but the output <span class="math inline">\(Y\)</span> cannot be easily obtained. In this setting, since the error term averages to zero, we can predict <span class="math inline">\(Y\)</span> using</p>
<p><span class="math display">\[ \hat{Y} = \hat{f}(X) \]</span></p>
<p>where <span class="math inline">\(\hat{f}\)</span> represents our estimate for <span class="math inline">\(f\)</span>, and <span class="math inline">\(\hat{Y}\)</span> represents the resulting prediction for <span class="math inline">\(Y\)</span>. Like in the example above about predicting housing prices.</p>
<p>We can measure the accuracy of <span class="math inline">\(\hat{Y}\)</span> by using a <strong>cost function</strong>. In the regression models, the most commonly-used measure is the <em>mean squared error</em> (MSE), given by</p>
<p><span class="math display">\[ MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{f}(x_i))^2\]</span></p>
</div>
<div id="inference" class="section level3 unnumbered">
<h3>Inference</h3>
<p>We are often interested in understanding the way that <span class="math inline">\(Y\)</span> is affected as <span class="math inline">\(X_1 , X_2 ,\ldots, X_p\)</span> change. In this situation we wish to estimate <span class="math inline">\(f\)</span> , but our goal is not necessarily to make predictions for <span class="math inline">\(Y\)</span>. We instead want to understand the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, or more specifically, to understand how <span class="math inline">\(Y\)</span> changes as a function of <span class="math inline">\(X_1 , X_2 ,\ldots, X_p\)</span>. In this case, one may be interested in answering the following questions:</p>
<ul>
<li>Which predictors are associated with the response?</li>
<li>What is the relationship between the response and each predictor?</li>
<li>Can the relationship between Y and each predictor be adequately summarized using a linear equation, or is the relationship more complicated?</li>
</ul>
</div>
</div>
<div id="simple-linear-regression" class="section level2">
<h2><span class="header-section-number">1.4</span> Simple Linear Regression</h2>
<div id="model" class="section level3">
<h3><span class="header-section-number">1.4.1</span> Model</h3>
<p><em>Simple linear regression</em> is a very straightforward approach for predicting a quantitative response <span class="math inline">\(Y\)</span> on the basis of a single predictor variable <span class="math inline">\(X\)</span>. It assumes that there is approximately a linear relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Mathematically, we can write this linear relationship as</p>
<p><span class="math display">\[ Y = \beta_0 + \beta_1 X + \epsilon \]</span> <span class="math display">\[Y \approx \beta_0 + \beta_1 X\]</span></p>
<p>where <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are two unknown constants that represent the <em>intercept</em> and <em>slope</em>, also known as <strong><em>coefficients</em></strong> or <em>parameters</em>, and <span class="math inline">\(\epsilon\)</span> is the error term.</p>
<p>Given some estimates <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span> for the model coefficients, we predict future inputs <span class="math inline">\(x\)</span> using</p>
<p><span class="math display">\[\hat{y} = \hat{\beta_0} + \hat{\beta_1} x\]</span></p>
<p>where <span class="math inline">\(\hat{y}\)</span> indicates a prediction of <span class="math inline">\(Y\)</span> on the basis of <span class="math inline">\(X = x\)</span>. The <em>hat</em> symbol, <span class="math inline">\(\hat{}\)</span>, denotes an estimated value.</p>
</div>
<div id="estimating-the-coefficients" class="section level3">
<h3><span class="header-section-number">1.4.2</span> Estimating the Coefficients</h3>
<p>Let <span class="math inline">\(\hat{y}_i = \hat{\beta_0} + \hat{\beta_1} x_i\)</span> be the prediction for <span class="math inline">\(Y\)</span> based on the <span class="math inline">\(i\)</span>th value of <span class="math inline">\(X\)</span>. Then <span class="math inline">\(e_i = y_i - \hat{y}_i\)</span> represents the <span class="math inline">\(i\)</span>th <strong><em>residual</em></strong>.</p>
<p>We define the <strong><em>residual sum of squares</em></strong> (<strong>RSS</strong>) as</p>
<p><span class="math display">\[  \begin{aligned}
RSS &amp;= e_1^2 + e_2^2 + \ldots + e_n^2 \\
    &amp;= \sum_{i=1}^{n} e_i^2
 \end{aligned}  \]</span></p>
<p>or equivantly as</p>
<p><span class="math display">\[ \begin{aligned}
RSS &amp;= (y_1 - \hat{\beta_0} - \hat{\beta_1} x_1)^2 + (y_2 - \hat{\beta_0} - \hat{\beta_1} x_2)^2 + \ldots + (y_n - \hat{\beta_0} - \hat{\beta_1} x_n)^2 \\
    &amp;= \sum_{i=1}^{n} (y_i - \hat{\beta_0} - \hat{\beta_1} x_i)^2
\end{aligned} \]</span></p>
<p>The <em>least squares</em> approach chooses <span class="math inline">\(\hat{\beta_0}\)</span> and <span class="math inline">\(\hat{\beta_1}\)</span> to minimize the RSS. The minimizing values can be show to be</p>
<p><span class="math display">\[  \begin{aligned}
\hat{\beta_1} &amp;=  \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})  }{\sum_{i=1}^{n} (x_i - \bar{x})^2 } \\
\text{and} \\
\hat{\beta_0} &amp;= \bar{y} - \hat{\beta_1} \bar{x}
\end{aligned}  \]</span></p>
<p>where <span class="math inline">\(\bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i\)</span> and <span class="math inline">\(\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i\)</span> are the sample means.</p>
</div>
<div id="assessing-the-accuracy-of-the-coefficient-estimates" class="section level3">
<h3><span class="header-section-number">1.4.3</span> Assessing the Accuracy of the Coefficient Estimates</h3>
<p>The standard error of an estimator reflects how it varies under repeated sampling. We have</p>
<p><span class="math display">\[ \text{SE}(\hat{\beta_1})^2 =  \frac{\sigma^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \]</span></p>
<p><span class="math display">\[ \text{SE}(\hat{\beta_0})^2 = \sigma^2 \bigg[ \frac{1}{n} +  \frac{\bar{x}^2}{\sum_{i=1}^{n} (x_i - \bar{x})^2} \bigg] \]</span></p>
<p>where <span class="math inline">\(\sigma^2 = Var(\epsilon)\)</span></p>
<p>In general, <span class="math inline">\(\sigma^2\)</span> is know known, but can be estimated from the data. The estimate of <span class="math inline">\(\sigma\)</span> is known as the <em>residual standard error</em>, and is given by</p>
<p><span class="math display">\[ \text{RSE} = \sqrt{\frac{\text{RSS}}{(n-2)}} \]</span></p>
<p>These standard errors can be used to compute <em>confidence intervals</em>. A <span class="math inline">\(95\%\)</span> confidence interval is defined as a range of values such that with <span class="math inline">\(95\%\)</span> probability, the range will contain the true unknown value of the parameter. It has the form</p>
<p><span class="math display">\[ \hat{\beta_1} \pm 2 \cdot \text{SE}(\hat{\beta_1}) \]</span></p>
<p>That is, there is approximately a <span class="math inline">\(95\%\)</span> chance that the interval</p>
<p><span class="math display">\[ \bigg[  \hat{\beta_1} - 2 \cdot \text{SE}(\hat{\beta_1}), \hat{\beta_1} + 2 \cdot \text{SE}(\hat{\beta_1})   \bigg] \]</span></p>
<p>will contain the true value of <span class="math inline">\(\beta_1\)</span>. Similarly, a confidence interval for <span class="math inline">\(\beta_0\)</span> approximately takes the form</p>
<p><span class="math display">\[ \hat{\beta_0} \pm 2 \cdot \text{SE}(\hat{\beta_0}) \]</span></p>
<div id="hypothesis-testing" class="section level4 unnumbered">
<h4>Hypothesis testing</h4>
<p>Standard errors can also be used to perform <em>hypothesis tests</em> on the coefficients. The most common hypothesis test involves testing the <em>null hypothesis</em> of</p>
<p><span class="math display">\[ H_0 : \text{There is no relationship between} \, X \, \text{and} \, Y \]</span></p>
<p>versus the <em>alternative hypothesis</em></p>
<p><span class="math display">\[ H_1 : \text{There is some relationship between} \, X \, \text{and} \, Y \]</span></p>
<p>Mathematically, this corresponds to testing</p>
<p><span class="math display">\[ H_0 : \beta_1 = 0 \]</span></p>
<p>versus</p>
<p><span class="math display">\[ H_1 : \beta_1 \neq 0 \]</span></p>
<p>since if <span class="math inline">\(\beta_1 = 0\)</span> then the simple linear regression model reduces to <span class="math inline">\(Y = \beta_0 + \epsilon\)</span>, and <span class="math inline">\(X\)</span> is not associated with <span class="math inline">\(Y\)</span>.</p>
<p>To test the null hypothesis <span class="math inline">\(H_0\)</span>, we compute a <strong><em>t-statistic</em></strong>, given by</p>
<p><span class="math display">\[ t = \frac{\hat{\beta_1} - 0}{\text{SE}(\hat{\beta_1})} \]</span></p>
<p>This will have a <span class="math inline">\(t\)</span>-distribution (<em>Student</em>) with <span class="math inline">\(n-2\)</span> degrees of freedom, assuming <span class="math inline">\(\beta_1=0\)</span>.</p>
<p>Using statistical software, it is easy to compute the probability of observing any value equal to <span class="math inline">\(|t|\)</span> or larger. We call this probability the <strong><em>p-value</em></strong>.</p>
<p>If p-value is small enough (typically under <span class="math inline">\(0.01\)</span> (<span class="math inline">\(1\%\)</span> error) or <span class="math inline">\(0.05\)</span> (<span class="math inline">\(5\%\)</span> error)) we reject the null hypothesis, that is we declare a relationship to exist between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
</div>
</div>
<div id="assessing-the-accuracy-of-the-model" class="section level3">
<h3><span class="header-section-number">1.4.4</span> Assessing the Accuracy of the Model</h3>
<p>Once we have rejected the null hypothesis in favor of the alternative hypothesis, it is natural to want to quantify the extent to which the model fits the data. The quality of a linear regression fit is typically assessed using two related quantities: the <em>residual standard error</em> (RSE) and the <span class="math inline">\(R^2\)</span> statistic.</p>
<div id="residual-standard-error-rse" class="section level4 unnumbered">
<h4>Residual Standard Error (RSE)</h4>
<p>The Residual Standard Error (RSE) is computed using the formula</p>
<p><span class="math display">\[ \text{RSE} = \sqrt{\frac{\text{RSS}}{(n-2)}} =  \sqrt{\frac{ \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 }{(n-2)}} \]</span></p>
<p>recall that RSS is the <em>residual sum-of-squares</em>.</p>
<p>The RSE is considered a measure of the lack of fit of the model to the data. If the predictions obtained using the model are very close to the true outcome values (if <span class="math inline">\(\hat{y}_i = y_i\)</span> for <span class="math inline">\(i = 1, \ldots, n\)</span>) then RSE will be small, and we can conclude that the model fits the data very well. On the other hand, if <span class="math inline">\(\hat{y}_i\)</span> is very far from <span class="math inline">\(y_i\)</span> for one or more observations, then the RSE may be quite large, indicating that the model doesn’t fit the data well.</p>
</div>
<div id="r2-statistic" class="section level4 unnumbered">
<h4><span class="math inline">\(R^2\)</span> Statistic</h4>
<p>To calculate <span class="math inline">\(R^2\)</span>, we use the formula</p>
<p><span class="math display">\[ R^2 = \frac{\text{TSS} - \text{RSS}}{\text{TSS}} = 1- \frac{\text{RSS}}{\text{TSS}} \]</span></p>
<p>where <span class="math inline">\(\text{TSS} = \sum (y_i - \bar{y})^2\)</span> is the <em>total sum of squared</em>.</p>
<p><span class="math inline">\(R^2\)</span> measures the <em>proportion of variability in</em> <span class="math inline">\(Y\)</span> <em>that can be explained using</em> <span class="math inline">\(X\)</span>. An <span class="math inline">\(R^2\)</span> statistic that is close to 1 indicates that a large proportion of the variability in the response has been explained by the regression. A number near 0 indicates that the regression did not explain much of the variability in the response; this might occur because the linear model is wrong, or the inherent error <span class="math inline">\(\sigma^2\)</span> is high, or both.</p>
<p>It can be shown that in this simple linear linear regression setting that <span class="math inline">\(R^2 = r^2\)</span>, where <span class="math inline">\(r\)</span> is the correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>:</p>
<p><span class="math display">\[ r = \frac{cov(X,Y)}{\sigma_X \sigma_Y} \]</span></p>
<p align="right">
◼
</p>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="pw-1.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["Machine-Learning.pdf"],
"toc": {
"collapse": "section"
},
"toolbar": {
"position": "fixed"
},
"search": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
