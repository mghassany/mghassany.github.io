[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"","code":""},{"path":"discriminant-analysis.html","id":"discriminant-analysis","chapter":"1 Discriminant Analysis","heading":"1 Discriminant Analysis","text":"Slides chapter: Discriminant analysis popular method multiple-class classiﬁcation. start first Linear Discriminant Analysis (LDA).","code":""},{"path":"discriminant-analysis.html","id":"introduction","chapter":"1 Discriminant Analysis","heading":"1.1 Introduction","text":"saw previously, Logistic regression involves directly modeling \\(\\mathbb{P} (Y = k|X = x)\\) using logistic function, case two response classes. logistic regression, model conditional distribution response \\(Y\\), given predictor(s) \\(X\\). now consider alternative less direct approach estimating probabilities. alternative approach, model distribution predictors \\(X\\) separately response classes (.e. given \\(Y\\)), use Bayes’ theorem flip around estimates \\(\\mathbb{P} (Y = k|X = x)\\). distributions assumed Normal, turns model similar form logistic regression.logistic regression?\nneed another method, logistic regression? several reasons:classes well-separated, parameter estimates logistic regression model surprisingly unstable. Linear discriminant analysis suﬀer problem.\\(n\\) small distribution predictors \\(X\\) approximately normal classes, linear discriminant model stable logistic regression model.Linear discriminant analysis popular two response classes.","code":""},{"path":"discriminant-analysis.html","id":"bayes-theorem","chapter":"1 Discriminant Analysis","heading":"1.2 Bayes’ Theorem","text":"Bayes’ theorem stated mathematically following equation:\\[ P(| B) = \\frac{P(\\cap B)}{P(B)} =  \\frac{P(B|) P()}{P(B)}\\]\\(\\) \\(B\\) events \\(P(B) \\neq 0\\).\\(P(| B)\\), conditional probability, probability observing event \\(\\) given \\(B\\) true. called posterior probability.\\(P()\\), called prior, initial degree belief .\\(P(B)\\) likelihood.posterior probability can written memorable form :Posterior probability \\(\\propto\\) Likelihood \\(\\times\\) Prior probability.Extended form:Suppose partition \\(\\{A_i\\}\\) sample space, even space given conceptualized terms \\(P(A_j)\\) \\(P(B | A_j)\\). useful compute \\(P(B)\\) using law total probability:\\[ P(B) = \\sum_j P(B|A_j) P(A_j) \\]\\[ \\Rightarrow P(A_i|B) = \\frac{P(B|A_i) P(A_i)}{\\sum_j P(B|A_j) P(A_j)} \\]Bayes’ Theorem Classification:Suppose wish classify observation one \\(K\\) classes, \\(K \\geq 2\\). words, qualitative response variable \\(Y\\) can take \\(K\\) possible distinct unordered values.Let \\(\\pi_k\\) represent overall prior probability randomly chosen observation comes \\(k\\)-th class; probability given observation associated \\(k\\)-th category response variable \\(Y\\).Let \\(f_k(X) \\equiv P(X = x|Y = k)\\) denote density function \\(X\\) observation comes \\(k\\)-th class. words, \\(f_k(x)\\) relatively large high probability observation \\(k\\)-th class \\(X \\approx x\\), \\(f_k(x)\\) small unlikely observation \\(k\\)-th class \\(X \\approx x\\). Bayes’ theorem states \\[\\begin{equation}\nP(Y=k|X=x) = \\frac{ \\pi_k f_k(x)}{\\sum_{c=1}^K \\pi_c f_c(x)}\n\\tag{1.1}\n\\end{equation}\\]last chapter, use abbreviation \\(p_k(X) =P(Y = k|X)\\).equation stated Bayes’ theorem suggests instead directly computing \\(p_k(X)\\) logistic regression, can simply plug estimates \\(\\pi_k\\) \\(f_k(X)\\) equation. general, estimating \\(\\pi_k\\) easy (fraction training observations belong \\(k\\)-th class). estimating \\(f_k(X)\\) tends challenging.Recall \\(p_k(x)\\) posterior probability observation \\(X=x\\) belongs \\(k\\)-th class.can find way estimate \\(f_k(X)\\), can develop classifier lowest possibe error rate classifiers.","code":""},{"path":"discriminant-analysis.html","id":"lda-for-p1","chapter":"1 Discriminant Analysis","heading":"1.3 LDA for \\(p=1\\)","text":"Assume \\(p=1\\), mean one predictor. like obtain estimate \\(f_k(x)\\) can plug Equation (1.1) order estimate \\(p_k(x)\\). classify observation class \\(p_k(x)\\) greatest.order estimate \\(f_k(x)\\), first make assumptions form.Suppose assume \\(f_k(x)\\) normal (Gaussian). one-dimensional setting, normal density take form\\[\\begin{equation}\nf_k(x)= \\frac{1}{\\sigma_k\\sqrt{2\\pi}} \\exp \\big( - \\frac{1}{2\\sigma_k^2 } (x-\\mu_k)^2\\big)\n\\tag{1.2}\n\\end{equation}\\]\\(\\mu_k\\) \\(\\sigma_k^2\\) mean variance parameters \\(k\\)-th class. Let us assume \\(\\sigma_1^2 = \\ldots = \\sigma_K^2 = \\sigma^2\\) (means shared variance term across \\(K\\) classes). Plugging Eq. (1.2) Bayes formula Eq. (1.1) get,\\[\\begin{equation}\np_k(x) = \\frac{  \\pi_k \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2} \\big(\\frac{x-\\mu_k}{\\sigma}\\big)^2 } }{  \\sum_{c=1}^K  \\pi_c \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2} \\big(\\frac{x-\\mu_c}{\\sigma}\\big)^2 } }\n\\tag{1.3}\n\\end{equation}\\]Note \\(\\pi_k\\) \\(\\pi_c\\) denote prior probabilities. \\(\\pi\\) mathematical constant \\(\\pi \\approx 3.14159\\).classify value \\(X = x\\), need see \\(p_k(x)\\) largest. Taking logs, discarding terms depend \\(k\\), see equivalent assigning \\(x\\) class largest discriminant score:\\[\\begin{equation}\n\\delta_k(x) = x.\\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + \\log (\\pi_k)\n\\tag{1.4}\n\\end{equation}\\]Note \\(\\delta_k(x)\\) linear function \\(x\\).decision surfaces (e.g. decision boundaries) linear discriminant classifiers defined linear equations \\(\\delta_k(x) = \\delta_c(x)\\), classes \\(k\\neq c\\). represents set values \\(x\\) probability belonging classes \\(k\\) \\(c\\) , \\(0.5\\).Example: \\(K=2\\) \\(\\pi_1=\\pi_2\\), desicion boundary \\(x=\\frac{\\mu_1+\\mu2}{2}\\) (Prove !).example \\(\\mu_1=-1.5\\), \\(\\mu_2=1.5\\), \\(\\pi_1=\\pi_2=0.5\\) \\(\\sigma^2=1\\) shown following figureSee  video  understand decision boundary (Applied logistic regression).See  video  understand decision boundary (Applied logistic regression).classify new point according density highest, priors diﬀerent take account well, compare \\(\\pi_k f_k(x)\\). right following figure, favor pink class (remark decision boundary shifted left).classify new point according density highest, priors diﬀerent take account well, compare \\(\\pi_k f_k(x)\\). right following figure, favor pink class (remark decision boundary shifted left).","code":""},{"path":"discriminant-analysis.html","id":"estimating-the-parameters","chapter":"1 Discriminant Analysis","heading":"1.4 Estimating the parameters","text":"Typically don’t know parameters; just training data. case simply estimate parameters plug rule.Let \\(n\\) total number training observations, \\(n_k\\) number training observations \\(k\\)-th class. following estimates used:\\[\\begin{align*}\n\\hat{\\pi}_k &= \\frac{n_k}{n} \\\\\n\\hat{\\mu}_k &= \\frac{1}{n_k} \\sum_{: y_i=k} x_i \\\\\n\\hat{\\sigma}^2 &= \\frac{1}{n - K} \\sum_{k=1}^K \\sum_{: y_i=k} (x_i-\\hat{\\mu}_k)^2 \\\\\n&= \\sum_{k=1}^K \\frac{n_k-1}{n - K} . \\hat{\\sigma}_k^2\n\\end{align*}\\]\\(\\hat{\\sigma}_k^2 = \\frac{1}{n_k-1}\\sum_{: y_i=k}(x_i-\\hat{\\mu}_k)^2\\) usual formula estimated variance -\\(k\\)-th class.linear discriminant analysis (LDA) classifier plugs estimates Eq. (1.4) assigns observation \\(X=x\\) class \\[\\begin{equation}\n\\hat{\\delta}_k(x) = x.\\frac{\\hat{\\mu}_k}{\\hat{\\sigma}^2} - \\frac{\\hat{\\mu}_k^2}{2\\hat{\\sigma}^2} + \\log (\\hat{\\pi}_k)\n\\tag{1.5}\n\\end{equation}\\]largest.discriminant functions Eq. (1.5) linear functions \\(x\\).Recall assumed observations come normal distribution common variance \\(\\sigma^2\\).","code":""},{"path":"discriminant-analysis.html","id":"lda-for-p-1","chapter":"1 Discriminant Analysis","heading":"1.5 LDA for \\(p > 1\\)","text":"Let us now suppose multiple predictors. assume \\(X=(X_1,X_2,\\ldots,X_p)\\) drawn multivariate Gaussian distribution (assuming common covariance matrix, e.g. variances case \\(p=1\\)). multivariate Gaussian distribution assumes individual predictor follows one-dimensional normal distribution Eq. (1.2), correlation pair predictors.indicate \\(p\\)-dimensional random variable \\(X\\) multivariate Gaussian distribution, write \\(X \\sim \\mathcal{N}(\\mu,\\Sigma)\\). \\[ \\mu = E(X) = \\begin{pmatrix}\n    \\mu_1 \\\\\n    \\mu_2 \\\\\n    \\vdots \\\\\n    \\mu_p\n\\end{pmatrix} \\],\n\\[ \\Sigma = Cov(X) = \\begin{pmatrix}\n    \\sigma_1^2 & Cov[X_1, X_2]  & \\dots  & Cov[X_1, X_p] \\\\\n    Cov[X_2, X_1] & \\sigma_2^2  & \\dots  & Cov[X_2, X_p] \\\\\n    \\vdots & \\vdots &  \\ddots & \\vdots \\\\\n    Cov[X_p, X_1] & Cov[X_p, X_2]  & \\dots  & \\sigma_p^2\n\\end{pmatrix}  \\]\\(\\Sigma\\) \\(p\\times p\\) covariance matrix \\(X\\).Formally, multivariate Gaussian density deﬁned \\[f(x) = \\frac{1}{(2\\pi)^{p/2} |\\Sigma|^{1/2}} \\exp \\bigg( - \\frac{1}{2} (x-\\mu)^T \\Sigma^{-1} (x-\\mu) \\bigg)\n\\]Plugging density function \\(k\\)-th class, \\(f_k(X=x)\\), Eq. (1.1) reveals Bayes classifier assigns observation \\(X=x\\) class \\[\\begin{equation}\n\\delta_k(x) = x^T \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\Sigma^{-1}  \\mu_k + \\log \\pi_k\n\\tag{1.6}\n\\end{equation}\\]largest. vector/matrix version (1.4).example shown following figure. Three equally-sized Gaussian classes shown class-specific mean vectors common covariance matrix (\\(\\pi_1=\\pi_2=\\pi_3=1/3\\)). three ellipses represent regions contain \\(95\\%\\) probability three classes. dashed lines Bayes decision boundaries.Recall decision boundaries represent set values \\(x\\) \\(\\delta_k(x)=\\delta_c(x)\\); .e. \\(k\\neq c\\).\\[ x^T \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\Sigma^{-1}  \\mu_k = x^T \\Sigma^{-1} \\mu_c - \\frac{1}{2} \\mu_c^T \\Sigma^{-1}  \\mu_c  \\]Note three lines representing Bayes decision boundaries three pairs classes among three classes. , one Bayes decision boundary separates class 1 class 2, one separates class 1 class 3, one separates class 2 class 3. three Bayes decision boundaries divide predictor space three regions. Bayes classiﬁer classify observation according region located., need estimate unknown parameters \\(\\mu_1,\\ldots,\\mu_k,\\) \\(\\pi_1,\\ldots,\\pi_k,\\) \\(\\Sigma\\); formulas similar used one-dimensional case. assign new observation \\(X = x\\), LDA plugs estimates Eq. (1.6) classiﬁes class \\(\\delta_k(x)\\) largest.Note Eq. (1.6) \\(\\delta_k(x)\\) linear function \\(x\\); , LDA decision rule depends \\(x\\) linear combination elements (e.g. decision boundaries linear). reason word linear LDA.","code":""},{"path":"discriminant-analysis.html","id":"making-predictions","chapter":"1 Discriminant Analysis","heading":"1.6 Making predictions","text":"estimates \\(\\hat{\\delta}_k(x)\\), can turn estimates class probabilities:\\[ \\hat{P}(Y=k|X=x)= \\frac{e^{\\hat{\\delta}_k(x)}}{\\sum_{c=1}^K e^{\\hat{\\delta}_c(x)}} \\]classifying largest \\(\\hat{\\delta}_k(x)\\) amounts classifying class \\(\\hat{P}(Y=k|X=x)\\) largest.\\(K=2\\), classify class 2 \\(\\hat{P}(Y=2|X=x) \\geq 0.5\\), else class \\(1\\).","code":""},{"path":"discriminant-analysis.html","id":"other-forms-of-discriminant-analysis","chapter":"1 Discriminant Analysis","heading":"1.7 Other forms of Discriminant Analysis","text":"\\[P(Y=k|X=x) = \\frac{ \\pi_k f_k(x)}{\\sum_{c=1}^K \\pi_c f_c(x)}\\]saw \\(f_k(x)\\) Gaussian densities, whith covariance matrix \\(\\Sigma\\) class, leads Linear Discriminant Analysis (LDA).altering forms \\(f_k(x)\\), get different classifiers.Gaussians different \\(\\Sigma_k\\) class, get Quadratic Discriminant Analysis (QDA).\\(f_k(x) = \\prod_{j=1}^p f_{jk}(x_j)\\) (conditional independence model) class get Naive Bayes. (Gaussian, mean \\(\\Sigma_k\\) diagonal, e.g. \\(Cov(X_i,X_j)=0 \\,\\, \\forall \\, \\, 1\\leq ,j \\leq p\\)).Many forms proposing specific density models \\(f_k(x)\\), including nonparametric approaches.","code":""},{"path":"discriminant-analysis.html","id":"quadratic-discriminant-analysis-qda","chapter":"1 Discriminant Analysis","heading":"1.7.1 Quadratic Discriminant Analysis (QDA)","text":"Like LDA, QDA classiﬁer results assuming observations class drawn Gaussian distribution, plugging estimates parameters Bayes’ theorem order perform prediction.However, unlike LDA, QDA assumes class covariance matrix. assumption, Bayes classiﬁer assigns observation \\(X = x\\) class \\[\\begin{align*}\n\\delta_k(x) &= - \\frac{1}{2} (x-\\mu)^T \\Sigma_k^{-1} (x-\\mu) - \\frac{1}{2} \\log |\\Sigma_k| + \\log \\pi_k \\\\\n            &= - \\frac{1}{2} x^T \\Sigma_k^{-1} x + \\frac{1}{2} x^T \\Sigma_k^{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\Sigma_k^{-1} \\mu_k - \\frac{1}{2} \\log |\\Sigma_k| + \\log \\pi_k\n\\end{align*}\\]largest.Unlike LDA, quantity \\(x\\) appears quadratic function QDA. QDA gets name.decision boundary QDA non-linear. quadratic (curve).","code":""},{"path":"discriminant-analysis.html","id":"naive-bayes","chapter":"1 Discriminant Analysis","heading":"1.7.2 Naive Bayes","text":"use Naive Bayes classifier features independant class. useful \\(p\\) large (unklike LDA QDA).Naive Bayes assumes \\(\\Sigma_k\\) diagonal, \\[\\begin{align*}\n\\delta_k(x) &\\propto \\log \\bigg[\\pi_k \\prod_{j=1}^p f_{kj}(x_j) \\bigg] \\\\\n            &= -\\frac{1}{2} \\sum_{j=1}^p \\frac{(x_j-\\mu_{kj})^2}{\\sigma_{kj}^2} + \\log \\pi_k\n\\end{align*}\\]can used mixed feature vectors (qualitative quantitative). \\(X_j\\) qualitative, replace \\(f_{kj}(x_j)\\) probability mass function (histogram) discrete categories.","code":""},{"path":"discriminant-analysis.html","id":"lda-vs-logistic-regression","chapter":"1 Discriminant Analysis","heading":"1.8 LDA vs Logistic Regression","text":"logistic regression LDA methods closely connected. Consider two-class setting \\(p =1\\) predictor, let \\(p_1(x)\\) \\(p_2(x)=1−p_1(x)\\) probabilities observation \\(X = x\\) belongs class 1 class 2, respectively. LDA framework, can see Eq. (1.4) (bit simple algebra) log odds given \\[ \\log \\bigg(\\frac{p_1(x)}{1-p_1(x)}\\bigg) = \\log \\bigg(\\frac{p_1(x)}{p_2(x)}\\bigg) = c_0 + c_1 x\\]\\(c_0\\) \\(c_1\\) functions \\(\\mu_1, \\mu_2,\\) \\(\\sigma^2\\).hand, know logistic regression\\[ \\log \\bigg(\\frac{p_1}{1-p_1}\\bigg) = \\beta_0 + \\beta_1 x\\]equations linear functions \\(x\\). Hence logistic regression LDA produce linear decision boundaries. diﬀerence two approaches lies fact \\(\\beta_0\\) \\(\\beta_1\\) estimated using maximum likelihood, whereas \\(c_0\\) \\(c_1\\) computed using estimated mean variance normal distribution. connection LDA logistic regression also holds multidimensional data \\(p> 1\\).Logistic regression uses conditional likelihood based \\(P(Y|X)\\) (known discriminative learning).LDA uses full likelihood based \\(P(X,Y )\\) (known \ngenerative learning).Despite differences, practice results often similar.Remark: Logistic regression can also fit quadratic boundaries like QDA, explicitly including quadratic terms model.\n◼\n","code":""},{"path":"lab.html","id":"lab","chapter":"Lab","heading":"Lab","text":"free apply lab  Python. codes given lab main instructions  codes. adapt use Python.use Python, verify scikit-learn installed verify version (least 0.21)., going use lda() qda() functions MASS library.Python, can use :session going analyse Social_Network_Ads dataset . dataset contains informations users social network bought specified product. going model variable Purchased function Age EstimatedSalary. fit using models Logistic Regression, LDA, QDA, Naive Bayes.","code":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"},{"path":"lab.html","id":"logistic-regression","chapter":"Lab","heading":"Logistic Regression","text":"1. First, let’s pre-processing steps fit logistic regression model. Please read understand well following code (read comments!). copy necessary report (remove comments!).can download dataset .now logistic regression model stored classifier.logreg. model Purchased function Age EstimatedSalary.","code":"\n\n# Loading the dataset.. I have putted it into a folder called \"datasets\"\ndataset <- read.csv('http://www.mghassany.com/courses/MLcourse/datasets/Social_Network_Ads.csv')\n\n# Describing and Exploring the dataset\n\nstr(dataset) # to show the structure of the dataset. \nsummary(dataset) # will show some statistics of every column. \n# Remark what it shows when the column is a numerical or categorical variable.\n# Remark that it has no sense for the variable User.ID\n\nboxplot(Age ~ Purchased, data=dataset, col = \"blue\", main=\"Boxplot Age ~ Purchased\")\n# You know what is a boxplot right? I will let you interpret it.\nboxplot(EstimatedSalary ~ Purchased, data=dataset,col = \"red\",\n main=\"Boxplot EstimatedSalary ~ Purchased\")\n# Another boxplot\n\naov(EstimatedSalary ~Purchased, data=dataset)\n# Anova test, but we need to show the summary of \n# it in order to see the p-value and to interpret.\n\nsummary(aov(EstimatedSalary ~Purchased, data=dataset))\n# What do you conclude ?\n# Now another anova test for the variable Age\nsummary(aov(Age ~Purchased, data=dataset))\n\n# There is a categorical variable in the dataset, which is Gender.\n# Of course we cannot show a boxplot of Gender and Purchased.\n# But we can show a table, or a mosaic plot, both tell the same thing.\ntable(dataset$Gender,dataset$Purchased)\n# Remark for the function table(), that\n# in lines we have the first argument, and in columns we have the second argument.\n# Don't forget this when you use table() to show a confusion matrix!\nmosaicplot(~ Purchased + Gender, data=dataset,\n  main = \"MosaicPlot of two categorical variables: Puchased & Gender\",\n  color = 2:3, las = 1)\n\n# since these 2 variables are categorical, we can apply\n# a Chi-square test. The null hypothesis is the independance between\n# these variables. You will notice that p-value = 0.4562 which is higher than 0.05 (5%)\n# so we cannot reject the null hypothesis. \n# conclusion: there is no dependance between Gender and Purchased (who\n# said that women buy more than men? hah!)\n\nchisq.test(dataset$Purchased, dataset$Gender)\n\n# Let's say we want to remove the first two columns as we are not going to use them.\n# But, we can in fact use a categorical variable as a predictor in logistic regression.\n# It will treat it the same way as in regression. Check Appendix C.\n# Try it by yourself if you would like to.\ndataset = dataset[3:5]\nstr(dataset) # show the new structure of dataset\n\n\n# splitting the dataset into training and testing sets\nlibrary(caTools)\nset.seed(123) # CHANGE THE VALUE OF SEED. PUT YOUR STUDENT'S NUMBER INSTEAD OF 123.\nsplit = sample.split(dataset$Purchased, SplitRatio = 0.75)\ntraining_set = subset(dataset, split == TRUE)\ntest_set = subset(dataset, split == FALSE)\n\n# scaling\n# So here, we have two continuous predictors, Age and EstimatedSalary.\n# There is a very big difference in their scales (units).\n# That's why we scale them. But it is not always necessary.\n\ntraining_set[-3] <- scale(training_set[-3]) #only first two columns\ntest_set[-3] <- scale(test_set[-3])\n\n# Note that, we replace the columns of Age and EstimatedSalary in the training and\n# test sets but their scaled versions. I noticed in a lot of reports that you scaled\n# but you did not do the replacing.\n# Note too that if you do it column by column you will have a problem because \n# it will replace the column by a matrix, you need to retransform it to a vector then.\n# Last note, to call the columns Age and EstimatedSalary we can it like I did or \n# training_set[c(1,2)] or training_set[,c(1,2)] or training_set[,c(\"Age\",\"EstimatedSalary\")]\n\n\n# logistic regression\n\nclassifier.logreg <- glm(Purchased ~ Age + EstimatedSalary , family = binomial, data=training_set)\nclassifier.logreg\nsummary(classifier.logreg)\n\n# prediction\npred.glm = predict(classifier.logreg, newdata = test_set[,-3], type=\"response\")\n# Do not forget to put type response. \n# By the way, you know what you get when you do not put it, right?\n\n# Now let's assign observations to classes with respect to the probabilities\npred.glm_0_1 = ifelse(pred.glm >= 0.5, 1,0)\n# I created a new vector, because we need the probabilities later for the ROC curve.\n\n# show some values of the vectors\nhead(pred.glm)\nhead(pred.glm_0_1)\n\n# confusion matrix\ncm = table(test_set[,3], pred.glm_0_1)\ncm\n# First line to store it into cm, second line to show the matrix! \n\n# You remember my note about table() function and the order of the arguments?\ncm = table(pred.glm_0_1, test_set[,3])\ncm\n\n# You can show the confusion matrix in a mosaic plot by the way\nmosaicplot(cm,col=sample(1:8,2)) # colors are random between 8 colors.\n\n# ROC\nrequire(ROCR)\nscore <- prediction(pred.glm,test_set[,3]) # we use the predicted probabilities not the 0 or 1\nperformance(score,\"auc\") # y.values\nplot(performance(score,\"tpr\",\"fpr\"),col=\"green\")\nabline(0,1,lty=8)"},{"path":"lab.html","id":"linear-discriminant-analysis-lda","chapter":"Lab","heading":"Linear Discriminant Analysis (LDA)","text":"Let us apply linear discriminant analysis (LDA) now. First make use lda() function package MASS. Second, going create model predict classes without using lda() function. visualize decision boundary LDA.2. Fit LDA model Purchased function Age EstimatedSalary. Name model classifier.lda.3. Call classifier.lda understand compute.Plus: enter following returned list summary information concerning computation:4. test set, predict probability purchasing product users using model classifier.lda. Remark predict using LDA, obtain list instead matrix, str() predictions see get.Remark: get predicted class , without obligated round predictions logistic regression.5. Compute confusion matrix compare predictions results obtained LDA ones obtained logistic regression. remark? (Hint: compare accuracy)6. Now let us plot decision boundary obtained LDA. saw course decision boundary LDA represent set values \\(x\\) \\(\\delta_k(x) = \\delta_c(x)\\). Recall \n\\[ \\delta_k(X) = x^T \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\Sigma^{-1}  \\mu_k + \\log \\pi_k \\]case, 2 classes (\\(K=2\\)) 2 predictors (\\(p=2\\)). decision boundary (linear case LDA, line case since \\(p=2\\)) verify equation \\(\\delta_0(x) = \\delta_1(x)\\) Since two classes “0” “1”. case LDA leads linear boundary easy plotted. complicated cases difficult manually simplify equations plot decision boundary. Anyway, smart method plot (little bit costy) decision boundary R using function contour(), corresponding code following (must adapt use plot decision boundary):","code":"\nlibrary(MASS)\nclassifier.lda <- lda(Purchased~Age+EstimatedSalary, data=training_set)\nclassifier.lda$prior\nclassifier.lda$means\n# create a grid corresponding to the scales of Age and EstimatedSalary\n# and fill this grid with lot of points\nX1 = seq(min(training_set[, 1]) - 1, max(training_set[, 1]) + 1, by = 0.01)\nX2 = seq(min(training_set[, 2]) - 1, max(training_set[, 2]) + 1, by = 0.01)\ngrid_set = expand.grid(X1, X2)\n# Adapt the variable names\ncolnames(grid_set) = c('Age', 'EstimatedSalary')\n\n# plot 'Estimated Salary' ~ 'Age'\nplot(test_set[, 1:2],\n     main = 'Decision Boundary LDA',\n     xlab = 'Age', ylab = 'Estimated Salary',\n     xlim = range(X1), ylim = range(X2))\n\n# color the plotted points with their real label (class)\npoints(test_set[1:2], pch = 21, bg = ifelse(test_set[, 3] == 1, 'green4', 'red3'))\n\n# Make predictions on the points of the grid, this will take some time\npred_grid = predict(classifier.lda, newdata = grid_set)$class\n\n# Separate the predictions by a contour\ncontour(X1, X2, matrix(as.numeric(pred_grid), length(X1), length(X2)), add = TRUE)"},{"path":"lab.html","id":"quadratic-discriminant-analysis-qda-1","chapter":"Lab","heading":"Quadratic Discriminant Analysis (QDA)","text":"Training assessing QDA model R similar syntax training assessing LDA model. difference function name qda()7. Fit QDA model Purchased function Age EstimatedSalary. Name model classifier.qda.8. Make predictions test_set using QDA model classifier.qda. Show confusion matrix compare results predictions obtained using LDA model classifier.lda.9. Plot decision boundary obtained QDA. Color points real labels.","code":"\n# qda() is a function of library(MASS)\nclassifier.qda <- qda(Purchased~., data = training_set)"},{"path":"lab.html","id":"comparison","chapter":"Lab","heading":"Comparison","text":"10. order compare methods used, plot Figure ROC curve classifier fitted compare correspondant AUC. best model dataset? Can justify ?Remark:\nuse ROCR package:Logistic regression, use predicted probabilities prediction() (round values “0” “1”).LDA QDA, put pred.lda$posterior[,2] prediction() function (posterior probabilities observations belong class “1”).","code":""},{"path":"lab.html","id":"lda-from-scratch","chapter":"Lab","heading":"LDA from scratch","text":"11. Now let us build LDA model data set without using lda() function. free creating function without creating one. Go back question 6 see obtain using lda(). computes prior probability group membership estimated group means two groups. Additional information provided, may important, single covariance matrix used various groupings.LDA, compute every observation \\(x\\) discriminant score \\(\\delta_k(x)\\). attribute \\(x\\) class highest \\(\\delta\\). Recall \\[\\delta_k(x) = x^T \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\Sigma^{-1}  \\mu_k + \\log \\pi_k\\]compute \\(\\delta_k(x)\\) need estimate \\(\\pi_k\\), \\(\\mu_k\\) \\(\\Sigma\\).Note \\[x=\\begin{pmatrix}\n            X_1 \\\\\n            X_2\n            \\end{pmatrix}\\] \\(X_1\\)=Age \\(X_2\\)=EstimatedSalary.let us step step, first estimates:11.1 Subset training set two sets: class0 Purchased = 0 class1 Purchased = 1).11.2 Compute \\(\\pi_0\\) \\(\\pi_1\\).\\[\\pi_i = N_i / N, \\,\\, \\text{} \\,\\, N_i \\,\\, \\text{number data points group } \\]11.3 Compute \\(\\mu_0\\) \\(\\mu_1\\).\n\\[\\mu_0 = \\begin{pmatrix}\n   \\mu_0(X_1) \\\\\n   \\mu_0(X_2)\n   \\end{pmatrix} \\,\\, \\text{} \\,\\, \\mu_1 = \\begin{pmatrix}\n   \\mu_1(X_1) \\\\\n   \\mu_1(X_2)\n   \\end{pmatrix}\\], example, \\(\\mu_0(X_1)\\) mean variable \\(X_1\\) group \\(0\\) (subset class0).11.4 Compute \\(\\Sigma\\). case two classes like , computed calculating following:\\[\\Sigma = \\frac{(N_0-1)\\Sigma_0 + (N_1-1)\\Sigma_1}{N_0+N_1-2}\\]\\(\\Sigma_i\\) estimated covariance matrix specific group \\(\\).Remark: Recall LDA use \\(\\Sigma\\). QDA .11.5. Now computed needed estimates, can calculate \\(\\delta_0(x)\\) \\(\\delta_1(x)\\) observation \\(x\\). attribute \\(x\\) class highest \\(\\delta\\). First, try \\(x\\) \\(x^T=(1,1.5)\\), class prediction spesific \\(x\\)?11.6. Compute discriminant scores \\(\\delta\\) test set (matrix \\(100\\times 2\\)), predict classes compare results results obtained lda() function.\n◼\n","code":""},{"path":"dimensionality-reduction.html","id":"dimensionality-reduction","chapter":"2 Dimensionality Reduction","heading":"2 Dimensionality Reduction","text":"Dimensionality reduction, dimension reduction, transformation data high-dimensional space low-dimensional space low-dimensional representation retains meaningful properties original data, ideally close intrinsic dimension.Slides chapter: \n◼\n","code":""},{"path":"lab-1.html","id":"lab-1","chapter":"Lab","heading":"Lab","text":"free apply lab  Python.Python, use sklearn.decomposition.PCA PCA sklearn.manifold.TSNE t-SNE., free use princomp(), prcomp() factominer::PCA().","code":""},{"path":"lab-1.html","id":"the-dataset","chapter":"Lab","heading":"The dataset","text":"Employement European countries late 70sThe purpose case study reveal structure job market economy different developed countries. final aim meaningful rigorous plot able show important features countries concise form.dataset eurojob  contains data employed case study. contains percentage workforce employed 1979 9 industries 26 European countries. industries measured :Agriculture (Agr)Mining (Min)Manufacturing (Man)Power supply industries (Pow)Construction (Con)Service industries (Ser)Finance (Fin)Social personal services (Soc)Transport communications (Tra)","code":""},{"path":"lab-1.html","id":"pca","chapter":"Lab","heading":"PCA","text":"1. Import eurojob  dataset.dataset imported correctly, look like :\nTable 2.1: eurojob dataset.\n2. Describe dataset make hypotheses. can example:Calculate measurements variableCalculate visualize correlation matrixShow scatterplot matrixetc..3. Apply PCA dataset. Show variation explained principal components cumulative variation. Comment.Don’t forget standardize dataset, use eigendecomposition correlation matrix instead variance-covariance matrix (need standardize case).4. following plot, see scatterplot matrix principal components. green lines correspond ? notice?PCs uncorrelated, independent (uncorrelated imply independent).5. Plot following:scree plot.graph individuals.graph variables.biplot graph.contributions variables first 2 principal components.Interpret results (least 3 interpretations).","code":""},{"path":"lab-1.html","id":"pca-from-scratch","chapter":"Lab","heading":"PCA from scratch","text":"6. Implement PCA eurojob dataset:Standardize data.Obtain Eigenvectors Eigenvalues covariance matrix correlation matrix.Extra: Verify variance-covariance matrix standardized data equal correlation matrix unstandardized data, yield igenvectors eigenvalue pairsSort eigenvalues descending order choose \\(k\\) eigenvectors correspond \\(k\\) largest eigenvalues, \\(k\\) number dimensions new feature subspace (\\(k \\le p\\)).Construct projection matrix \\(\\mathbf{}\\) selected \\(k\\) eigenvectors.Transform original dataset \\(X\\) via \\(\\mathbf{}\\) obtain \\(k\\)-dimensional feature subspace \\(\\mathbf{Y}\\).Visualize graph individuals. Compare graph obtained question 5.Eigendecomposition - Computing Eigenvectors EigenvaluesThe eigenvectors eigenvalues covariance (correlation) matrix represent “core” PCA: eigenvectors (principal components) determine directions new feature space, eigenvalues determine magnitude. words, eigenvalues explain variance data along new feature axes.","code":""},{"path":"lab-1.html","id":"t-sne","chapter":"Lab","heading":"t-SNE","text":"part, going use sample digits dataset. can download sample MNIST dataset contains tens thousands handwritten digits ranging zero nine. image size 28×28 pixels.following image displays couple handwritten digits dataset:required flatten images 28×28 1×784 (already done given csv).Load dataset describe .Show numbers like image .Apply PCA t-SNE dataset visualize 2D plot observations. Label points coloring showing corresponding letter. Compare results.effect perplexity parameter using t-SNE?use R, use Rtsne package.\n◼\n","code":""}]
