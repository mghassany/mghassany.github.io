[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Welcome! In this course you will learn about the state of the art of Machine Learning and also gain practice implementing and deploying machine learning algorithms.\nThis course is destined for students of Data Science Filière in EFREI Paris engineering school. In Data Science Filière there is the following master programs:\n\nData & Artificial Intelligence\nData Engineering\nBusiness intelligence and Analytics\nBioInformatics\n\nThe aim of Machine Learning is to build computer systems that can adapt to their environments and learn form experience. Learning techniques and methods from this field are successfully applied to a variety of learning tasks in a broad range of areas, including, for example, spam recognition, text classification, gene discovery, financial forecasting. The course will give an overview of many concepts, techniques, and algorithms in machine learning, beginning with topics such as linear regression and classification and ending up with topics such as kmeans and Expectation Maximization. The course will give you the basic ideas and intuition behind these methods, as well as a more formal statistical and computational understanding. You will have an opportunity to experiment with machine learning techniques in R and apply them to a selected problem.\n\n\n\n\n\nSession\nTopic\nSlides\nLab\n\n\n\n\n1\nIntroduction to ML  Regression\n📖\n💻\n\n\n2\nClassification: Logistic Regression & Regularization\n📖\n💻\n\n\n3\nDecision Trees & Random Forests\n📖\n💻\n\n\n4\nIntroduction to Neural Networks & Deep Learning\n📖\n💻\n\n\n5\nProject\n\n💻"
  },
  {
    "objectID": "TD1.html#python-environment",
    "href": "TD1.html#python-environment",
    "title": "Lab1: Linear Regression",
    "section": "Python environment",
    "text": "Python environment\n\n\n\n\n\n\nAnaconda\n\n\n\nDuring this course we are going to use Python as programming language. Anaconda is an open-source distribution for Python. It is used for data science, machine learning, deep learning, etc. It comes with more than 300 libraries for data science. Anaconda helps in simplified package management and deployment.\nTo install it, go to Anaconda website.\nRemark: if you have a Mac with M1 ship, you must install the 2022.05 release of Anaconda: (Anaconda Distribution Now Supporting M1).\n\n\n\n\n\n\n\n\nJupyter\n\n\n\nDuring the labs, you must use Jupyter notebooks. The Jupyter Notebook is the original web application for creating and sharing computational documents. It offers a simple, streamlined, document-centric experience. Jupyter is installed by default when you install Anaconda. You can create notebooks using JupyterLab via your browser or using a text editor like VScode."
  },
  {
    "objectID": "TD1.html#predicting-house-value-boston-dataset",
    "href": "TD1.html#predicting-house-value-boston-dataset",
    "title": "Lab1: Linear Regression",
    "section": "Predicting House Value: Boston dataset",
    "text": "Predicting House Value: Boston dataset\nIn this lab we are going to use a dataset called Boston. It records the median value of houses for 506 neighborhoods around Boston. Our task is to predict the median house value.\nLoading Data\n\n\n\n\n\n\nBoston dataset\n\n\n\nThe dataset is available in scikit-learn or also here 🔗. Notice that the format/approach is not the same. You are free to use any of them, it is up to you to adapt your codes correctly.\nThere is mainly two approaches you need to know for instance:\n\nThe features and the target variable are in the same dataframe. In this case you can use the argument formula = target ~ features in certain fitting functions (like in ols(), imitating R’s programming language functions).\nThe features and the target variable are separated in X and y.\n\n\n\n\n1. Load these necessary libraries for this lab (install them if needed).\n\nimport numpy as np\nimport matplotlib.pyplot as plt \n\nimport pandas as pd  \nimport seaborn as sns \n\n# Run the following line to obtain the matplotlib figures in the notebook\n%matplotlib inline\n\n# We will also use sklearn but we will load the necessary modules when needed\n\n\n\n\n2. Load the Boston dataset.\n\nfrom sklearn.datasets import load_boston\nboston_dataset = load_boston()\n\n3. Print the value of the boston_dataset to understand what it contains.\n\nprint(boston_dataset.keys())\n\ndict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename', 'data_module'])\n\n\n\ndata: contains the information for various houses\ntarget: prices of the house\nfeature_names: names of the features\nDESCR: describes the dataset\n\nTo know more about the features run boston_dataset.DESCR.\nThe prices of the house indicated by the variable MEDV is our target variable and the remaining are the feature variables based on which we will predict the median value of houses in a district.\n3. Load the data into a pandas dataframe using pd.DataFrame. Then print the first 5 rows of the data using head().\n\nboston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\nboston.head()\n\n      CRIM    ZN  INDUS  CHAS    NOX  ...  RAD    TAX  PTRATIO       B  LSTAT\n0  0.00632  18.0   2.31   0.0  0.538  ...  1.0  296.0     15.3  396.90   4.98\n1  0.02731   0.0   7.07   0.0  0.469  ...  2.0  242.0     17.8  396.90   9.14\n2  0.02729   0.0   7.07   0.0  0.469  ...  2.0  242.0     17.8  392.83   4.03\n3  0.03237   0.0   2.18   0.0  0.458  ...  3.0  222.0     18.7  394.63   2.94\n4  0.06905   0.0   2.18   0.0  0.458  ...  3.0  222.0     18.7  396.90   5.33\n\n[5 rows x 13 columns]\n\n\nWe can see that the target value MEDV is missing from the data. We create a new column of target values and add it to the dataframe.\n\nboston['MEDV'] = boston_dataset.target\n\nRemark: the previous steps we evitable if we loaded the data from csv given above using pd.read_csv()."
  },
  {
    "objectID": "TD1.html#data-preprocessing",
    "href": "TD1.html#data-preprocessing",
    "title": "Lab1: Linear Regression",
    "section": "Data preprocessing",
    "text": "Data preprocessing\n4. Check if there are any missing values in the data.\n\n\n\n\n\n\nSolution (Click to expand)\n\n\n\n\n\n\nboston.isnull().sum()\n\nCRIM       0\nZN         0\nINDUS      0\nCHAS       0\nNOX        0\nRM         0\nAGE        0\nDIS        0\nRAD        0\nTAX        0\nPTRATIO    0\nB          0\nLSTAT      0\nMEDV       0\ndtype: int64\n\n\n\n\n\nExploratory Data Analysis\nExploratory Data Analysis is a very important step before training the model. In this section, we will use some visualizations to understand the relationship of the target variable with other features.\n5. Plot the distribution of the target variable MEDV. You can use the distplot() function from the seaborn library.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsns.distplot(boston['MEDV'], bins=30)\nplt.title('Distribution of the target value')\nplt.show()\n\n\n\n\n\n\n\n6. Calculate the correlation matrix and visualize it (you may use heatmap() from seaborn library). Name the features that are highly correlated with the target variable.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nboston.corr()\n\n             CRIM        ZN     INDUS  ...         B     LSTAT      MEDV\nCRIM     1.000000 -0.200469  0.406583  ... -0.385064  0.455621 -0.388305\nZN      -0.200469  1.000000 -0.533828  ...  0.175520 -0.412995  0.360445\nINDUS    0.406583 -0.533828  1.000000  ... -0.356977  0.603800 -0.483725\nCHAS    -0.055892 -0.042697  0.062938  ...  0.048788 -0.053929  0.175260\nNOX      0.420972 -0.516604  0.763651  ... -0.380051  0.590879 -0.427321\nRM      -0.219247  0.311991 -0.391676  ...  0.128069 -0.613808  0.695360\nAGE      0.352734 -0.569537  0.644779  ... -0.273534  0.602339 -0.376955\nDIS     -0.379670  0.664408 -0.708027  ...  0.291512 -0.496996  0.249929\nRAD      0.625505 -0.311948  0.595129  ... -0.444413  0.488676 -0.381626\nTAX      0.582764 -0.314563  0.720760  ... -0.441808  0.543993 -0.468536\nPTRATIO  0.289946 -0.391679  0.383248  ... -0.177383  0.374044 -0.507787\nB       -0.385064  0.175520 -0.356977  ...  1.000000 -0.366087  0.333461\nLSTAT    0.455621 -0.412995  0.603800  ... -0.366087  1.000000 -0.737663\nMEDV    -0.388305  0.360445 -0.483725  ...  0.333461 -0.737663  1.000000\n\n[14 rows x 14 columns]\n\n\n\ncorrelation_matrix = boston.corr().round(2)\n# annot = True to print the values inside the square\nsns.heatmap(data=correlation_matrix, annot=True)\n\n\n\n\n\ncorrelation_matrix['MEDV'].sort_values(ascending=False)\n\nMEDV       1.00\nRM         0.70\nZN         0.36\nB          0.33\nDIS        0.25\nCHAS       0.18\nAGE       -0.38\nRAD       -0.38\nCRIM      -0.39\nNOX       -0.43\nTAX       -0.47\nINDUS     -0.48\nPTRATIO   -0.51\nLSTAT     -0.74\nName: MEDV, dtype: float64\n\n\nThe features that are highly correlated with the target variable are RM and LSTAT where\n\n\nRM has a strong positive correlation with MEDV (0.7) and \n\n\nLSTAT has a high negative correlation with MEDV(-0.74).\n\n\n\n\n\n\n\n\n\n\nCorrelation\n\n\n\nThe correlation coefficient ranges from -1 to 1. If the value is close to 1, it means that there is a strong positive correlation (linear tendancy) between the two variables. When it is close to -1, the variables have a strong negative correlation.\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor a linear regression model, we select the features which have a high correlation with the target variable. Anyway there is some feature selection techniques you may use, one of them is Backward selection:\nBackward selection:\n\nStart with all variables in the model.\nRemove the variable with the largest p-value — that is, the variable that is the least statistically significant.\nThe new \\((p − 1)\\)-variable model is fit, and the variable with the largest p-value is removed.\nContinue until a stopping rule is reached. For instance, we may stop when all remaining variables have a significant p-value defined by some significance threshold.\n\n\n\n7. Check for multi-colinearity between the features. More specifically RAD and TAX.\n\n\n\n\n\n\nTip\n\n\n\nWe should not select colinear features together for training the model. Check this link for one explanation.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIt is not great to have colinearities between your features as it will make harder the interpretation of which feature is important and which is not. Although I don’t believe it will harm your final estimate that much. In a extreme case it can make the matrix \\(X^T\\cdot X\\) hard / impossible to invert. It will also lower the p-value of the two colinear column which may lead to not selecting them.\n“Multicollinearity does not reduce the predictive power or reliability of the model as a whole, at least within the sample data set; it only affects calculations regarding individual predictors”\n\nplt.scatter(boston['RAD'], boston['TAX'])\nplt.title(f\"Correlation RAD - TAX : {correlation_matrix['RAD']['TAX']:.2f}\" )\nplt.show()\n\n\n\n\n\nncorr = correlation_matrix.drop('MEDV', axis=0).drop('MEDV', axis=1)\nnp.fill_diagonal(ncorr.values, 0)\nncorr[(ncorr < 0.7) & (ncorr > -0.7)] = None\n\nplt.figure(figsize=(10,5))\nsns.heatmap(ncorr, annot=True)\nplt.title('High correlations in features')\nplt.show()\n\n\n\n\nAs we can see we have other quite high colinearity between features but RAD-TAX is the highest.\n\n\n\nSplitting the data into training and testing sets\nTrain test split is a model validation procedure that allows you to simulate how a model would perform on new/unseen data. Here is how the procedure works:\n\n8. Split the data into training and testing sets. We are going to train the model with 80% of the samples and test with the remaining 20%. Use train_test_split() function provided by scikit-learn library\n\nfrom sklearn.model_selection import train_test_split\n\n# complete the code\nX = ...\nY = ...\n\nX_train, X_test, Y_train, Y_test = ...(, , test_size = ..., random_state=5)\n\n# print the shapes to verify if the splitting has occured properly\nprint(X_train.shape)\nprint(X_test.shape)\nprint(Y_train.shape)\nprint(Y_test.shape)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n\n# complete the code\nX = boston.drop('MEDV', axis=1)\nY = boston[['MEDV']]\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2,random_state=5)\n\n# print the shapes to verify if the splitting has occured properly\nprint(X_train.shape)\n\n(404, 13)\n\nprint(X_test.shape)\n\n(102, 13)\n\nprint(Y_train.shape)\n\n(404, 1)\n\nprint(Y_test.shape)\n\n(102, 1)"
  },
  {
    "objectID": "TD1.html#simple-linear-regression-model",
    "href": "TD1.html#simple-linear-regression-model",
    "title": "Lab1: Linear Regression",
    "section": "\nSimple Linear Regression model",
    "text": "Simple Linear Regression model\nIn this part, we are going to build a simple linear regression model. We will choose LSTAT as a feature.\n9. Plot MEDV in function of LSTAT.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nplt.scatter(X_train['LSTAT'], Y_train['MEDV'])\nplt.show()\n\n\n\n\n\n\n\n10. Fit a simple regression model using LinearRegression() from sklearn.linear_model.\n\nfrom sklearn.linear_model import LinearRegression\n\nslm = LinearRegression()\nslm.fit(..., ...)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nX_train_s = X_train[['LSTAT']] # Select LSTAT only\nX_test_s = X_test[['LSTAT']] # Select LSTAT only\n\nfrom sklearn.linear_model import LinearRegression\n\nslm = LinearRegression(fit_intercept=True)\nslm.fit(X_train_s, Y_train)\n\nLinearRegression()\n\n\n\n\n\n11. The LinearRegression() module from scikit-learn does not provide a statistical summary of the regression model. To obtain this summary, re-fit a model using ols() from statsmodels. Analyse the p-value from the summary and interpret.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nimport statsmodels.api as sm\n\nX_train_d = sm.add_constant(X_train_s)\nmodel = sm.OLS(Y_train,X_train_d)\nresults = model.fit()\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   MEDV   R-squared:                       0.552\nModel:                            OLS   Adj. R-squared:                  0.551\nMethod:                 Least Squares   F-statistic:                     495.9\nDate:                Thu, 08 Sep 2022   Prob (F-statistic):           3.76e-72\nTime:                        22:18:56   Log-Likelihood:                -1310.5\nNo. Observations:                 404   AIC:                             2625.\nDf Residuals:                     402   BIC:                             2633.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         34.8729      0.630     55.341      0.000      33.634      36.112\nLSTAT         -0.9798      0.044    -22.269      0.000      -1.066      -0.893\n==============================================================================\nOmnibus:                      111.582   Durbin-Watson:                   1.948\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              237.785\nSkew:                           1.447   Prob(JB):                     2.32e-52\nKurtosis:                       5.399   Cond. No.                         29.3\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nMost of the features seem relevant except for INDUS and AGE.\n\n\n\n12. Plot the regression model.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nplt.plot(X_train_s['LSTAT'], slm.predict(X_train_s), c='red')\nplt.scatter(X_train_s['LSTAT'], Y_train['MEDV'])\nplt.xlabel('LSTAT')\nplt.ylabel('Predicted Values')\nplt.show()\n\n\n\n\n\n\n\nModel evaluation\n13. Evaluate the model using MSE (Mean Squarred Error) and R2-score.\n\nfrom sklearn.metrics import mean_squared_error\n\n# train error (MSE)\ny_train_predict = slm.predict(...)\nmse_train = ...(..., ...)\n\nprint(\"The model performance for training set\")\n\nprint('MSE is {}'.format(mse_train))\n\n\n# test error\ny_test_predict = slm.predict(...)\nmse_test = mean_squared_error(..., ...)\nr2 = r2_score(..., ...)\n\nprint(\"The model performance for testing set\")\n\nprint('MSE is {}'.format(mse_test))\nprint('R2 score is {}'.format(r2))\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# train error (MSE)\ny_train_predict =  slm.predict(X_train_s)\nmse_train = mean_squared_error(Y_train, y_train_predict)\n\nprint(\"The model performance for training set\")\n\nThe model performance for training set\n\nprint('MSE is {}'.format(mse_train))\n\n\n# test error\n\nMSE is 38.45801898706332\n\ny_test_predict = slm.predict(X_test_s)\nmse_test = mean_squared_error(Y_test, y_test_predict)\nr2 = r2_score(Y_test, y_test_predict)\n\nprint(\"The model performance for testing set\")\n\nThe model performance for testing set\n\nprint('MSE is {}'.format(mse_test))\n\nMSE is 38.821829014286585\n\nprint('R2 score is {}'.format(r2))\n\nR2 score is 0.5041523728903132\n\n\n\n\n\n14. According to the plot in 9, the relationship between LSTAT and MEDV is not linear. Let’s try a transformation of our explanatory variable LSTAT. Re-do the steps from 9 to 13 but using the log of LSTAT. Do you obtain a better model?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nX_train_s = np.log(X_train[['LSTAT']]) # Select LSTAT only\nX_test_s = np.log(X_test[['LSTAT']]) # Select LSTAT only\n\nslm = LinearRegression(fit_intercept=True)\nslm.fit(X_train_s, Y_train)\n\n# train error (MSE)\n\nLinearRegression()\n\ny_train_predict =  slm.predict(X_train_s)\nmse_train = mean_squared_error(Y_train, y_train_predict)\n\nprint(\"The model performance for training set\")\n\nThe model performance for training set\n\nprint('MSE is {}'.format(mse_train))\n\n\n# test error\n\nMSE is 28.39419482862284\n\ny_test_predict = slm.predict(X_test_s)\nmse_test = mean_squared_error(Y_test, y_test_predict)\nr2 = r2_score(Y_test, y_test_predict)\n\nprint(\"The model performance for testing set\")\n\nThe model performance for testing set\n\nprint('MSE is {}'.format(mse_test))\n\nMSE is 27.899544077825233\n\nprint('R2 score is {}'.format(r2))\n\nR2 score is 0.6436560801053237\n\n\n\nplt.scatter(X_train['LSTAT'], Y_train['MEDV'])\nplt.scatter(X_train['LSTAT'], slm.predict(X_train_s), c='red')\nplt.xlabel('LSTAT')\nplt.ylabel('Predicted Values')\nplt.show()\n\n\n\n\nWe do obtain a better model, interestingly we can visualise that the model is representing a non linear relationship between the variable and the output thanks to this trick."
  },
  {
    "objectID": "TD1.html#multiple-linear-regression-model",
    "href": "TD1.html#multiple-linear-regression-model",
    "title": "Lab1: Linear Regression",
    "section": "Multiple Linear Regression model",
    "text": "Multiple Linear Regression model\n15. Train a new model using all the variables of the dataset. Evalute the performance of the model.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nX_train_d = sm.add_constant(X_train)\nmodel = sm.OLS(Y_train,X_train_d)\nresults = model.fit()\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   MEDV   R-squared:                       0.738\nModel:                            OLS   Adj. R-squared:                  0.730\nMethod:                 Least Squares   F-statistic:                     84.65\nDate:                Thu, 08 Sep 2022   Prob (F-statistic):          8.21e-105\nTime:                        22:18:59   Log-Likelihood:                -1202.0\nNo. Observations:                 404   AIC:                             2432.\nDf Residuals:                     390   BIC:                             2488.\nDf Model:                          13                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         37.9125      5.775      6.565      0.000      26.559      49.266\nCRIM          -0.1308      0.036     -3.603      0.000      -0.202      -0.059\nZN             0.0494      0.016      3.131      0.002       0.018       0.080\nINDUS          0.0011      0.072      0.015      0.988      -0.141       0.143\nCHAS           2.7054      0.989      2.737      0.006       0.762       4.649\nNOX          -15.9571      4.517     -3.532      0.000     -24.838      -7.076\nRM             3.4140      0.470      7.266      0.000       2.490       4.338\nAGE            0.0011      0.015      0.077      0.939      -0.027       0.030\nDIS           -1.4931      0.233     -6.404      0.000      -1.951      -1.035\nRAD            0.3644      0.079      4.628      0.000       0.210       0.519\nTAX           -0.0132      0.004     -2.950      0.003      -0.022      -0.004\nPTRATIO       -0.9524      0.149     -6.411      0.000      -1.244      -0.660\nB              0.0117      0.003      3.795      0.000       0.006       0.018\nLSTAT         -0.5941      0.058    -10.271      0.000      -0.708      -0.480\n==============================================================================\nOmnibus:                      134.272   Durbin-Watson:                   2.057\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              542.194\nSkew:                           1.423   Prob(JB):                    1.84e-118\nKurtosis:                       7.911   Cond. No.                     1.51e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.51e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\n\n\n16. Which features are significant for the model?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe ones with p-values is less than the error we fix (let’s say 5%). Age for example.\n\n\n\n17. Apply backward selection to fit a model with the best subset of features.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ncolumns_to_drop = ['AGE']\nX_train_bck = X_train.drop(columns_to_drop, axis=1)\nX_test_bck = X_test.drop(columns_to_drop, axis=1)\n\n\nmodel = sm.OLS(Y_train,sm.add_constant(X_train_bck))\nresults = model.fit()\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   MEDV   R-squared:                       0.738\nModel:                            OLS   Adj. R-squared:                  0.730\nMethod:                 Least Squares   F-statistic:                     91.94\nDate:                Thu, 08 Sep 2022   Prob (F-statistic):          8.39e-106\nTime:                        22:18:59   Log-Likelihood:                -1202.0\nNo. Observations:                 404   AIC:                             2430.\nDf Residuals:                     391   BIC:                             2482.\nDf Model:                          12                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         37.8841      5.756      6.582      0.000      26.568      49.200\nCRIM          -0.1309      0.036     -3.610      0.000      -0.202      -0.060\nZN             0.0492      0.016      3.151      0.002       0.019       0.080\nINDUS          0.0009      0.072      0.013      0.990      -0.141       0.143\nCHAS           2.7098      0.986      2.749      0.006       0.772       4.648\nNOX          -15.8774      4.392     -3.615      0.000     -24.512      -7.243\nRM             3.4208      0.461      7.423      0.000       2.515       4.327\nDIS           -1.4985      0.222     -6.754      0.000      -1.935      -1.062\nRAD            0.3640      0.078      4.640      0.000       0.210       0.518\nTAX           -0.0132      0.004     -2.953      0.003      -0.022      -0.004\nPTRATIO       -0.9516      0.148     -6.430      0.000      -1.243      -0.661\nB              0.0118      0.003      3.818      0.000       0.006       0.018\nLSTAT         -0.5925      0.054    -10.938      0.000      -0.699      -0.486\n==============================================================================\nOmnibus:                      134.673   Durbin-Watson:                   2.057\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              545.931\nSkew:                           1.426   Prob(JB):                    2.84e-119\nKurtosis:                       7.929   Cond. No.                     1.49e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.49e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\ncolumns_to_drop = ['INDUS']\nX_train_bck = X_train.drop(columns_to_drop, axis=1)\nX_test_bck = X_test.drop(columns_to_drop, axis=1)\n\nmodel = sm.OLS(Y_train,sm.add_constant(X_train_bck))\nresults = model.fit()\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   MEDV   R-squared:                       0.738\nModel:                            OLS   Adj. R-squared:                  0.730\nMethod:                 Least Squares   F-statistic:                     91.94\nDate:                Thu, 08 Sep 2022   Prob (F-statistic):          8.37e-106\nTime:                        22:18:59   Log-Likelihood:                -1202.0\nNo. Observations:                 404   AIC:                             2430.\nDf Residuals:                     391   BIC:                             2482.\nDf Model:                          12                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         37.9074      5.758      6.584      0.000      26.588      49.227\nCRIM          -0.1308      0.036     -3.610      0.000      -0.202      -0.060\nZN             0.0494      0.016      3.154      0.002       0.019       0.080\nCHAS           2.7073      0.979      2.765      0.006       0.783       4.632\nNOX          -15.9390      4.351     -3.663      0.000     -24.494      -7.384\nRM             3.4133      0.467      7.304      0.000       2.495       4.332\nAGE            0.0011      0.014      0.077      0.939      -0.027       0.030\nDIS           -1.4938      0.227     -6.570      0.000      -1.941      -1.047\nRAD            0.3641      0.076      4.799      0.000       0.215       0.513\nTAX           -0.0131      0.004     -3.269      0.001      -0.021      -0.005\nPTRATIO       -0.9521      0.147     -6.479      0.000      -1.241      -0.663\nB              0.0117      0.003      3.802      0.000       0.006       0.018\nLSTAT         -0.5940      0.058    -10.308      0.000      -0.707      -0.481\n==============================================================================\nOmnibus:                      134.288   Durbin-Watson:                   2.057\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              542.346\nSkew:                           1.423   Prob(JB):                    1.70e-118\nKurtosis:                       7.911   Cond. No.                     1.49e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.49e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\ny_test_predict = results.predict(sm.add_constant(X_test_bck))\nmse_test = mean_squared_error(Y_test, y_test_predict)\nr2 = r2_score(Y_test, y_test_predict)\n\nprint(\"The model performance for testing set\")\n\nThe model performance for testing set\n\nprint('MSE is {}'.format(mse_test))\n\nMSE is 20.872226101228375\n\nprint('R2 score is {}'.format(r2))\n\nR2 score is 0.7334117416007803\n\n\n\n\n\n18. Is the new model better than the last one with all the features?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ny_test_predict = results.predict(sm.add_constant(X_test_bck))\nmse_test = mean_squared_error(Y_test, y_test_predict)\nr2 = r2_score(Y_test, y_test_predict)\n\nprint(\"The model performance for testing set\")\n\nThe model performance for testing set\n\nprint('MSE is {}'.format(mse_test))\n\nMSE is 20.872226101228375\n\nprint('R2 score is {}'.format(r2))\n\nR2 score is 0.7334117416007803\n\n\n\n\n\n19. In the last model we didn’t transform LSTAT. Re train the model using log(LSTAT) instead of LSTAT. Does this new model performs better?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nX_train_lg = X_train.drop('LSTAT',axis=1)\nX_train_lg['log(LSTAT)'] = np.log(X_train['LSTAT'])\n\nX_test_lg = X_test.drop('LSTAT',axis=1)\nX_test_lg['log(LSTAT)'] = np.log(X_test['LSTAT'])\n\n\nmodel = sm.OLS(Y_train,sm.add_constant(X_train_lg))\nresults = model.fit()\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   MEDV   R-squared:                       0.787\nModel:                            OLS   Adj. R-squared:                  0.780\nMethod:                 Least Squares   F-statistic:                     111.1\nDate:                Thu, 08 Sep 2022   Prob (F-statistic):          3.07e-122\nTime:                        22:19:00   Log-Likelihood:                -1160.0\nNo. Observations:                 404   AIC:                             2348.\nDf Residuals:                     390   BIC:                             2404.\nDf Model:                          13                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         56.7251      5.535     10.249      0.000      45.843      67.607\nCRIM          -0.1438      0.032     -4.444      0.000      -0.207      -0.080\nZN             0.0286      0.014      2.009      0.045       0.001       0.057\nINDUS         -0.0018      0.065     -0.027      0.978      -0.130       0.126\nCHAS           2.0388      0.894      2.281      0.023       0.281       3.796\nNOX          -15.0430      4.065     -3.701      0.000     -23.034      -7.052\nRM             2.1356      0.443      4.820      0.000       1.265       3.007\nAGE            0.0252      0.013      1.892      0.059      -0.001       0.051\nDIS           -1.2703      0.211     -6.010      0.000      -1.686      -0.855\nRAD            0.3307      0.071      4.663      0.000       0.191       0.470\nTAX           -0.0116      0.004     -2.874      0.004      -0.019      -0.004\nPTRATIO       -0.8343      0.134     -6.212      0.000      -1.098      -0.570\nB              0.0094      0.003      3.348      0.001       0.004       0.015\nlog(LSTAT)    -9.5323      0.643    -14.825      0.000     -10.796      -8.268\n==============================================================================\nOmnibus:                      120.438   Durbin-Watson:                   2.046\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              557.257\nSkew:                           1.212   Prob(JB):                    9.84e-122\nKurtosis:                       8.218   Cond. No.                     1.56e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.56e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\ny_test_predict = results.predict(sm.add_constant(X_test_lg))\nmse_test = mean_squared_error(Y_test, y_test_predict)\nr2 = r2_score(Y_test, y_test_predict)\n\nprint(\"The model performance for testing set\")\n\nThe model performance for testing set\n\nprint('MSE is {}'.format(mse_test))\n\nMSE is 15.039960438345249\n\nprint('R2 score is {}'.format(r2))\n\nR2 score is 0.8079037262146341\n\n\nThe model is better (lower test MSE)."
  },
  {
    "objectID": "TD1.html#anova-analysis-of-variances",
    "href": "TD1.html#anova-analysis-of-variances",
    "title": "Lab1: Linear Regression",
    "section": "ANOVA (ANalysis Of VAriances)",
    "text": "ANOVA (ANalysis Of VAriances)\nIn this last part we will apply an analysis of variances (ANOVA) in order to test if there is a significant difference of means between two groups \\(i\\) and \\(j\\) (Consider group \\(i\\) is the suburbs bounding the river and \\(j\\) the suburbs which not). The hypotheses are\n\\[ H_0 : \\mu_i = \\mu_j \\]\n\\[ H_1 : \\mu_i \\neq \\mu_j \\]\nWhere \\(\\mu_i\\) is the mean of MEDV in group \\(i\\).\n\n\n\n\n\n\nAnova\n\n\n\nThis analysis can be conducted during the exploratory data analysis part especially when the target is continuous and a feature is discrete.\n\n\n20. In the Boston data set there is a categorical variable CHAS which corresponds to Charles River (= 1 if a suburb bounds the river; 0 otherwise). How many of the suburbs in this data set bound the Charles river?\n21. Create Boxplots of the median value of houses with respect to the variable CHAS. Do we observe some difference between the median value of houses with respect to the neighborhood to Charles River?\n\n\n\n22. Calculate \\(\\mu_i\\) and \\(\\mu_j\\).\n\n\n\n23. Apply an ANOVA test of MEDV with respect to CHAS. What do you conclude ?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nboston['CHAS'].value_counts()\n\n0.0    471\n1.0     35\nName: CHAS, dtype: int64\n\n\n\nsns.boxplot(x='CHAS', y='MEDV', data=boston)\nplt.show()\n\n\n\n\n\nboston.groupby('CHAS')['MEDV'].mean()\n\nCHAS\n0.0    22.093843\n1.0    28.440000\nName: MEDV, dtype: float64\n\n\nYou can use Anova’s test from statsmodels or from scipy.\n\nfrom scipy.stats import f_oneway\n\nf_oneway(boston[boston['CHAS'] == 1.0]['MEDV'].to_numpy(), \n         boston[boston['CHAS'] == 0.0]['MEDV'].to_numpy())\n\nF_onewayResult(statistic=15.971512420371962, pvalue=7.390623170520815e-05)\n\n\nOr\n\n\nfrom statsmodels.stats.anova import anova_lm\nfrom statsmodels.formula.api import ols\n\nanova_boston_bound_river = anova_lm(\n    ols('MEDV~C(CHAS)', data=boston[['MEDV', 'CHAS']]).fit())\nprint(anova_boston_bound_river)\n\n             df        sum_sq      mean_sq          F    PR(>F)\nC(CHAS)     1.0   1312.079271  1312.079271  15.971512  0.000074\nResidual  504.0  41404.216144    82.151223        NaN       NaN\n\n\nBecause the p-value is low we can reject the hypothesis that the two groups have the same mean and thus conclude that they have different means. Note that we didn’t check which was the biggest here, just that they are not the same. Because we are checking only the difference between two distribution and not multiple ones."
  },
  {
    "objectID": "TD2.html",
    "href": "TD2.html",
    "title": "Lab2: Logistic Regression & Regularization",
    "section": "",
    "text": "In this lab, you will implement logistic regression and apply it to two different datasets.\nBefore we begin with the exercises, we need to import all libraries required for this programming exercise. Throughout the course, we will be using numpy for all arrays and matrix operations, and matplotlib for plotting. In this assignment, we will also use scipy, which contains scientific and numerical computation functions and tools.\n\n# Scientific and vector computation for python\nimport numpy as np\n\n# Plotting library\nimport matplotlib.pyplot as plt\n\n# Optimization module in scipy\nfrom scipy import optimize\n\n# do not forget to tell matplotlib to embed plots within the notebook\n%matplotlib inline"
  },
  {
    "objectID": "TD2.html#logistic-regression",
    "href": "TD2.html#logistic-regression",
    "title": "Lab2: Logistic Regression & Regularization",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nIn this part of the exercise, you will build a logistic regression model to predict whether a student gets admitted into a university. Suppose that you are the administrator of a university department and you want to determine each applicant’s chance of admission based on their results on two exams. You have historical data from previous applicants that you can use as a training set for logistic regression. For each training example, you have the applicant’s scores on two exams and the admissions decision.\nYour task is to build a classification model that estimates an applicant’s probability of admission based the scores from those two exams.\n1. Load the data tp2data1.txt from here  using the loadtxt() from numpy. The first two columns contains the exam scores and the third column contains the label. Then separate the features from label. Name the feature matrix X and the label y.\n\n\n\n\n\n\n2. Print the first 5 samples from X and y.\nVisualizing the data\n3. Display the data on a 2-dimensional plot where the axes are the two exam scores, and the positive and negative examples are shown with different colors (or markers).\nYou should produce something like this:\n\n\n\n\n\n\n\n\nImplementation\nSigmoid function\nRecall that the logistic regression hypothesis is defined as:\n\\[ h_\\omega(x) = g(\\omega^T x)\\]\nwhere function \\(g\\) is the sigmoid function. The sigmoid function is defined as:\n\\[g(z) = \\frac{1}{1+e^{-z}}\\]\n4. Implement the sigmoid function so it can be called by the rest of your program. When you are finished, try testing a few values by calling sigmoid(x) in a new cell. For large positive values of x, the sigmoid should be close to 1, while for large negative values, the sigmoid should be close to 0. Evaluating sigmoid(0) should give you exactly 0.5. Your code should also work with vectors and matrices. For a matrix, your function should perform the sigmoid function on every element.\n5. Plot the sigmoid function, like this:\n\n\n\n\n\n\n\n\n\n\n\nCost function and gradient\n6. Before proceeding, add the intercept term to X. (hint: you can use np.concatenate or np.hstack)\n\n\n\n7. Implement the cost function and its gradient for logistic regression.\nRecall that the cost function for logistic regression is\n\\[ J(\\omega) = \\frac{1}{m} \\sum_{i=1}^{m} \\left[ -y^{(i)} \\log\\left(h_\\omega\\left( x^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - h_\\omega\\left( x^{(i)} \\right) \\right) \\right]\\]\nRecall that the gradient of the cost is a vector of the same length as \\(\\omega\\) where the \\(j^{th}\\) element (for \\(j = 0, 1, \\cdots , n\\)) is defined as follows:\n\\[ \\frac{\\partial J(\\omega)}{\\partial \\omega_j} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\omega \\left( x^{(i)} \\right) - y^{(i)} \\right) x_j^{(i)} \\]\n\n\n\n8. Call your implemented function using two test cases for \\(\\omega\\). You should see that the cost is about 0.693 for \\(\\omega = (0,0,0)\\).\n\n\n\n\n\n\nLearning the parameters\nLearning parameters using your implemented Gradient Descent\n9. Implement the gradient descent algorithm for logistic regression: write a cost function and calculate its gradient, then take a gradient descent step accordingly in order to find the optimal parameters. Run it on the training set. Print the results (the parameters values and the cost function).\nLearning parameters using scipy.optimize\nIn this part you will use the scipy.optimize module. SciPy is a numerical computing library for python. It provides an optimization module for root finding and minimization. As of scipy 1.0, the function scipy.optimize.minimize is the method to use for optimization problems(both constrained and unconstrained).\nFor logistic regression, you want to optimize the cost function \\(J(\\omega)\\) with parameters \\(\\omega\\). Concretely, you are going to use optimize.minimize to find the best parameters \\(\\omega\\) for the logistic regression cost function, given a fixed dataset (of X and y values). You will pass to optimize.minimize the following inputs:\n\n\ncostFunction: A cost function that, when given the training set and a particular \\(\\omega\\), computes the logistic regression cost and gradient with respect to \\(\\omega\\) for the dataset (X, y). It is important to note that we only pass the name of the function without the parenthesis. This indicates that we are only providing a reference to this function, and not evaluating the result from this function.\n\ninitial_omega: The initial values of the parameters we are trying to optimize.\n\n(X, y): These are additional arguments to the cost function.\n\njac: Indication if the cost function returns the Jacobian (gradient) along with cost value. (True)\n\nmethod: Optimization method/algorithm to use\n\noptions: Additional options which might be specific to the specific optimization method. In the following, we only tell the algorithm the maximum number of iterations before it terminates.\n\nIf you have calculated the cost function correctly, optimize.minimize will converge on the right optimization parameters and return the final values of the cost and \\(\\omega\\) in a class object. Notice that by using optimize.minimize, you did not have to write any loops yourself, or set a learning rate like you did for gradient descent. This is all done by optimize.minimize: you only needed to provide a function calculating the cost and the gradient.\nIn the following, a code written to call optimize.minimize with the correct arguments.\n\n# set options for optimize.minimize\noptions= {'maxiter': 400}\n\n# see documention for scipy's optimize.minimize  for description about\n# the different parameters\n# The function returns an object `OptimizeResult`\n# We use truncated Newton algorithm for optimization which is \n# equivalent to MATLAB's fminunc\n# See https://stackoverflow.com/questions/18801002/fminunc-alternate-in-numpy\n\n# initial_omega = np.array([0, 0, 0])\n\nres = optimize.minimize(costFunction,\n                        initial_omega,\n                        (X_train, y),\n                        jac=True,\n                        method='TNC',\n                        options=options)\n\n# the fun property of `OptimizeResult` object returns\n# the value of costFunction at optimized omega\ncost = res.fun\n\n# the optimized omega is in the x property\nomega = res.x\n\n# Print omega to screen\nprint('Cost at omega found by optimize.minimize: {:.3f}'.format(cost))\nprint('Expected cost (approx): 0.203\\n');\n\nprint('omega:')\nprint('\\t[{:.3f}, {:.3f}, {:.3f}]'.format(*omega))\nprint('Expected omega (approx):\\n\\t[-25.161, 0.206, 0.201]')\n\n10. Run the code above on your cost function and verify that you obtain the correct (or almost) values.\n\n\n\nPlotting the decision boundary\nSince the decision boundary of logistic regression is a linear (you know that right?) and the dimension of the feature space here is 2, the decision boundary in this 2-dimensional space is a line that separates the predicted classes “0” and “1”.\nFor logistic regression, we predict \\(y=1\\) if \\(\\omega^T X \\geq 0\\) (right side of the line) and \\(y=0\\) if \\(\\omega^T X \\lt 0\\) (left side of the line). Where\n\\[\n\\omega = \\begin{pmatrix} \\omega_0 \\\\ \\omega_1 \\\\ \\omega2 \\end{pmatrix} \\,\\, \\text{and} \\,\\, X = \\begin{pmatrix}\n  1 \\\\\n  X_1 \\\\\n  X_2\n  \\end{pmatrix}\n\\]\nSo we predict \\(y=1\\) if \\(\\omega_0 + \\omega_1 X_1 + \\omega_2 X_2 \\geq 0\\) which means that the equation of the decision boundary (a line here) is \\(X_2 = - \\frac{\\omega_1}{\\omega_2}X_1 - \\frac{\\omega_0}{\\omega_2}\\)\n11. Plot the decision boundary obtained with logistic regression.\nEvaluating logistic regression\n12. After learning the parameters, you can use the model to predict whether a particular student will be admitted. For a student with an Exam 1 score of 45 and an Exam 2 score of 85, you should expect to see an admission probability of 0.776. Another way to evaluate the quality of the parameters we have found is to see how well the learned model predicts on our training set. Write a function predict that will produce “1” or “0” predictions given a dataset and a learned parameter vector \\(\\omega\\).\n13. Calculate the confusion matrix, and use it to calculate the training accuracy of your classifier and the F1 score.\n14. In order to verify that your line (decision boundary) is well plotted, color the points on the last Figure with respect to the predicted response.\n15. Now make the same plot but color the points with respect to their real labels. From this figure, count the number of the false positive predictions.\nplotDecisionBoundary function\nFor the rest, use the following function (or modify it to adapt it) for plotting the decision boundary.\n\ndef plotDecisionBoundary(omega, X, y):\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n                         np.arange(y_min, y_max, 0.1))\n    X_plot = np.c_[xx.ravel(), yy.ravel()]\n    X_plot = np.hstack((np.ones((X_plot.shape[0], 1)), X_plot))\n    y_plot = np.dot(X_plot, omega).reshape(xx.shape)\n    \n    plt.figure()\n    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], label=\"Admitted\")\n    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], label=\"Not admitted\")\n    plt.contour(xx, yy, y_plot, levels=[0])\n    plt.xlabel(\"Exam 1 score\")\n    plt.ylabel(\"Exam 2 score\")\n    plt.legend()\n    plt.show()\n\n\nplotDecisionBoundary(res.x, X, y)\n\n\n\n0.20365864300942144\n\n\n[-24.13930679   0.19805053   0.19320881]"
  },
  {
    "objectID": "TD2.html#regularized-logistic-regression",
    "href": "TD2.html#regularized-logistic-regression",
    "title": "Lab2: Logistic Regression & Regularization",
    "section": "Regularized logistic regression",
    "text": "Regularized logistic regression\nIn this part of the exercise, you will implement regularized logistic regression to predict whether microchips from a fabrication plant passes quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly.\nSuppose you are the product manager of the factory and you have the test results for some microchips on two different tests. From these two tests, you would like to determine whether the microchips should be accepted or rejected. To help you make the decision, you have a dataset of test results on past microchips, from which you can build a logistic regression model.\n2.1. Load the data in file tp2data2.txt from from here . Separate the features from the labels in two differents objects.\n\n\n\n2.2. Visualize the data in a 2-dimensional plot. Color the point with respect to their labels. What do you notice ?\n\n\n\n\n\nFeature mapping\nOne way to fit the data better is to create more features from each data point. In the function mapFeature defined below, we will map the features into all polynomial terms of \\(x_1\\) and \\(x_2\\) up to the sixth power.\n\\[\\text{mapFeature}(x) = \\begin{bmatrix} 1 & x_1 & x_2 & x_1^2 & x_1 x_2 & x_2^2 & x_1^3 & \\dots & x_1 x_2^5 & x_2^6 \\end{bmatrix}^T\\]\n\ndef mapFeature(X1, X2, degree=6):\n    \"\"\"\n    Maps the two input features to quadratic features used in the regularization exercise.\n\n    Returns a new feature array with more features, comprising of\n    X1, X2, X1.^2, X2.^2, X1*X2, X1*X2.^2, etc..\n\n    Parameters\n    ----------\n    X1 : array_like\n        A vector of shape (m, 1), containing one feature for all examples.\n\n    X2 : array_like\n        A vector of shape (m, 1), containing a second feature for all examples.\n        Inputs X1, X2 must be the same size.\n\n    degree: int, optional\n        The polynomial degree.\n\n    Returns\n    -------\n    : array_like\n        A matrix of of m rows, and columns depend on the degree of polynomial.\n    \"\"\"\n    if X1.ndim > 0:\n        out = [np.ones(X1.shape[0])]\n    else:\n        out = [np.ones(1)]\n\n    for i in range(1, degree + 1):\n        for j in range(i + 1):\n            out.append((X1 ** (i - j)) * (X2 ** j))\n\n    if X1.ndim > 0:\n        return np.stack(out, axis=1)\n    else:\n        return np.array(out)\n\nAs a result of this mapping, our vector of two features (the scores on two QA tests) has been transformed into a 28-dimensional vector. A logistic regression classifier trained on this higher-dimension feature vector will have a more complex decision boundary and will appear nonlinear when drawn in our 2-dimensional plot.\nWhile the feature mapping allows us to build a more expressive classifier, it also more susceptible to overfitting.\nIn the next parts of the exercise, you will implement regularized logistic regression to fit the data and also see for yourself how regularization can help combat the overfitting problem.\n2.3. Apply the mapFeature() function on the dataset. Verify that you get a 28-dimensional vector.\n\n\n\nCost function and gradient\nRecall that the regularized cost function in logistic regression is\n\\[ J(\\omega) = \\frac{1}{m} \\sum_{i=1}^m \\left[ -y^{(i)}\\log \\left( h_\\omega \\left(x^{(i)} \\right) \\right) - \\left( 1 - y^{(i)} \\right) \\log \\left( 1 - h_\\omega \\left( x^{(i)} \\right) \\right) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\omega_j^2 \\]\nNote that we do not regularize the parameters \\(\\omega_0\\). The gradient of the cost function is a vector where the \\(j^{th}\\) element is defined as follows:\n\\[ \\frac{\\partial J(\\omega)}{\\partial \\omega_0} = \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\omega \\left(x^{(i)}\\right) - y^{(i)} \\right) x_j^{(i)} \\qquad \\text{for } j =0 \\]\n\\[ \\frac{\\partial J(\\omega)}{\\partial \\omega_j} = \\left( \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\omega \\left(x^{(i)}\\right) - y^{(i)} \\right) x_j^{(i)} \\right) + \\frac{\\lambda}{m}\\omega_j \\qquad \\text{for } j \\ge 1 \\]\n2.4. Complete the function costFunctionReg() below. This function computes and returns the cost function and gradient for regularized logistic regression.\n\ndef costFunctionReg(omega, X, y, lambda_):\n    \"\"\"\n    Compute cost and gradient for logistic regression with regularization.\n    \n    Parameters\n    ----------\n    omega : array_like\n        Logistic regression parameters. A vector with shape (n, ). n is \n        the number of features including any intercept. If we have mapped\n        our initial features into polynomial features, then n is the total \n        number of polynomial features. \n    \n    X : array_like\n        The data set with shape (m x n). m is the number of examples, and\n        n is the number of features (after feature mapping).\n    \n    y : array_like\n        The data labels. A vector with shape (m, ).\n    \n    lambda_ : float\n        The regularization parameter. \n    \n    Returns\n    -------\n    J : float\n        The computed value for the regularized cost function. \n    \n    grad : array_like\n        A vector of shape (n, ) which is the gradient of the cost\n        function with respect to omega, at the current values of omega.\n    \n    Instructions\n    ------------\n    Compute the cost `J` of a particular choice of omega.\n    Compute the partial derivatives and set `grad` to the partial\n    derivatives of the cost w.r.t. each parameter in omega.\n    \"\"\"\n    # Initialize some useful values\n    m = y.size  # number of training examples\n\n    # You need to return the following variables correctly \n    J = 0\n    grad = np.zeros(omega.shape)\n\n    # ===================== YOUR CODE HERE ======================\n\n    \n    \n    # =============================================================\n    return J, grad\n\n\n\n\n2.5. Once you are done with the costFunctionReg, call it using the initial value of \\(\\omega\\) (initialized to all zeros), and also another test case where \\(\\omega\\) is all ones. The code is given below with the expected values. You should obtain the same values.\n\n# Initialize fitting parameters\n# X here has 28 columns\ninitial_omega = np.zeros(X.shape[1])\n\n# Set regularization parameter lambda to 1\n# DO NOT use `lambda` as a variable name in python\n# because it is a python keyword\nlambda_ = 1\n\n# Compute and display initial cost and gradient for regularized logistic\n# regression\ncost, grad = costFunctionReg(initial_omega, X, y, lambda_)\n\nprint('Cost at initial omega (zeros): {:.3f}'.format(cost))\nprint('Expected cost (approx)       : 0.693\\n')\n\nprint('Gradient at initial omega (zeros) - first five values only:')\nprint('\\t[{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}]'.format(*grad[:5]))\nprint('Expected gradients (approx) - first five values only:')\nprint('\\t[0.0085, 0.0188, 0.0001, 0.0503, 0.0115]\\n')\n\n\n# Compute and display cost and gradient\n# with all-ones omega and lambda = 10\ntest_omega = np.ones(X.shape[1])\ncost, grad = costFunctionReg(test_omega, X, y, 10)\n\nprint('------------------------------\\n')\nprint('Cost at test omega    : {:.2f}'.format(cost))\nprint('Expected cost (approx): 3.16\\n')\n\nprint('Gradient at test omega - first five values only:')\nprint('\\t[{:.4f}, {:.4f}, {:.4f}, {:.4f}, {:.4f}]'.format(*grad[:5]))\nprint('Expected gradients (approx) - first five values only:')\nprint('\\t[0.3460, 0.1614, 0.1948, 0.2269, 0.0922]')\n\nLearning parameters using scipy.optimize.minimize\n\n2.6. Use optimize.minimize to learn the optimal parameters \\(\\omega\\).\n\n\n\nPlotting the decision boundary\n2.7. To visualize the model learned by this classifier use the function plotDecisionBoundary to plot the (non-linear) decision boundary that separates the positive and negative examples.\n\nIn plotDecisionBoundary, we plot the non-linear decision boundary by computing the classifier’s predictions on an evenly spaced grid and then and draw a contour plot where the predictions change from y = 0 to y = 1.\n\nExtra\nTry out different regularization parameters for the dataset to understand how regularization prevents overfitting.\nNotice the changes in the decision boundary as you vary \\(\\lambda\\). With a small \\(\\lambda\\), you should find that the classifier gets almost every training example correct, but draws a very complicated boundary, thus overfitting the data.\nCredits\n\nThis lab is hugely inspired from Andrew Ng’s Machine Learning course. Supplementary material from dibgerge’s github was used."
  },
  {
    "objectID": "TD3.html",
    "href": "TD3.html",
    "title": "Lab3: Decision Trees and Random Forests",
    "section": "",
    "text": "To be appeared soon .."
  },
  {
    "objectID": "TD4.html",
    "href": "TD4.html",
    "title": "Lab4: Neural Networks",
    "section": "",
    "text": "To be appeared soon .."
  }
]