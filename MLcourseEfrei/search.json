[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "In this course you will learn about the state of the art of Machine Learning and also gain practice implementing and deploying machine learning algorithms."
  },
  {
    "objectID": "discriminantanalysis.html",
    "href": "discriminantanalysis.html",
    "title": "\n1Â  Discriminant Analysis\n",
    "section": "",
    "text": "Slides\n\n\n\nSlides for this chapter: ðŸ“„\nDiscriminant analysis is a popular method for multiple-class classiï¬cation. We will start first by the Linear Discriminant Analysis (LDA)."
  },
  {
    "objectID": "discriminantanalysis.html#introduction",
    "href": "discriminantanalysis.html#introduction",
    "title": "\n1Â  Discriminant Analysis\n",
    "section": "\n1.1 Introduction",
    "text": "1.1 Introduction\nAs we saw previously, Logistic regression involves directly modeling \\(\\mathbb{P} (Y = k|X = x)\\) using the logistic function, for the case of two response classes. In logistic regression, we model the conditional distribution of the response \\(Y\\), given the predictor(s) \\(X\\). We now consider an alternative and less direct approach to estimating these probabilities. In this alternative approach, we model the distribution of the predictors \\(X\\) separately in each of the response classes (i.e.Â given \\(Y\\)), and then use Bayesâ€™ theorem to flip these around into estimates for \\(\\mathbb{P} (Y = k|X = x)\\). When these distributions are assumed to be Normal, it turns out that the model is very similar in form to logistic regression.\nWhy not logistic regression? Why do we need another method, when we have logistic regression? There are several reasons:\n\nWhen the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable. Linear discriminant analysis does not suï¬€er from this problem.\nIf \\(n\\) is small and the distribution of the predictors \\(X\\) is approximately normal in each of the classes, the linear discriminant model is again more stable than the logistic regression model.\nLinear discriminant analysis is popular when we have more than two response classes."
  },
  {
    "objectID": "discriminantanalysis.html#bayes-theorem",
    "href": "discriminantanalysis.html#bayes-theorem",
    "title": "\n1Â  Discriminant Analysis\n",
    "section": "\n1.2 Bayesâ€™ Theorem",
    "text": "1.2 Bayesâ€™ Theorem\nBayesâ€™ theorem is stated mathematically as the following equation:\n\\[ P(A | B) = \\frac{P(A \\cap B)}{P(B)} =  \\frac{P(B|A) P(A)}{P(B)}\\]\nwhere \\(A\\) and \\(B\\) are events and \\(P(B) \\neq 0\\).\n\n\n\\(P(A | B)\\), a conditional probability, is the probability of observing event \\(A\\) given that \\(B\\) is true. It is called the posterior probability.\n\n\\(P(A)\\), is called the prior, is the initial degree of belief in A.\n\n\\(P(B)\\) is the likelihood.\n\n\n\n\n\n\n\nTip\n\n\n\nThe posterior probability can be written in the memorable form as :\nPosterior probability \\(\\propto\\) Likelihood \\(\\times\\) Prior probability.\n\n\nExtended form:\nSuppose we have a partition \\(\\{A_i\\}\\) of the sample space, the even space is given or conceptualized in terms of \\(P(A_j)\\) and \\(P(B | A_j)\\). It is then useful to compute \\(P(B)\\) using the law of total probability:\n\\[ P(B) = \\sum_j P(B|A_j) P(A_j) \\]\n\\[ \\Rightarrow P(A_i|B) = \\frac{P(B|A_i) P(A_i)}{\\sum_j P(B|A_j) P(A_j)} \\]\nBayesâ€™ Theorem for Classification:\nSuppose that we wish to classify an observation into one of \\(K\\) classes, where \\(K \\geq 2\\). In other words, the qualitative response variable \\(Y\\) can take on \\(K\\) possible distinct and unordered values.\nLet \\(\\pi_k\\) represent the overall or prior probability that a randomly chosen observation comes from the \\(k\\)-th class; this is the probability that a given observation is associated with the \\(k\\)-th category of the response variable \\(Y\\).\nLet \\(f_k(X) \\equiv P(X = x|Y = k)\\) denote the density function of \\(X\\) for an observation that comes from the \\(k\\)-th class. In other words, \\(f_k(x)\\) is relatively large if there is a high probability that an observation in the \\(k\\)-th class has \\(X \\approx x\\), and \\(f_k(x)\\) is small if it is very unlikely that an observation in the \\(k\\)-th class has \\(X \\approx x\\). Then Bayesâ€™ theorem states that\n\\[\nP(Y=k|X=x) = \\frac{ \\pi_k f_k(x)}{\\sum_{c=1}^K \\pi_c f_c(x)}\n\\qquad(1.1)\\]\nAs we did in the last chapter, we will use the abbreviation \\(p_k(X) =P(Y = k|X)\\).\nThe equation above stated by Bayesâ€™ theorem suggests that instead of directly computing \\(p_k(X)\\) as we did in the logistic regression, we can simply plug in estimates of \\(\\pi_k\\) and \\(f_k(X)\\) into the equation. In general, estimating \\(\\pi_k\\) is easy (the fraction of the training observations that belong to the \\(k\\)-th class). But estimating \\(f_k(X)\\) tends to be more challenging.\n\nRecall that \\(p_k(x)\\) is the posterior probability that an observation \\(X=x\\) belongs to \\(k\\)-th class.\n\n\nIf we can find a way to estimate \\(f_k(X)\\), we can develop a classifier with the lowest possibe error rate out of all classifiers."
  },
  {
    "objectID": "discriminantanalysis.html#lda-for-p1",
    "href": "discriminantanalysis.html#lda-for-p1",
    "title": "\n1Â  Discriminant Analysis\n",
    "section": "\n1.3 LDA for \\(p=1\\)\n",
    "text": "1.3 LDA for \\(p=1\\)\n\nAssume that \\(p=1\\), which mean we have only one predictor. We would like to obtain an estimate for \\(f_k(x)\\) that we can plug into the Equation (1.1) in order to estimate \\(p_k(x)\\). We will then classify an observation to the class for which \\(p_k(x)\\) is greatest.\nIn order to estimate \\(f_k(x)\\), we will first make some assumptions about its form.\nSuppose we assume that \\(f_k(x)\\) is normal (Gaussian). In the one-dimensional setting, the normal density take the form\n\\[\nf_k(x)= \\frac{1}{\\sigma_k\\sqrt{2\\pi}} \\exp \\big( - \\frac{1}{2\\sigma_k^2 } (x-\\mu_k)^2\\big)\n\\qquad(1.2)\\]\nwhere \\(\\mu_k\\) and \\(\\sigma_k^2\\) are the mean and variance parameters for \\(k\\)-th class. Let us assume that \\(\\sigma_1^2 = \\ldots = \\sigma_K^2 = \\sigma^2\\) (which means there is a shared variance term across all \\(K\\) classes). Plugging Equation (1.2) into the Bayes formula in Equation (1.1) we get,\n\\[\np_k(x) = \\frac{  \\pi_k \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2} \\big(\\frac{x-\\mu_k}{\\sigma}\\big)^2 } }{  \\sum_{c=1}^K  \\pi_c \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2} \\big(\\frac{x-\\mu_c}{\\sigma}\\big)^2 } }\n\\qquad(1.3)\\]\n\n\n\n\n\n\nImportant\n\n\n\nNote that \\(\\pi_k\\) and \\(\\pi_c\\) denote the prior probabilities. And \\(\\pi\\) is the mathematical constant \\(\\pi \\approx 3.14159\\).\n\n\nTo classify at the value \\(X = x\\), we need to see which of the \\(p_k(x)\\) is largest. Taking logs, and discarding terms that do not depend on \\(k\\), we see that this is equivalent to assigning \\(x\\) to the class with the largest discriminant score:\n\\[\n\\delta_k(x) = x.\\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + \\log (\\pi_k)\n\\qquad(1.4)\\]\nNote that \\(\\delta_k(x)\\) is a linear function of \\(x\\).\n\n\n\n\n\n\nTip\n\n\n\n\nThe decision surfaces (e.g.Â decision boundaries) for a linear discriminant classifiers are defined by the linear equations \\(\\delta_k(x) = \\delta_c(x)\\), for all classes \\(k\\neq c\\). It represents the set of values \\(x\\) for which the probability of belonging to classes \\(k\\) and \\(c\\) is the same, \\(0.5\\).\nExample: If \\(K=2\\) and \\(\\pi_1=\\pi_2\\), then the desicion boundary is at \\(x=\\frac{\\mu_1+\\mu2}{2}\\) (Prove it!).\nAn example where \\(\\mu_1=-1.5\\), \\(\\mu_2=1.5\\), \\(\\pi_1=\\pi_2=0.5\\) and \\(\\sigma^2=1\\) is shown in this following figure\n\n\n\n\n\nSee this  video  to understand more about decision boundary (Applied on logistic regression).\nAs we classify a new point according to which density is highest, when the priors are diï¬€erent we take them into account as well, and compare \\(\\pi_k f_k(x)\\). On the right of the following figure, we favor the pink class (remark that the decision boundary has shifted to the left)."
  },
  {
    "objectID": "discriminantanalysis.html#estimating-the-parameters",
    "href": "discriminantanalysis.html#estimating-the-parameters",
    "title": "\n1Â  Discriminant Analysis\n",
    "section": "\n1.4 Estimating the parameters",
    "text": "1.4 Estimating the parameters\nTypically we donâ€™t know these parameters; we just have the training data. In that case we simply estimate the parameters and plug them into the rule.\nLet \\(n\\) the total number of training observations, and \\(n_k\\) the number of training observations in the \\(k\\)-th class. The following estimates are used:\n\\[\\begin{align*}\n\\hat{\\pi}_k &= \\frac{n_k}{n} \\\\\n\\hat{\\mu}_k &= \\frac{1}{n_k} \\sum_{i: y_i=k} x_i \\\\\n\\hat{\\sigma}^2 &= \\frac{1}{n - K} \\sum_{k=1}^K \\sum_{i: y_i=k} (x_i-\\hat{\\mu}_k)^2 \\\\\n&= \\sum_{k=1}^K \\frac{n_k-1}{n - K} . \\hat{\\sigma}_k^2\n\\end{align*}\\]\nwhere \\(\\hat{\\sigma}_k^2 = \\frac{1}{n_k-1}\\sum_{i: y_i=k}(x_i-\\hat{\\mu}_k)^2\\) is the usual formula for the estimated variance in the -\\(k\\)-th class.\nThe linear discriminant analysis (LDA) classifier plugs these estimates in Equation (1.4) and assigns an observation \\(X=x\\) to the class for which\n\\[\n\\hat{\\delta}_k(x) = x.\\frac{\\hat{\\mu}_k}{\\hat{\\sigma}^2} - \\frac{\\hat{\\mu}_k^2}{2\\hat{\\sigma}^2} + \\log (\\hat{\\pi}_k)\n\\qquad(1.5)\\]\nis largest.\nThe discriminant functions in Equation (1.5) are linear functions of \\(x\\).\n\nRecall that we assumed that the observations come from a normal distribution with a common variance \\(\\sigma^2\\)."
  },
  {
    "objectID": "discriminantanalysis.html#lda-for-p-1",
    "href": "discriminantanalysis.html#lda-for-p-1",
    "title": "\n1Â  Discriminant Analysis\n",
    "section": "\n1.5 LDA for \\(p > 1\\)\n",
    "text": "1.5 LDA for \\(p > 1\\)\n\nLet us now suppose that we have multiple predictors. We assume that \\(X=(X_1,X_2,\\ldots,X_p)\\) is drawn from multivariate Gaussian distribution (assuming they have a common covariance matrix, e.g.Â same variances as in the case of \\(p=1\\)). The multivariate Gaussian distribution assumes that each individual predictor follows a one-dimensional normal distribution as in Equation (1.2), with some correlation between each pair of predictors.\nTo indicate that a \\(p\\)-dimensional random variable \\(X\\) has a multivariate Gaussian distribution, we write \\(X \\sim \\mathcal{N}(\\mu,\\Sigma)\\). Where\n\\[ \\mu = E(X) = \\begin{pmatrix}\n    \\mu_1 \\\\\n    \\mu_2 \\\\\n    \\vdots \\\\\n    \\mu_p\n\\end{pmatrix} \\]\nand, $$ = Cov(X) =\n\\[\\begin{pmatrix}\n\n\n    \\sigma_1^2 & Cov[X_1, X_2]  & \\dots  & Cov[X_1, X_p] \\\\\n    Cov[X_2, X_1] & \\sigma_2^2  & \\dots  & Cov[X_2, X_p] \\\\\n    \\vdots & \\vdots &  \\ddots & \\vdots \\\\\n    Cov[X_p, X_1] & Cov[X_p, X_2]  & \\dots  & \\sigma_p^2\n\\end{pmatrix}\\]\n$$\n\n\\(\\Sigma\\) is the \\(p\\times p\\) covariance matrix of \\(X\\).\n\nFormally, the multivariate Gaussian density is deï¬ned as\n\\[f(x) = \\frac{1}{(2\\pi)^{p/2} |\\Sigma|^{1/2}} \\exp \\bigg( - \\frac{1}{2} (x-\\mu)^T \\Sigma^{-1} (x-\\mu) \\bigg)\n\\]\nPlugging the density function for the \\(k\\)-th class, \\(f_k(X=x)\\), into Equation (1.1) reveals that the Bayes classifier assigns an observation \\(X=x\\) to the class for which\n\\[\n\\delta_k(x) = x^T \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\Sigma^{-1}  \\mu_k + \\log \\pi_k\n\\qquad(1.6)\\]\nis largest. This is the vector/matrix version of (1.4).\nAn example is shown in the following figure. Three equally-sized Gaussian classes are shown with class-specific mean vectors and a common covariance matrix (\\(\\pi_1=\\pi_2=\\pi_3=1/3\\)). The three ellipses represent regions that contain \\(95\\%\\) of the probability for each of the three classes. The dashed lines are the Bayes decision boundaries.\n\n\n\nRecall that the decision boundaries represent the set of values \\(x\\) for which \\(\\delta_k(x)=\\delta_c(x)\\); i.e.Â for \\(k\\neq c\\).\n\\[ x^T \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\Sigma^{-1}  \\mu_k = x^T \\Sigma^{-1} \\mu_c - \\frac{1}{2} \\mu_c^T \\Sigma^{-1}  \\mu_c  \\]\nNote that there are three lines representing the Bayes decision boundaries because there are three pairs of classes among the three classes. That is, one Bayes decision boundary separates class 1 from class 2, one separates class 1 from class 3, and one separates class 2 from class 3. These three Bayes decision boundaries divide the predictor space into three regions. The Bayes classiï¬er will classify an observation according to the region in which it is located.\nOnce again, we need to estimate the unknown parameters \\(\\mu_1,\\ldots,\\mu_k,\\) and \\(\\pi_1,\\ldots,\\pi_k,\\) and \\(\\Sigma\\); the formulas are similar to those used in the one-dimensional case. To assign a new observation \\(X = x\\), LDA plugs these estimates into Equation (1.6) and classiï¬es to the class for which \\(\\delta_k(x)\\) is largest.\nNote that in Equation (1.6) \\(\\delta_k(x)\\) is a linear function of \\(x\\); that is, the LDA decision rule depends on \\(x\\) only through a linear combination of its elements (e.g.Â the decision boundaries are linear). This is the reason for the word linear in LDA."
  },
  {
    "objectID": "discriminantanalysis.html#making-predictions",
    "href": "discriminantanalysis.html#making-predictions",
    "title": "\n1Â  Discriminant Analysis\n",
    "section": "\n1.6 Making predictions",
    "text": "1.6 Making predictions\nOnce we have estimates \\(\\hat{\\delta}_k(x)\\), we can turn these into estimates for class probabilities:\n\\[ \\hat{P}(Y=k|X=x)= \\frac{e^{\\hat{\\delta}_k(x)}}{\\sum_{c=1}^K e^{\\hat{\\delta}_c(x)}} \\]\nSo classifying to the largest \\(\\hat{\\delta}_k(x)\\) amounts to classifying to the class for which \\(\\hat{P}(Y=k|X=x)\\) is largest.\nWhen \\(K=2\\), we classify to class 2 if \\(\\hat{P}(Y=2|X=x) \\geq 0.5\\), else to class \\(1\\)."
  },
  {
    "objectID": "discriminantanalysis.html#other-forms-of-discriminant-analysis",
    "href": "discriminantanalysis.html#other-forms-of-discriminant-analysis",
    "title": "\n1Â  Discriminant Analysis\n",
    "section": "\n1.7 Other forms of Discriminant Analysis",
    "text": "1.7 Other forms of Discriminant Analysis\n\\[P(Y=k|X=x) = \\frac{ \\pi_k f_k(x)}{\\sum_{c=1}^K \\pi_c f_c(x)}\\]\nWe saw before that when \\(f_k(x)\\) are Gaussian densities, whith the same covariance matrix \\(\\Sigma\\) in each class, this leads to Linear Discriminant Analysis (LDA).\nBy altering the forms for \\(f_k(x)\\), we get different classifiers.\n\nWith Gaussians but different \\(\\Sigma_k\\) in each class, we get Quadratic Discriminant Analysis (QDA).\nWith \\(f_k(x) = \\prod_{j=1}^p f_{jk}(x_j)\\) (conditional independence model) in each class we get Naive Bayes. (For Gaussian, this mean the \\(\\Sigma_k\\) are diagonal, e.g.Â \\(Cov(X_i,X_j)=0 \\,\\, \\forall \\, \\, 1\\leq i,j \\leq p\\)).\nMany other forms by proposing specific density models for \\(f_k(x)\\), including nonparametric approaches.\n\n\n1.7.1 Quadratic Discriminant Analysis (QDA)\nLike LDA, the QDA classiï¬er results from assuming that the observations from each class are drawn from a Gaussian distribution, and plugging estimates for the parameters into Bayesâ€™ theorem in order to perform prediction.\nHowever, unlike LDA, QDA assumes that each class has its own covariance matrix. Under this assumption, the Bayes classiï¬er assigns an observation \\(X = x\\) to the class for which\n\\[\\begin{align*}\n\\delta_k(x) &= - \\frac{1}{2} (x-\\mu)^T \\Sigma_k^{-1} (x-\\mu) - \\frac{1}{2} \\log |\\Sigma_k| + \\log \\pi_k \\\\\n            &= - \\frac{1}{2} x^T \\Sigma_k^{-1} x + \\frac{1}{2} x^T \\Sigma_k^{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\Sigma_k^{-1} \\mu_k - \\frac{1}{2} \\log |\\Sigma_k| + \\log \\pi_k\n\\end{align*}\\]\nis largest.\nUnlike in LDA, the quantity \\(x\\) appears as a quadratic function in QDA. This is where QDA gets its name.\n\n\n\n\n\n\nImportant\n\n\n\nThe decision boundary in QDA is non-linear. It is quadratic (a curve).\n\n\n\n1.7.2 Naive Bayes\nWe use Naive Bayes classifier if the features are independant in each class. It is useful when \\(p\\) is large (unklike LDA and QDA).\nNaive Bayes assumes that each \\(\\Sigma_k\\) is diagonal, so\n\\[\\begin{align*}\n\\delta_k(x) &\\propto \\log \\bigg[\\pi_k \\prod_{j=1}^p f_{kj}(x_j) \\bigg] \\\\\n            &= -\\frac{1}{2} \\sum_{j=1}^p \\frac{(x_j-\\mu_{kj})^2}{\\sigma_{kj}^2} + \\log \\pi_k\n\\end{align*}\\]\nIt can used for mixed feature vectors (qualitative and quantitative). If \\(X_j\\) is qualitative, we replace \\(f_{kj}(x_j)\\) by probability mass function (histogram) over discrete categories."
  },
  {
    "objectID": "discriminantanalysis.html#lda-vs-logistic-regression",
    "href": "discriminantanalysis.html#lda-vs-logistic-regression",
    "title": "\n1Â  Discriminant Analysis\n",
    "section": "\n1.8 LDA vs Logistic Regression",
    "text": "1.8 LDA vs Logistic Regression\nthe logistic regression and LDA methods are closely connected. Consider the two-class setting with \\(p =1\\) predictor, and let \\(p_1(x)\\) and \\(p_2(x)=1âˆ’p_1(x)\\) be the probabilities that the observation \\(X = x\\) belongs to class 1 and class 2, respectively. In the LDA framework, we can see from Equation (1.4) (and a bit of simple algebra) that the log odds is given by\n\\[ \\log \\bigg(\\frac{p_1(x)}{1-p_1(x)}\\bigg) = \\log \\bigg(\\frac{p_1(x)}{p_2(x)}\\bigg) = c_0 + c_1 x\\]\nwhere \\(c_0\\) and \\(c_1\\) are functions of \\(\\mu_1, \\mu_2,\\) and \\(\\sigma^2\\).\nOn the other hand, we know that in logistic regression\n\\[ \\log \\bigg(\\frac{p_1}{1-p_1}\\bigg) = \\beta_0 + \\beta_1 x\\]\nBoth of the equations above are linear functions of \\(x\\). Hence both logistic regression and LDA produce linear decision boundaries. The only diï¬€erence between the two approaches lies in the fact that \\(\\beta_0\\) and \\(\\beta_1\\) are estimated using maximum likelihood, whereas \\(c_0\\) and \\(c_1\\) are computed using the estimated mean and variance from a normal distribution. This same connection between LDA and logistic regression also holds for multidimensional data with \\(p> 1\\).\n\n\n\n\n\n\nTip\n\n\n\n\nLogistic regression uses the conditional likelihood based on \\(P(Y|X)\\) (known as discriminative learning).\nLDA uses the full likelihood based on \\(P(X,Y )\\) (known as generative learning).\nDespite these differences, in practice the results are often very similar.\n\nRemark: Logistic regression can also fit quadratic boundaries like QDA, by explicitly including quadratic terms in the model.\n\n\n\nâ—¼"
  },
  {
    "objectID": "Lab-DA.html",
    "href": "Lab-DA.html",
    "title": "Lab on Discriminant Analysis",
    "section": "",
    "text": "You are free to apply this lab in  or Python. The codes given in the lab and the main instructions are  codes. It is up to you to adapt them if you use Python.\nIf you use Python, verify that scikit-learn is installed and verify its version (it should at least 0.21).\nIn , we are going to use the lda() and qda() functions from MASS library.\nIn Python, you can use :\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nDuring this session we are going to analyse the Social_Network_Ads dataset ðŸ”¢. This dataset contains informations of users of a social network and if they bought a specified product. We are going to model the variable Purchased in function of Age and EstimatedSalary. We will fit using the models Logistic Regression, LDA, QDA, and Naive Bayes."
  },
  {
    "objectID": "Lab-DA.html#logistic-regression",
    "href": "Lab-DA.html#logistic-regression",
    "title": "Lab on Discriminant Analysis",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n1. First, letâ€™s do the pre-processing steps and fit a logistic regression model. Please read and understand very well the following code (read the comments!). Then copy what is necessary for your report (but remove my comments!).\n\n\n\n\n\n\nTip\n\n\n\nYou can download the dataset from here ðŸ”¢.\n\n\n\n# Loading the dataset.. I have putted it into a folder called \"datasets\"\ndataset <- read.csv('http://www.mghassany.com/MLcourseEfrei/datasets/Social_Network_Ads.csv')\n\n# Describing and Exploring the dataset\n\nstr(dataset) # to show the structure of the dataset. \nsummary(dataset) # will show some statistics of every column. \n# Remark what it shows when the column is a numerical or categorical variable.\n# Remark that it has no sense for the variable User.ID\n\nboxplot(Age ~ Purchased, data=dataset, col = \"blue\", main=\"Boxplot Age ~ Purchased\")\n# You know what is a boxplot right? I will let you interpret it.\nboxplot(EstimatedSalary ~ Purchased, data=dataset,col = \"red\",\n main=\"Boxplot EstimatedSalary ~ Purchased\")\n# Another boxplot\n\naov(EstimatedSalary ~Purchased, data=dataset)\n# Anova test, but we need to show the summary of \n# it in order to see the p-value and to interpret.\n\nsummary(aov(EstimatedSalary ~Purchased, data=dataset))\n# What do you conclude ?\n# Now another anova test for the variable Age\nsummary(aov(Age ~Purchased, data=dataset))\n\n# There is a categorical variable in the dataset, which is Gender.\n# Of course we cannot show a boxplot of Gender and Purchased.\n# But we can show a table, or a mosaic plot, both tell the same thing.\ntable(dataset$Gender,dataset$Purchased)\n# Remark for the function table(), that\n# in lines we have the first argument, and in columns we have the second argument.\n# Don't forget this when you use table() to show a confusion matrix!\nmosaicplot(~ Purchased + Gender, data=dataset,\n  main = \"MosaicPlot of two categorical variables: Puchased & Gender\",\n  color = 2:3, las = 1)\n\n# since these 2 variables are categorical, we can apply\n# a Chi-square test. The null hypothesis is the independance between\n# these variables. You will notice that p-value = 0.4562 which is higher than 0.05 (5%)\n# so we cannot reject the null hypothesis. \n# conclusion: there is no dependance between Gender and Purchased (who\n# said that women buy more than men? hah!)\n\nchisq.test(dataset$Purchased, dataset$Gender)\n\n# Let's say we want to remove the first two columns as we are not going to use them.\n# But, we can in fact use a categorical variable as a predictor in logistic regression.\n# It will treat it the same way as in regression. Check Appendix C.\n# Try it by yourself if you would like to.\ndataset = dataset[3:5]\nstr(dataset) # show the new structure of dataset\n\n\n# splitting the dataset into training and testing sets\nlibrary(caTools)\nset.seed(123) # CHANGE THE VALUE OF SEED. PUT YOUR STUDENT'S NUMBER INSTEAD OF 123.\nsplit = sample.split(dataset$Purchased, SplitRatio = 0.75)\ntraining_set = subset(dataset, split == TRUE)\ntest_set = subset(dataset, split == FALSE)\n\n# scaling\n# So here, we have two continuous predictors, Age and EstimatedSalary.\n# There is a very big difference in their scales (units).\n# That's why we scale them. But it is not always necessary.\n\ntraining_set[-3] <- scale(training_set[-3]) #only first two columns\ntest_set[-3] <- scale(test_set[-3])\n\n# Note that, we replace the columns of Age and EstimatedSalary in the training and\n# test sets but their scaled versions. I noticed in a lot of reports that you scaled\n# but you did not do the replacing.\n# Note too that if you do it column by column you will have a problem because \n# it will replace the column by a matrix, you need to retransform it to a vector then.\n# Last note, to call the columns Age and EstimatedSalary we can it like I did or \n# training_set[c(1,2)] or training_set[,c(1,2)] or training_set[,c(\"Age\",\"EstimatedSalary\")]\n\n\n# logistic regression\n\nclassifier.logreg <- glm(Purchased ~ Age + EstimatedSalary , family = binomial, data=training_set)\nclassifier.logreg\nsummary(classifier.logreg)\n\n# prediction\npred.glm = predict(classifier.logreg, newdata = test_set[,-3], type=\"response\")\n# Do not forget to put type response. \n# By the way, you know what you get when you do not put it, right?\n\n# Now let's assign observations to classes with respect to the probabilities\npred.glm_0_1 = ifelse(pred.glm >= 0.5, 1,0)\n# I created a new vector, because we need the probabilities later for the ROC curve.\n\n# show some values of the vectors\nhead(pred.glm)\nhead(pred.glm_0_1)\n\n# confusion matrix\ncm = table(test_set[,3], pred.glm_0_1)\ncm\n# First line to store it into cm, second line to show the matrix! \n\n# You remember my note about table() function and the order of the arguments?\ncm = table(pred.glm_0_1, test_set[,3])\ncm\n\n# You can show the confusion matrix in a mosaic plot by the way\nmosaicplot(cm,col=sample(1:8,2)) # colors are random between 8 colors.\n\n# ROC\nrequire(ROCR)\nscore <- prediction(pred.glm,test_set[,3]) # we use the predicted probabilities not the 0 or 1\nperformance(score,\"auc\") # y.values\nplot(performance(score,\"tpr\",\"fpr\"),col=\"green\")\nabline(0,1,lty=8)\n\nSo now we have a logistic regression model stored in classifier.logreg. It is a model of Purchased in function of Age and EstimatedSalary."
  },
  {
    "objectID": "Lab-DA.html#linear-discriminant-analysis-lda",
    "href": "Lab-DA.html#linear-discriminant-analysis-lda",
    "title": "Lab on Discriminant Analysis",
    "section": "Linear Discriminant Analysis (LDA)",
    "text": "Linear Discriminant Analysis (LDA)\nLet us apply linear discriminant analysis (LDA) now. First we will make use of the lda() function in the package MASS. Second, you are going to create the model and predict the classes by yourself without using the lda() function. And we will visualize the decision boundary of LDA.\n2. Fit a LDA model of Purchased in function of Age and EstimatedSalary. Name the model classifier.lda.\n\nlibrary(MASS)\nclassifier.lda <- lda(Purchased~Age+EstimatedSalary, data=training_set)\n\n3. Call classifier.lda and understand what does it compute.\n\n\n\nPlus: If you enter the following you will be returned with a list of summary information concerning the computation:\n\nclassifier.lda$prior\nclassifier.lda$means\n\n4. On the test set, predict the probability of purchasing the product by the users using the model classifier.lda. Remark that when we predict using LDA, we obtain a list instead of a matrix, do str() for your predictions to see what do you get.\nRemark: we get the predicted class here, without being obligated to round the predictions as we did for logistic regression.\n\n\n\n5. Compute the confusion matrix and compare the predictions results obtained by LDA to the ones obtained by logistic regression. What do you remark? (Hint: compare the accuracy)\n\n\n\n6. Now let us plot the decision boundary obtained with LDA. You saw in the course that decision boundary for LDA represent the set of values \\(x\\) where \\(\\delta_k(x) = \\delta_c(x)\\). Recall that\n\\[\\delta_k(X) = x^T \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\Sigma^{-1}  \\mu_k + \\log \\pi_k\\]\nHere in our case, we have 2 classes (\\(K=2\\)) and 2 predictors (\\(p=2\\)). So the decision boundary (which is linear in the case of LDA, and line in our case since \\(p=2\\)) will verify the equation \\(\\delta_0(x) = \\delta_1(x)\\) Since we have two classes â€œ0â€ and â€œ1â€. In the case of LDA this leads to linear boundary and is easy to be plotted. But in more complicated cases it is difficult to manually simplify the equations and plot the decision boundary. Anyway, there is a smart method to plot (but a little bit costy) the decision boundary in R using the function contour(), the corresponding code is the following (you must adapt it and use it to plot your decision boundary):\n\n# create a grid corresponding to the scales of Age and EstimatedSalary\n# and fill this grid with lot of points\nX1 = seq(min(training_set[, 1]) - 1, max(training_set[, 1]) + 1, by = 0.01)\nX2 = seq(min(training_set[, 2]) - 1, max(training_set[, 2]) + 1, by = 0.01)\ngrid_set = expand.grid(X1, X2)\n# Adapt the variable names\ncolnames(grid_set) = c('Age', 'EstimatedSalary')\n\n# plot 'Estimated Salary' ~ 'Age'\nplot(test_set[, 1:2],\n     main = 'Decision Boundary LDA',\n     xlab = 'Age', ylab = 'Estimated Salary',\n     xlim = range(X1), ylim = range(X2))\n\n# color the plotted points with their real label (class)\npoints(test_set[1:2], pch = 21, bg = ifelse(test_set[, 3] == 1, 'green4', 'red3'))\n\n# Make predictions on the points of the grid, this will take some time\npred_grid = predict(classifier.lda, newdata = grid_set)$class\n\n# Separate the predictions by a contour\ncontour(X1, X2, matrix(as.numeric(pred_grid), length(X1), length(X2)), add = TRUE)"
  },
  {
    "objectID": "Lab-DA.html#quadratic-discriminant-analysis-qda",
    "href": "Lab-DA.html#quadratic-discriminant-analysis-qda",
    "title": "Lab on Discriminant Analysis",
    "section": "Quadratic Discriminant Analysis (QDA)",
    "text": "Quadratic Discriminant Analysis (QDA)\nTraining and assessing a QDA model in R is very similar in syntax to training and assessing a LDA model. The only difference is in the function name qda()\n7. Fit a QDA model of Purchased in function of Age and EstimatedSalary. Name the model classifier.qda.\n\n# qda() is a function of library(MASS)\nclassifier.qda <- qda(Purchased~., data = training_set)\n\n8. Make predictions on the test_set using the QDA model classifier.qda. Show the confusion matrix and compare the results with the predictions obtained using the LDA model classifier.lda.\n\n\n\n9. Plot the decision boundary obtained with QDA. Color the points with the real labels."
  },
  {
    "objectID": "Lab-DA.html#comparison",
    "href": "Lab-DA.html#comparison",
    "title": "Lab on Discriminant Analysis",
    "section": "Comparison",
    "text": "Comparison\n10. In order to compare the methods we used, plot on the same Figure the ROC curve for each classifier we fitted and compare the correspondant AUC. What was the best model for this dataset? Can you justify it?\nRemark: If you use the ROCR package:\n\nFor Logistic regression, use the predicted probabilities in the prediction() (and not the round values â€œ0â€ or â€œ1â€).\nFor LDA and QDA, put pred.lda$posterior[,2] in the prediction() function (those are the posterior probabilities that observations belong to class â€œ1â€)."
  },
  {
    "objectID": "Lab-DA.html#lda-from-scratch",
    "href": "Lab-DA.html#lda-from-scratch",
    "title": "Lab on Discriminant Analysis",
    "section": "LDA from scratch",
    "text": "LDA from scratch\n11. Now let us build a LDA model for our data set without using the lda() function. You are free to do it by creating a function or without creating one. Go back to question 6 and see what did you obtain by using lda(). It computes the prior probability of group membership and the estimated group means for each of the two groups. Additional information that is not provided, but may be important, is the single covariance matrix that is being used for the various groupings.\n\n\n\n\n\n\nTip\n\n\n\nIn LDA, we compute for every observation \\(x\\) its discriminant score \\(\\delta_k(x)\\). Then we attribute \\(x\\) to the class that has the highest \\(\\delta\\). Recall that\n\\[\\delta_k(x) = x^T \\Sigma^{-1} \\mu_k - \\frac{1}{2} \\mu_k^T \\Sigma^{-1}  \\mu_k + \\log \\pi_k\\]\nSo to compute \\(\\delta_k(x)\\) we need to estimate \\(\\pi_k\\), \\(\\mu_k\\) and \\(\\Sigma\\).\nNote that \\[x=\\begin{pmatrix}\n            X_1 \\\\\n            X_2\n            \\end{pmatrix}\\] and here \\(X_1\\)=Age and \\(X_2\\)=EstimatedSalary.\n\n\nSo let us do it step by step, first we will do the estimates:\n11.1 Subset the training set into two sets: class0 where Purchased = 0 and class1 where Purchased = 1).\n11.2 Compute \\(\\pi_0\\) and \\(\\pi_1\\).\n\\[\\pi_i = N_i / N, \\,\\, \\text{where} \\,\\, N_i \\,\\, \\text{is the number of data points in group i}\\]\n11.3 Compute \\(\\mu_0\\) and \\(\\mu_1\\).\n\\[\\mu_0 = \\begin{pmatrix}\n   \\mu_0(X_1) \\\\\n   \\mu_0(X_2)\n   \\end{pmatrix} \\,\\, \\text{and} \\,\\, \\mu_1 = \\begin{pmatrix}\n   \\mu_1(X_1) \\\\\n   \\mu_1(X_2)\n   \\end{pmatrix}\\]\nwhere, for example, \\(\\mu_0(X_1)\\) is the mean of the variable \\(X_1\\) in the group \\(0\\) (the subset class0).\n11.4 Compute \\(\\Sigma\\). In the case of two classes like here, it is computed by calculating the following:\n\\[\\Sigma = \\frac{(N_0-1)\\Sigma_0 + (N_1-1)\\Sigma_1}{N_0+N_1-2}\\]\nwhere \\(\\Sigma_i\\) is the estimated covariance matrix for specific group \\(i\\).\nRemark: Recall that in LDA we use the same \\(\\Sigma\\). But in QDA we do not.\n11.5. Now that we have computed all the needed estimates, we can calculate \\(\\delta_0(x)\\) and \\(\\delta_1(x)\\) for any observation \\(x\\). And we will attribute \\(x\\) to the class with the highest \\(\\delta\\). First, try it for \\(x\\) where \\(x^T=(1,1.5)\\), what is class prediction for this spesific \\(x\\)?\n11.6. Compute the discriminant scores \\(\\delta\\) for the test set (a matrix \\(100\\times 2\\)), predict the classes and compare your results with the results obtained with the lda() function.\n\n\n\n\nâ—¼"
  },
  {
    "objectID": "dimreduction.html",
    "href": "dimreduction.html",
    "title": "2Â  Dimensionality Reduction",
    "section": "",
    "text": "Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension."
  },
  {
    "objectID": "dimreduction.html#test-slides",
    "href": "dimreduction.html#test-slides",
    "title": "2Â  Dimensionality Reduction",
    "section": "2.1 test slides",
    "text": "2.1 test slides\n\n\nâ—¼"
  },
  {
    "objectID": "Lab-Dim-Red.html",
    "href": "Lab-Dim-Red.html",
    "title": "Lab on Dimensionality Reduction",
    "section": "",
    "text": "You are free to apply this lab in  or Python.\nIn Python, use sklearn.decomposition.PCA for PCA and sklearn.manifold.TSNE for t-SNE.\nIn , you are free to use princomp(), prcomp() or factominer::PCA()."
  },
  {
    "objectID": "Lab-Dim-Red.html#the-dataset",
    "href": "Lab-Dim-Red.html#the-dataset",
    "title": "Lab on Dimensionality Reduction",
    "section": "The dataset",
    "text": "The dataset\nEmployement in European countries in the late 70s\nThe purpose of this case study is to reveal the structure of the job market and economy in different developed countries. The final aim is to have a meaningful and rigorous plot that is able to show the most important features of the countries in a concise form.\nThe eurojob dataset ðŸ”¢ contains the data employed in this case study. It contains the percentage of workforce employed in 1979 in 9 industries for 26 European countries. The industries measured are:\n\nAgriculture (Agr)\nMining (Min)\nManufacturing (Man)\nPower supply industries (Pow)\nConstruction (Con)\nService industries (Ser)\nFinance (Fin)\nSocial and personal services (Soc)\nTransport and communications (Tra)"
  },
  {
    "objectID": "Lab-Dim-Red.html#pca",
    "href": "Lab-Dim-Red.html#pca",
    "title": "Lab on Dimensionality Reduction",
    "section": "PCA",
    "text": "PCA\n1. Import the eurojob dataset ðŸ”¢ .\nIf the dataset is imported correctly, then it should look like this:\n\n\n\nThe eurojob dataset.\n\nCountry\nAgr\nMin\nMan\nPow\nCon\nSer\nFin\nSoc\nTra\n\n\n\nBelgium\n3.3\n0.9\n27.6\n0.9\n8.2\n19.1\n6.2\n26.6\n7.2\n\n\nDenmark\n9.2\n0.1\n21.8\n0.6\n8.3\n14.6\n6.5\n32.2\n7.1\n\n\nFrance\n10.8\n0.8\n27.5\n0.9\n8.9\n16.8\n6.0\n22.6\n5.7\n\n\nWGerm\n6.7\n1.3\n35.8\n0.9\n7.3\n14.4\n5.0\n22.3\n6.1\n\n\nIreland\n23.2\n1.0\n20.7\n1.3\n7.5\n16.8\n2.8\n20.8\n6.1\n\n\nItaly\n15.9\n0.6\n27.6\n0.5\n10.0\n18.1\n1.6\n20.1\n5.7\n\n\nLuxem\n7.7\n3.1\n30.8\n0.8\n9.2\n18.5\n4.6\n19.2\n6.2\n\n\nNether\n6.3\n0.1\n22.5\n1.0\n9.9\n18.0\n6.8\n28.5\n6.8\n\n\nUK\n2.7\n1.4\n30.2\n1.4\n6.9\n16.9\n5.7\n28.3\n6.4\n\n\nAustria\n12.7\n1.1\n30.2\n1.4\n9.0\n16.8\n4.9\n16.8\n7.0\n\n\nFinland\n13.0\n0.4\n25.9\n1.3\n7.4\n14.7\n5.5\n24.3\n7.6\n\n\nGreece\n41.4\n0.6\n17.6\n0.6\n8.1\n11.5\n2.4\n11.0\n6.7\n\n\nNorway\n9.0\n0.5\n22.4\n0.8\n8.6\n16.9\n4.7\n27.6\n9.4\n\n\nPortugal\n27.8\n0.3\n24.5\n0.6\n8.4\n13.3\n2.7\n16.7\n5.7\n\n\nSpain\n22.9\n0.8\n28.5\n0.7\n11.5\n9.7\n8.5\n11.8\n5.5\n\n\nSweden\n6.1\n0.4\n25.9\n0.8\n7.2\n14.4\n6.0\n32.4\n6.8\n\n\nSwitz\n7.7\n0.2\n37.8\n0.8\n9.5\n17.5\n5.3\n15.4\n5.7\n\n\nTurkey\n66.8\n0.7\n7.9\n0.1\n2.8\n5.2\n1.1\n11.9\n3.2\n\n\nBulgaria\n23.6\n1.9\n32.3\n0.6\n7.9\n8.0\n0.7\n18.2\n6.7\n\n\nCzech\n16.5\n2.9\n35.5\n1.2\n8.7\n9.2\n0.9\n17.9\n7.0\n\n\nEGerm\n4.2\n2.9\n41.2\n1.3\n7.6\n11.2\n1.2\n22.1\n8.4\n\n\nHungary\n21.7\n3.1\n29.6\n1.9\n8.2\n9.4\n0.9\n17.2\n8.0\n\n\nPoland\n31.1\n2.5\n25.7\n0.9\n8.4\n7.5\n0.9\n16.1\n6.9\n\n\nRomania\n34.7\n2.1\n30.1\n0.6\n8.7\n5.9\n1.3\n11.7\n5.0\n\n\nUSSR\n23.7\n1.4\n25.8\n0.6\n9.2\n6.1\n0.5\n23.6\n9.3\n\n\nYugoslavia\n48.7\n1.5\n16.8\n1.1\n4.9\n6.4\n11.3\n5.3\n4.0\n\n\n\n\n\n2. Describe the dataset and make some hypotheses. You can for example:\n\nCalculate the measurements of each variable\nCalculate and visualize the correlation matrix\nShow the scatterplot matrix\netc..\n\n3. Apply PCA to the dataset. Show the variation explained by each of the principal components and the cumulative variation. Comment.\n\n\n\n\n\n\nImportant\n\n\n\nDonâ€™t forget to standardize the dataset, or to use the eigendecomposition of the correlation matrix instead of the variance-covariance matrix (no need to standardize in this case).\n\n\n4. In the following plot, you see a scatterplot matrix of the principal components. What does the green lines correspond to? what do you notice?\n\n\n\n\n\n\nThe PCs are uncorrelated, but not independent (uncorrelated does not imply independent).\n\n5. Plot the following:\n\nThe scree plot.\nThe graph of individuals.\nThe graph of variables.\nThe biplot graph.\nThe contributions of the variables to the first 2 principal components.\n\nInterpret the results (at least 3 interpretations).\nPCA from scratch\n6. Implement PCA on the eurojob dataset:\n\nStandardize the data.\nObtain the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix.\n\nExtra: Verify that the variance-covariance matrix of the standardized data is equal to the correlation matrix for the unstandardized data, and that both yield the same igenvectors and eigenvalue pairs\nSort eigenvalues in descending order and choose the \\(k\\) eigenvectors that correspond to the \\(k\\) largest eigenvalues, where \\(k\\) is the number of dimensions of the new feature subspace (\\(k \\le p\\)).\nConstruct the projection matrix \\(\\mathbf{A}\\) from the selected \\(k\\) eigenvectors.\nTransform the original dataset \\(X\\) via \\(\\mathbf{A}\\) to obtain a \\(k\\)-dimensional feature subspace \\(\\mathbf{Y}\\).\nVisualize the graph of individuals. Compare with the graph obtained in question 5.\n\n\nEigendecomposition - Computing Eigenvectors and Eigenvalues\nThe eigenvectors and eigenvalues of a covariance (or correlation) matrix represent the â€œcoreâ€ of a PCA: The eigenvectors (principal components) determine the directions of the new feature space, and the eigenvalues determine their magnitude. In other words, the eigenvalues explain the variance of the data along the new feature axes."
  },
  {
    "objectID": "Lab-Dim-Red.html#t-sne",
    "href": "Lab-Dim-Red.html#t-sne",
    "title": "Lab on Dimensionality Reduction",
    "section": "t-SNE",
    "text": "t-SNE\nIn this part, we are going to use a sample from the digits dataset. You can download the sample from here \nThe MNIST dataset contains tens of thousands of handwritten digits ranging from zero to nine. Each image is of size 28Ã—28 pixels.\nThe following image displays a couple of handwritten digits from the dataset:\n\n\n\n\n\n\n\n\nIt is required to flatten the images from 28Ã—28 to 1Ã—784 (which is already done in the given csv).\n\nLoad the dataset and describe it.\nShow some numbers like in the image above.\nApply PCA and t-SNE on the dataset and visualize in 2D plot the observations. Label the points by coloring them or showing the corresponding letter. Compare the results.\nWhat is the effect of the perplexity parameter when using t-SNE?\n\n\nIf you use R, use the Rtsne package.\n\n\n\n\n\nâ—¼"
  }
]